{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyflXuK1sxkVqtZxG6V7QZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhaj-mhd/two-tower-recommedation/blob/main/two_tower_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6rQ19NkZo8s",
        "outputId": "f3e06e54-47fe-4f27-a871-4872b63a5bb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Installing and upgrading all required packages...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "✅ All packages have been installed and upgraded.\n"
          ]
        }
      ],
      "source": [
        "print(\"⏳ Installing and upgrading all required packages...\")\n",
        "\n",
        "%pip install --upgrade -q tensorflow tensorflow-recommenders tf-keras tensorflow-text\n",
        "%pip install -q faiss-cpu\n",
        "\n",
        "print(\"\\n✅ All packages have been installed and upgraded.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade -q tensorflow-decision-forests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbdXsqBCabJ4",
        "outputId": "c609ac5f-d8cc-41c8-9230-90a07d4a94d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "import tf_keras\n",
        "import faiss\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"tensorflow-recommenders: {tfrs.__version__}\")\n",
        "print(f\"tf-keras: {tf_keras.__version__}\")\n",
        "print(f\"faiss-cpu: {faiss.__version__}\")\n",
        "print(f\"tensorflow-text: {tf_text.__version__}\")\n",
        "print(f\"tensorflow-decision-forests: {tfdf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "_roixOESalk5",
        "outputId": "a8a783f8-e0f6-400f-9efe-fb19aebe2eed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'float8_e4m3b11fnuz' from 'tensorflow.python.framework.dtypes' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/dtypes.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-855599773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/__internal__/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_initialize_variables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minitialize_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrack_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/applications/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtLarge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtSmall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/applications/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_coordinator_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtensor_api\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/dtensor/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdtensor_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v2/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masync_scope\u001b[0m \u001b[0;31m# line: 3054\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_executor_type\u001b[0m \u001b[0;31m# line: 2903\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloat8_e4m3b11fnuz\u001b[0m \u001b[0;31m# line: 465\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloat8_e4m3fn\u001b[0m \u001b[0;31m# line: 439\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloat8_e4m3fnuz\u001b[0m \u001b[0;31m# line: 452\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'float8_e4m3b11fnuz' from 'tensorflow.python.framework.dtypes' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/dtypes.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- Step 1: Fabricate Data with 20 Categories and Diverse User Behavior ---\n",
        "print(\"[1] Fabricating data with 20 categories and diverse user preferences...\")\n",
        "\n",
        "# 20 diverse categories\n",
        "categories = [\n",
        "    \"electronics\", \"clothing\", \"books\", \"home_garden\", \"sports_outdoors\",\n",
        "    \"beauty_health\", \"automotive\", \"toys_games\", \"jewelry\", \"music\",\n",
        "    \"movies_tv\", \"kitchen_dining\", \"office_supplies\", \"pet_supplies\", \"crafts\",\n",
        "    \"industrial\", \"grocery\", \"baby_products\", \"shoes\", \"watches\"\n",
        "]\n",
        "\n",
        "num_items = 10000  # Increased items to accommodate more categories\n",
        "num_users = 1000   # Increased users for better diversity\n",
        "\n",
        "# Create items with explicit category tracking\n",
        "item_titles = [f\"Product {i}\" for i in range(num_items)]\n",
        "item_categories = [categories[i % len(categories)] for i in range(num_items)]\n",
        "\n",
        "# Create diverse descriptions based on category\n",
        "description_templates = {\n",
        "    \"electronics\": [\n",
        "        \"High-tech electronic device with advanced features\",\n",
        "        \"Smart gadget with wireless connectivity and AI integration\",\n",
        "        \"Innovative electronic tool for modern digital life\",\n",
        "        \"Premium electronic device with cutting-edge technology\"\n",
        "    ],\n",
        "    \"clothing\": [\n",
        "        \"Fashionable apparel made from quality sustainable materials\",\n",
        "        \"Comfortable and stylish garment for everyday wear\",\n",
        "        \"Trendy clothing with modern design and premium fabric\",\n",
        "        \"Versatile wardrobe piece suitable for various occasions\"\n",
        "    ],\n",
        "    \"books\": [\n",
        "        \"Educational book covering important academic topics\",\n",
        "        \"Engaging literature for avid readers and book enthusiasts\",\n",
        "        \"Informative guide with practical knowledge and insights\",\n",
        "        \"Bestselling book with compelling storytelling and research\"\n",
        "    ],\n",
        "    \"home_garden\": [\n",
        "        \"Durable home improvement tool for DIY projects\",\n",
        "        \"Garden equipment for landscaping and plant care\",\n",
        "        \"Home decor item to enhance living space aesthetics\",\n",
        "        \"Functional household item for daily convenience\"\n",
        "    ],\n",
        "    \"sports_outdoors\": [\n",
        "        \"Professional sports equipment for athletic performance\",\n",
        "        \"Outdoor gear for adventure and recreational activities\",\n",
        "        \"Fitness equipment for home workout routines\",\n",
        "        \"Camping and hiking essentials for outdoor enthusiasts\"\n",
        "    ],\n",
        "    \"beauty_health\": [\n",
        "        \"Premium skincare product with natural ingredients\",\n",
        "        \"Health supplement for wellness and vitality\",\n",
        "        \"Cosmetic item for beauty enhancement and self-care\",\n",
        "        \"Personal care product for daily hygiene routine\"\n",
        "    ],\n",
        "    \"automotive\": [\n",
        "        \"High-quality automotive part for vehicle maintenance\",\n",
        "        \"Car accessory for enhanced driving experience\",\n",
        "        \"Professional-grade tool for automotive repair\",\n",
        "        \"Vehicle enhancement product for performance optimization\"\n",
        "    ],\n",
        "    \"toys_games\": [\n",
        "        \"Educational toy for children's development and learning\",\n",
        "        \"Board game for family entertainment and bonding\",\n",
        "        \"Creative plaything that sparks imagination and creativity\",\n",
        "        \"Interactive game for skill development and fun\"\n",
        "    ],\n",
        "    \"jewelry\": [\n",
        "        \"Elegant jewelry piece crafted with precious metals\",\n",
        "        \"Stylish accessory for fashion and personal expression\",\n",
        "        \"Handcrafted jewelry with unique design elements\",\n",
        "        \"Luxury jewelry item for special occasions\"\n",
        "    ],\n",
        "    \"music\": [\n",
        "        \"Professional music equipment for audio production\",\n",
        "        \"Musical instrument for creative expression and performance\",\n",
        "        \"High-quality audio device for music enthusiasts\",\n",
        "        \"Music accessory for enhanced listening experience\"\n",
        "    ],\n",
        "    \"movies_tv\": [\n",
        "        \"Entertainment media for leisure and relaxation\",\n",
        "        \"Classic film collection for movie enthusiasts\",\n",
        "        \"TV series with compelling storylines and characters\",\n",
        "        \"Documentary content for educational entertainment\"\n",
        "    ],\n",
        "    \"kitchen_dining\": [\n",
        "        \"Professional kitchen utensil for culinary excellence\",\n",
        "        \"Dining accessory for elegant meal presentation\",\n",
        "        \"Cooking tool made from premium food-safe materials\",\n",
        "        \"Kitchen gadget for efficient food preparation\"\n",
        "    ],\n",
        "    \"office_supplies\": [\n",
        "        \"Professional office equipment for workplace productivity\",\n",
        "        \"Stationery item for organization and documentation\",\n",
        "        \"Ergonomic office accessory for comfort and efficiency\",\n",
        "        \"Business tool for professional operations\"\n",
        "    ],\n",
        "    \"pet_supplies\": [\n",
        "        \"Pet care product for animal health and happiness\",\n",
        "        \"Pet toy for entertainment and exercise\",\n",
        "        \"Pet accessory for comfort and safety\",\n",
        "        \"Pet nutrition product for optimal health\"\n",
        "    ],\n",
        "    \"crafts\": [\n",
        "        \"Art supply for creative projects and expression\",\n",
        "        \"Craft material for DIY projects and hobbies\",\n",
        "        \"Creative tool for artistic endeavors and crafting\",\n",
        "        \"Handcraft supply for personalized creations\"\n",
        "    ],\n",
        "    \"industrial\": [\n",
        "        \"Industrial equipment for manufacturing and production\",\n",
        "        \"Heavy-duty tool for professional industrial use\",\n",
        "        \"Machinery component for industrial operations\",\n",
        "        \"Professional-grade equipment for industrial applications\"\n",
        "    ],\n",
        "    \"grocery\": [\n",
        "        \"Premium food product for nutritious meals\",\n",
        "        \"Organic ingredient for healthy cooking\",\n",
        "        \"Gourmet food item for culinary excellence\",\n",
        "        \"Essential grocery item for daily nutrition\"\n",
        "    ],\n",
        "    \"baby_products\": [\n",
        "        \"Safe baby product for infant care and development\",\n",
        "        \"Baby accessory for comfort and convenience\",\n",
        "        \"Child safety item for protection and security\",\n",
        "        \"Developmental toy for early childhood learning\"\n",
        "    ],\n",
        "    \"shoes\": [\n",
        "        \"Comfortable footwear for daily wear and activities\",\n",
        "        \"Athletic shoe for sports and fitness activities\",\n",
        "        \"Fashion shoe for style and personal expression\",\n",
        "        \"Professional footwear for workplace requirements\"\n",
        "    ],\n",
        "    \"watches\": [\n",
        "        \"Precision timepiece with advanced features\",\n",
        "        \"Luxury watch for style and status\",\n",
        "        \"Sports watch for active lifestyle tracking\",\n",
        "        \"Smart watch with digital connectivity features\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "item_descriptions = []\n",
        "for i in range(num_items):\n",
        "    category = item_categories[i]\n",
        "    template = description_templates[category][i % len(description_templates[category])]\n",
        "    item_descriptions.append(f\"{template}. Model v{i % 15}. Item #{i}.\")\n",
        "\n",
        "items_data = {\n",
        "    \"item_id\": [str(i) for i in range(num_items)],\n",
        "    \"item_title\": item_titles,\n",
        "    \"item_description\": item_descriptions,\n",
        "    \"category\": item_categories\n",
        "}\n",
        "items_df = pd.DataFrame(items_data)\n",
        "\n",
        "# Create category-to-items mapping for easier lookup\n",
        "category_to_items = defaultdict(list)\n",
        "for idx, row in items_df.iterrows():\n",
        "    category_to_items[row['category']].append(row['item_id'])\n",
        "\n",
        "# Generate user interactions: each user interacts with 3-5 categories, 1-5 interactions per category\n",
        "print(\"Generating user interactions with diverse category preferences...\")\n",
        "user_interactions = []\n",
        "user_categories = {}  # Track which categories each user prefers\n",
        "\n",
        "for user_id in range(num_users):\n",
        "    # Each user randomly selects 3-5 categories\n",
        "    num_categories = np.random.randint(3, 6)  # 3 to 5 categories\n",
        "    preferred_categories = np.random.choice(categories, size=num_categories, replace=False)\n",
        "    user_categories[str(user_id)] = preferred_categories\n",
        "\n",
        "    # Generate interactions for each preferred category\n",
        "    for category in preferred_categories:\n",
        "        # Random number of interactions per category (1-5)\n",
        "        num_interactions = np.random.randint(1, 6)  # 1 to 5 interactions\n",
        "\n",
        "        # Select random items from this category\n",
        "        available_items = category_to_items[category]\n",
        "        selected_items = np.random.choice(available_items, size=num_interactions, replace=True)\n",
        "\n",
        "        for item_id in selected_items:\n",
        "            user_interactions.append({\n",
        "                \"user_id\": str(user_id),\n",
        "                \"item_id\": item_id\n",
        "            })\n",
        "\n",
        "interactions_df = pd.DataFrame(user_interactions)\n",
        "\n",
        "# Calculate statistics\n",
        "total_interactions = len(interactions_df)\n",
        "avg_interactions_per_user = total_interactions / num_users\n",
        "category_interaction_counts = defaultdict(int)\n",
        "user_category_counts = defaultdict(int)\n",
        "\n",
        "for _, interaction in interactions_df.iterrows():\n",
        "    item_category = items_df[items_df['item_id'] == interaction['item_id']]['category'].iloc[0]\n",
        "    category_interaction_counts[item_category] += 1\n",
        "\n",
        "for user_id, cats in user_categories.items():\n",
        "    user_category_counts[len(cats)] += 1\n",
        "\n",
        "print(f\"Generated {len(items_df)} items across {len(categories)} categories\")\n",
        "print(f\"Generated {total_interactions} interactions from {num_users} users\")\n",
        "print(f\"Average interactions per user: {avg_interactions_per_user:.1f}\")\n",
        "print(f\"\\nUser category distribution:\")\n",
        "for num_cats, count in sorted(user_category_counts.items()):\n",
        "    print(f\"  {num_cats} categories: {count} users ({count/num_users:.1%})\")\n",
        "\n",
        "print(f\"\\nTop 10 most interacted categories:\")\n",
        "sorted_categories = sorted(category_interaction_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "for i, (cat, count) in enumerate(sorted_categories[:10]):\n",
        "    print(f\"  {i+1}. {cat}: {count} interactions\")\n",
        "\n",
        "# Display sample user preferences\n",
        "print(\"\\nSample user category preferences:\")\n",
        "for i in range(10):\n",
        "    user_id = str(i)\n",
        "    cats = user_categories[user_id]\n",
        "    user_interactions_count = len(interactions_df[interactions_df['user_id'] == user_id])\n",
        "    print(f\"User {user_id}: {list(cats)} ({user_interactions_count} total interactions)\")\n",
        "\n",
        "items_ds = tf.data.Dataset.from_tensor_slices(dict(items_df))\n",
        "\n",
        "# --- Step 2: Self-Supervised Item Tower ---\n",
        "print(\"\\n[2] Building and training the self-supervised Item Tower...\")\n",
        "embedding_dimension = 128  # Increased for better representation\n",
        "max_tokens = 15_000  # Increased vocabulary\n",
        "sequence_length = 120  # Increased sequence length\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=sequence_length)\n",
        "text_vectorizer.adapt(items_ds.map(lambda x: x[\"item_description\"]).batch(256))\n",
        "\n",
        "class ItemModel(tf.keras.Model):\n",
        "    def __init__(self, vectorizer):\n",
        "        super().__init__()\n",
        "        self.vectorizer = vectorizer\n",
        "        self.embedding = tf.keras.Sequential([\n",
        "            self.vectorizer,\n",
        "            tf.keras.layers.Embedding(input_dim=self.vectorizer.vocabulary_size(),\n",
        "                                    output_dim=embedding_dimension,\n",
        "                                    mask_zero=True),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ])\n",
        "        self.dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(embedding_dimension)\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(self.embedding(inputs[\"item_description\"]))\n",
        "\n",
        "class SelfSupervisedItemTwoTower(tfrs.Model):\n",
        "    def __init__(self, item_model):\n",
        "        super().__init__()\n",
        "        self.item_model = item_model\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        item_embeddings = self.item_model(features)\n",
        "        return self.task(query_embeddings=item_embeddings, candidate_embeddings=item_embeddings)\n",
        "\n",
        "item_tower = ItemModel(text_vectorizer)\n",
        "item_model_trainer = SelfSupervisedItemTwoTower(item_tower)\n",
        "item_model_trainer.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "# Train with more epochs for better convergence\n",
        "train_item_ds = items_ds.map(lambda x: {\"item_description\": x[\"item_description\"]}).batch(512).cache()\n",
        "print(\"Training Item Tower...\")\n",
        "item_model_trainer.fit(train_item_ds, epochs=15, verbose=1)\n",
        "print(\"Item Tower training complete.\")\n",
        "\n",
        "# --- Step 3: Generate and Store Item Embeddings in Faiss ---\n",
        "print(\"\\n[3] Generating item embeddings and storing in Faiss...\")\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "item_embeddings_generator = items_ds.batch(512).map(lambda x: item_tower(x))\n",
        "all_item_embeddings = np.concatenate(list(item_embeddings_generator.as_numpy_iterator()))\n",
        "\n",
        "# Normalize embeddings for better similarity search\n",
        "all_item_embeddings = all_item_embeddings / np.linalg.norm(all_item_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "index.add(all_item_embeddings)\n",
        "print(f\"Faiss index now contains {index.ntotal} vectors.\")\n",
        "index_to_item_id = {i: item_id for i, item_id in enumerate(items_df[\"item_id\"])}\n",
        "\n",
        "# --- Step 4: Train the User Tower ---\n",
        "print(\"\\n[4] Building and training the User Tower...\")\n",
        "unique_user_ids = interactions_df[\"user_id\"].unique()\n",
        "\n",
        "class UserModel(tf.keras.Model):\n",
        "    def __init__(self, user_ids):\n",
        "        super().__init__()\n",
        "        self.user_embedding = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=user_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(user_ids) + 1, embedding_dimension)\n",
        "        ])\n",
        "        self.dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(embedding_dimension)\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(self.user_embedding(inputs))\n",
        "\n",
        "class UserItemRetrievalModel(tfrs.Model):\n",
        "    def __init__(self, user_model, item_model):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.item_model = item_model\n",
        "        self.item_model.trainable = False  # Keep item model frozen\n",
        "\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, data, training=False):\n",
        "        user_embeddings = self.user_model(data[\"user_id\"])\n",
        "\n",
        "        # Get item embeddings for the interacted items\n",
        "        item_data = {\"item_description\": data[\"item_description\"]}\n",
        "        item_embeddings = self.item_model(item_data)\n",
        "\n",
        "        # Normalize embeddings\n",
        "        user_embeddings = tf.nn.l2_normalize(user_embeddings, axis=1)\n",
        "        item_embeddings = tf.nn.l2_normalize(item_embeddings, axis=1)\n",
        "\n",
        "        return self.task(\n",
        "            query_embeddings=user_embeddings,\n",
        "            candidate_embeddings=item_embeddings\n",
        "        )\n",
        "\n",
        "# Prepare training data with item descriptions and categories\n",
        "interactions_with_details_df = pd.merge(\n",
        "    interactions_df,\n",
        "    items_df[['item_id', 'item_description', 'category']],\n",
        "    on='item_id'\n",
        ")\n",
        "\n",
        "# Create negative sampling for better training\n",
        "print(\"Creating training dataset with negative sampling...\")\n",
        "positive_interactions = interactions_with_details_df.copy()\n",
        "positive_interactions['label'] = 1.0\n",
        "\n",
        "# Create negative samples\n",
        "negative_interactions = []\n",
        "for user_id in unique_user_ids:\n",
        "    user_positive_items = set(positive_interactions[positive_interactions['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Sample negative items (items the user hasn't interacted with)\n",
        "    num_negatives = min(len(user_positive_items), 20)  # Limit negatives to prevent memory issues\n",
        "    all_items = set(items_df['item_id'])\n",
        "    negative_items = list(all_items - user_positive_items)\n",
        "\n",
        "    if len(negative_items) >= num_negatives:\n",
        "        sampled_negatives = np.random.choice(negative_items, size=num_negatives, replace=False)\n",
        "\n",
        "        for item_id in sampled_negatives:\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            negative_interactions.append({\n",
        "                'user_id': user_id,\n",
        "                'item_id': item_id,\n",
        "                'item_description': item_row['item_description'],\n",
        "                'category': item_row['category'],\n",
        "                'label': 0.0\n",
        "            })\n",
        "\n",
        "negative_interactions_df = pd.DataFrame(negative_interactions)\n",
        "full_training_df = pd.concat([positive_interactions, negative_interactions_df], ignore_index=True)\n",
        "\n",
        "# Shuffle the training data\n",
        "full_training_df = full_training_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "full_interactions_ds = tf.data.Dataset.from_tensor_slices(dict(full_training_df))\n",
        "train_ds_user = full_interactions_ds.shuffle(50_000).batch(512).cache()\n",
        "\n",
        "user_tower = UserModel(unique_user_ids)\n",
        "user_model_trainer = UserItemRetrievalModel(user_tower, item_tower)\n",
        "user_model_trainer.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "# Train the user model\n",
        "print(\"Training User Tower...\")\n",
        "user_model_trainer.fit(train_ds_user, epochs=15, verbose=1)\n",
        "print(\"User Tower training complete.\")\n",
        "\n",
        "# --- Step 5: Enhanced Recommendation Function ---\n",
        "print(\"\\n[5] Implementing recommendation functions...\")\n",
        "\n",
        "def get_balanced_recommendations(user_id, top_k=15):\n",
        "    \"\"\"Get recommendations with explicit category balancing\"\"\"\n",
        "    print(f\"\\n--- Balanced recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return None\n",
        "\n",
        "    # Get user's preferred categories\n",
        "    preferred_categories = user_categories[user_id]\n",
        "    print(f\"User's preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get user embedding and normalize it\n",
        "    user_embedding = user_tower(tf.constant([user_id])).numpy()\n",
        "    user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Get recommendations for each preferred category\n",
        "    recommendations = []\n",
        "    items_per_category = max(1, top_k // len(preferred_categories))\n",
        "\n",
        "    for i, category in enumerate(preferred_categories):\n",
        "        # Get items from this specific category\n",
        "        category_items = items_df[items_df['category'] == category]\n",
        "\n",
        "        # Filter out already interacted items\n",
        "        available_category_items = category_items[\n",
        "            ~category_items['item_id'].isin(user_interacted_items)\n",
        "        ]\n",
        "\n",
        "        if len(available_category_items) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get embeddings for items in this category\n",
        "        category_item_indices = []\n",
        "        for _, item_row in available_category_items.iterrows():\n",
        "            item_idx = int(item_row['item_id'])\n",
        "            category_item_indices.append(item_idx)\n",
        "\n",
        "        # Calculate similarities to items in this category\n",
        "        category_embeddings = all_item_embeddings[category_item_indices]\n",
        "        similarities = np.dot(user_embedding, category_embeddings.T).flatten()\n",
        "\n",
        "        # Get top items from this category\n",
        "        top_indices = np.argsort(similarities)[::-1][:items_per_category]\n",
        "\n",
        "        for rank, idx in enumerate(top_indices):\n",
        "            item_row = available_category_items.iloc[idx]\n",
        "            similarity = similarities[idx]\n",
        "\n",
        "            recommendations.append({\n",
        "                'item_id': item_row['item_id'],\n",
        "                'category': item_row['category'],\n",
        "                'title': item_row['item_title'],\n",
        "                'similarity': similarity,\n",
        "                'rank_in_category': rank + 1\n",
        "            })\n",
        "\n",
        "    # Sort all recommendations by similarity\n",
        "    recommendations.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "    recommendations = recommendations[:top_k]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    category_counts = defaultdict(int)\n",
        "    correct_recommendations = 0\n",
        "\n",
        "    for rec in recommendations:\n",
        "        category_counts[rec['category']] += 1\n",
        "        if rec['category'] in preferred_categories:\n",
        "            correct_recommendations += 1\n",
        "\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in preferred_categories:\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        print(f\"  ✓ {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def get_collaborative_recommendations(user_id, top_k=15):\n",
        "    \"\"\"Pure collaborative filtering recommendations\"\"\"\n",
        "    print(f\"\\n--- Collaborative filtering for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return None\n",
        "\n",
        "    preferred_categories = user_categories[user_id]\n",
        "    print(f\"User's preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get user embedding\n",
        "    user_embedding = user_tower(tf.constant([user_id])).numpy()\n",
        "    user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Search for similar items\n",
        "    search_k = min(top_k * 3, index.ntotal)\n",
        "    distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "    recommendations = []\n",
        "    for idx in indices[0]:\n",
        "        item_id = index_to_item_id[idx]\n",
        "        if item_id in user_interacted_items:\n",
        "            continue\n",
        "\n",
        "        item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "        recommendations.append({\n",
        "            'item_id': item_id,\n",
        "            'category': item_row['category'],\n",
        "            'title': item_row['item_title']\n",
        "        })\n",
        "\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "\n",
        "    # Calculate accuracy\n",
        "    category_counts = defaultdict(int)\n",
        "    correct_recommendations = 0\n",
        "\n",
        "    for rec in recommendations:\n",
        "        category_counts[rec['category']] += 1\n",
        "        if rec['category'] in preferred_categories:\n",
        "            correct_recommendations += 1\n",
        "\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# --- Step 6: Testing and Evaluation ---\n",
        "print(\"\\n[6] Testing recommendation system...\")\n",
        "\n",
        "def comprehensive_test():\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPREHENSIVE RECOMMENDATION SYSTEM TEST\")\n",
        "    print(f\"Categories: {len(categories)}\")\n",
        "    print(f\"Users: {num_users}\")\n",
        "    print(f\"Items: {num_items}\")\n",
        "    print(f\"Total interactions: {len(interactions_df)}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Test multiple users\n",
        "    test_users = np.random.choice(unique_user_ids, size=10, replace=False)\n",
        "\n",
        "    balanced_accuracies = []\n",
        "    collaborative_accuracies = []\n",
        "\n",
        "    for i, user_id in enumerate(test_users):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TEST {i+1}/10: User ID '{user_id}'\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Show user stats\n",
        "        user_interactions = interactions_df[interactions_df['user_id'] == user_id]\n",
        "        preferred_categories = user_categories[user_id]\n",
        "\n",
        "        print(f\"User profile:\")\n",
        "        print(f\"  - Preferred categories: {list(preferred_categories)}\")\n",
        "        print(f\"  - Total interactions: {len(user_interactions)}\")\n",
        "\n",
        "        # Test balanced approach\n",
        "        balanced_acc = get_balanced_recommendations(user_id, top_k=12)\n",
        "        if balanced_acc is not None:\n",
        "            balanced_accuracies.append(balanced_acc)\n",
        "\n",
        "        # Test collaborative approach\n",
        "        collaborative_acc = get_collaborative_recommendations(user_id, top_k=12)\n",
        "        if collaborative_acc is not None:\n",
        "            collaborative_accuracies.append(collaborative_acc)\n",
        "\n",
        "    # Overall results\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"OVERALL RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if balanced_accuracies:\n",
        "        avg_balanced = np.mean(balanced_accuracies)\n",
        "        std_balanced = np.std(balanced_accuracies)\n",
        "        print(f\"Balanced Approach:\")\n",
        "        print(f\"  Average accuracy: {avg_balanced:.1%} ± {std_balanced:.1%}\")\n",
        "        print(f\"  Range: {min(balanced_accuracies):.1%} - {max(balanced_accuracies):.1%}\")\n",
        "\n",
        "    if collaborative_accuracies:\n",
        "        avg_collaborative = np.mean(collaborative_accuracies)\n",
        "        std_collaborative = np.std(collaborative_accuracies)\n",
        "        print(f\"Collaborative Filtering:\")\n",
        "        print(f\"  Average accuracy: {avg_collaborative:.1%} ± {std_collaborative:.1%}\")\n",
        "        print(f\"  Range: {min(collaborative_accuracies):.1%} - {max(collaborative_accuracies):.1%}\")\n",
        "\n",
        "    # Expected random performance\n",
        "    avg_categories_per_user = np.mean([len(cats) for cats in user_categories.values()])\n",
        "    random_accuracy = avg_categories_per_user / len(categories)\n",
        "    print(f\"Expected random accuracy: {random_accuracy:.1%}\")\n",
        "\n",
        "    # Performance comparison\n",
        "    if balanced_accuracies and collaborative_accuracies:\n",
        "        if avg_balanced > avg_collaborative:\n",
        "            improvement = ((avg_balanced - avg_collaborative) / avg_collaborative) * 100\n",
        "            print(f\"✓ Balanced approach performs {improvement:.1f}% better\")\n",
        "        else:\n",
        "            improvement = ((avg_collaborative - avg_balanced) / avg_balanced) * 100\n",
        "            print(f\"✓ Collaborative approach performs {improvement:.1f}% better\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYSTEM ANALYSIS COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# Run comprehensive test\n",
        "comprehensive_test()\n",
        "\n",
        "# Quick analysis of category distribution\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"CATEGORY ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "category_stats = []\n",
        "for category in categories:\n",
        "    items_count = len(category_to_items[category])\n",
        "    interactions_count = len(interactions_with_details_df[interactions_with_details_df['category'] == category])\n",
        "    users_count = len(set(interactions_with_details_df[interactions_with_details_df['category'] == category]['user_id']))\n",
        "\n",
        "    category_stats.append({\n",
        "        'category': category,\n",
        "        'items': items_count,\n",
        "        'interactions': interactions_count,\n",
        "        'users': users_count\n",
        "    })\n",
        "\n",
        "category_stats.sort(key=lambda x: x['interactions'], reverse=True)\n",
        "\n",
        "print(\"Category popularity (by interactions):\")\n",
        "for i, stats in enumerate(category_stats[:10]):\n",
        "    print(f\"  {i+1:2d}. {stats['category']:15s}: {stats['interactions']:4d} interactions, {stats['users']:3d} users\")\n",
        "\n",
        "print(f\"\\nLeast popular categories:\")\n",
        "for i, stats in enumerate(category_stats[-5:]):\n",
        "    print(f\"  {stats['category']:15s}: {stats['interactions']:4d} interactions, {stats['users']:3d} users\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlwTsIUjaz4S",
        "outputId": "6eb2bb03-da0c-429e-adf2-274596a9e64f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Fabricating data with 20 categories and diverse user preferences...\n",
            "Generating user interactions with diverse category preferences...\n",
            "Generated 10000 items across 20 categories\n",
            "Generated 12056 interactions from 1000 users\n",
            "Average interactions per user: 12.1\n",
            "\n",
            "User category distribution:\n",
            "  3 categories: 326 users (32.6%)\n",
            "  4 categories: 334 users (33.4%)\n",
            "  5 categories: 340 users (34.0%)\n",
            "\n",
            "Top 10 most interacted categories:\n",
            "  1. shoes: 736 interactions\n",
            "  2. office_supplies: 704 interactions\n",
            "  3. grocery: 689 interactions\n",
            "  4. watches: 677 interactions\n",
            "  5. clothing: 653 interactions\n",
            "  6. baby_products: 623 interactions\n",
            "  7. beauty_health: 621 interactions\n",
            "  8. books: 611 interactions\n",
            "  9. home_garden: 604 interactions\n",
            "  10. jewelry: 604 interactions\n",
            "\n",
            "Sample user category preferences:\n",
            "User 0: [np.str_('office_supplies'), np.str_('industrial'), np.str_('clothing'), np.str_('kitchen_dining')] (15 total interactions)\n",
            "User 1: [np.str_('sports_outdoors'), np.str_('home_garden'), np.str_('kitchen_dining'), np.str_('movies_tv'), np.str_('shoes')] (13 total interactions)\n",
            "User 2: [np.str_('home_garden'), np.str_('shoes'), np.str_('crafts'), np.str_('electronics')] (15 total interactions)\n",
            "User 3: [np.str_('kitchen_dining'), np.str_('pet_supplies'), np.str_('home_garden'), np.str_('music'), np.str_('shoes')] (18 total interactions)\n",
            "User 4: [np.str_('home_garden'), np.str_('crafts'), np.str_('automotive')] (12 total interactions)\n",
            "User 5: [np.str_('shoes'), np.str_('watches'), np.str_('toys_games')] (8 total interactions)\n",
            "User 6: [np.str_('watches'), np.str_('beauty_health'), np.str_('shoes'), np.str_('toys_games')] (13 total interactions)\n",
            "User 7: [np.str_('toys_games'), np.str_('sports_outdoors'), np.str_('shoes')] (11 total interactions)\n",
            "User 8: [np.str_('toys_games'), np.str_('watches'), np.str_('automotive'), np.str_('books'), np.str_('baby_products')] (15 total interactions)\n",
            "User 9: [np.str_('crafts'), np.str_('watches'), np.str_('books'), np.str_('music'), np.str_('jewelry')] (11 total interactions)\n",
            "\n",
            "[2] Building and training the self-supervised Item Tower...\n",
            "Training Item Tower...\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - loss: 2598.6877 - regularization_loss: 0.0000e+00 - total_loss: 2598.6877\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 1587.5535 - regularization_loss: 0.0000e+00 - total_loss: 1587.5535\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 1378.4154 - regularization_loss: 0.0000e+00 - total_loss: 1378.4154\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 1118.4304 - regularization_loss: 0.0000e+00 - total_loss: 1118.4304\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 1083.1123 - regularization_loss: 0.0000e+00 - total_loss: 1083.1123\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 1043.3713 - regularization_loss: 0.0000e+00 - total_loss: 1043.3713\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 1019.8990 - regularization_loss: 0.0000e+00 - total_loss: 1019.8990\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 1002.1135 - regularization_loss: 0.0000e+00 - total_loss: 1002.1135\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 999.2770 - regularization_loss: 0.0000e+00 - total_loss: 999.2770  \n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 1059.3287 - regularization_loss: 0.0000e+00 - total_loss: 1059.3287\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 1021.9964 - regularization_loss: 0.0000e+00 - total_loss: 1021.9964\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 954.0804 - regularization_loss: 0.0000e+00 - total_loss: 954.0804  \n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 858.5400 - regularization_loss: 0.0000e+00 - total_loss: 858.5400\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 759.7740 - regularization_loss: 0.0000e+00 - total_loss: 759.7740\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 1236.1692 - regularization_loss: 0.0000e+00 - total_loss: 1236.1692\n",
            "Item Tower training complete.\n",
            "\n",
            "[3] Generating item embeddings and storing in Faiss...\n",
            "Faiss index now contains 10000 vectors.\n",
            "\n",
            "[4] Building and training the User Tower...\n",
            "Creating training dataset with negative sampling...\n",
            "Training User Tower...\n",
            "Epoch 1/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - loss: 3180.3049 - regularization_loss: 0.0000e+00 - total_loss: 3180.3049\n",
            "Epoch 2/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 3145.6301 - regularization_loss: 0.0000e+00 - total_loss: 3145.6301\n",
            "Epoch 3/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 3133.0496 - regularization_loss: 0.0000e+00 - total_loss: 3133.0496\n",
            "Epoch 4/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3127.7542 - regularization_loss: 0.0000e+00 - total_loss: 3127.7542\n",
            "Epoch 5/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - loss: 3124.6968 - regularization_loss: 0.0000e+00 - total_loss: 3124.6968\n",
            "Epoch 6/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - loss: 3123.1570 - regularization_loss: 0.0000e+00 - total_loss: 3123.1570\n",
            "Epoch 7/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 3120.9087 - regularization_loss: 0.0000e+00 - total_loss: 3120.9087\n",
            "Epoch 8/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 3119.1243 - regularization_loss: 0.0000e+00 - total_loss: 3119.1243\n",
            "Epoch 9/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 3118.3596 - regularization_loss: 0.0000e+00 - total_loss: 3118.3596\n",
            "Epoch 10/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3118.1362 - regularization_loss: 0.0000e+00 - total_loss: 3118.1362\n",
            "Epoch 11/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - loss: 3118.2593 - regularization_loss: 0.0000e+00 - total_loss: 3118.2593\n",
            "Epoch 12/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 3117.4460 - regularization_loss: 0.0000e+00 - total_loss: 3117.4460\n",
            "Epoch 13/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 3116.0874 - regularization_loss: 0.0000e+00 - total_loss: 3116.0874\n",
            "Epoch 14/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 3115.0032 - regularization_loss: 0.0000e+00 - total_loss: 3115.0032\n",
            "Epoch 15/15\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 3114.1321 - regularization_loss: 0.0000e+00 - total_loss: 3114.1321\n",
            "User Tower training complete.\n",
            "\n",
            "[5] Implementing recommendation functions...\n",
            "\n",
            "[6] Testing recommendation system...\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE RECOMMENDATION SYSTEM TEST\n",
            "Categories: 20\n",
            "Users: 1000\n",
            "Items: 10000\n",
            "Total interactions: 12056\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "TEST 1/10: User ID '1'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('sports_outdoors'), np.str_('home_garden'), np.str_('kitchen_dining'), np.str_('movies_tv'), np.str_('shoes')]\n",
            "  - Total interactions: 13\n",
            "\n",
            "--- Balanced recommendations for user '1' ---\n",
            "User's preferred categories (5): [np.str_('sports_outdoors'), np.str_('home_garden'), np.str_('kitchen_dining'), np.str_('movies_tv'), np.str_('shoes')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ sports_outdoors: 2 items (20.0%)\n",
            "  ✓ home_garden: 2 items (20.0%)\n",
            "  ✓ kitchen_dining: 2 items (20.0%)\n",
            "  ✓ movies_tv: 2 items (20.0%)\n",
            "  ✓ shoes: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '1' ---\n",
            "User's preferred categories (5): [np.str_('sports_outdoors'), np.str_('home_garden'), np.str_('kitchen_dining'), np.str_('movies_tv'), np.str_('shoes')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ sports_outdoors: 12 items (100.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 2/10: User ID '895'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('books'), np.str_('automotive'), np.str_('grocery'), np.str_('pet_supplies')]\n",
            "  - Total interactions: 12\n",
            "\n",
            "--- Balanced recommendations for user '895' ---\n",
            "User's preferred categories (4): [np.str_('books'), np.str_('automotive'), np.str_('grocery'), np.str_('pet_supplies')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ books: 3 items (25.0%)\n",
            "  ✓ automotive: 3 items (25.0%)\n",
            "  ✓ grocery: 3 items (25.0%)\n",
            "  ✓ pet_supplies: 3 items (25.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '895' ---\n",
            "User's preferred categories (4): [np.str_('books'), np.str_('automotive'), np.str_('grocery'), np.str_('pet_supplies')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ grocery: 12 items (100.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 3/10: User ID '680'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('electronics'), np.str_('grocery'), np.str_('automotive'), np.str_('crafts')]\n",
            "  - Total interactions: 12\n",
            "\n",
            "--- Balanced recommendations for user '680' ---\n",
            "User's preferred categories (4): [np.str_('electronics'), np.str_('grocery'), np.str_('automotive'), np.str_('crafts')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ electronics: 3 items (25.0%)\n",
            "  ✓ grocery: 3 items (25.0%)\n",
            "  ✓ automotive: 3 items (25.0%)\n",
            "  ✓ crafts: 3 items (25.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '680' ---\n",
            "User's preferred categories (4): [np.str_('electronics'), np.str_('grocery'), np.str_('automotive'), np.str_('crafts')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✗ watches: 12 items (100.0%)\n",
            "Accuracy: 0/12 (0.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 4/10: User ID '390'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('jewelry'), np.str_('electronics'), np.str_('grocery')]\n",
            "  - Total interactions: 14\n",
            "\n",
            "--- Balanced recommendations for user '390' ---\n",
            "User's preferred categories (3): [np.str_('jewelry'), np.str_('electronics'), np.str_('grocery')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ jewelry: 4 items (33.3%)\n",
            "  ✓ electronics: 4 items (33.3%)\n",
            "  ✓ grocery: 4 items (33.3%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '390' ---\n",
            "User's preferred categories (3): [np.str_('jewelry'), np.str_('electronics'), np.str_('grocery')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✗ watches: 12 items (100.0%)\n",
            "Accuracy: 0/12 (0.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 5/10: User ID '372'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('sports_outdoors'), np.str_('office_supplies'), np.str_('baby_products')]\n",
            "  - Total interactions: 12\n",
            "\n",
            "--- Balanced recommendations for user '372' ---\n",
            "User's preferred categories (3): [np.str_('sports_outdoors'), np.str_('office_supplies'), np.str_('baby_products')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ sports_outdoors: 4 items (33.3%)\n",
            "  ✓ office_supplies: 4 items (33.3%)\n",
            "  ✓ baby_products: 4 items (33.3%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '372' ---\n",
            "User's preferred categories (3): [np.str_('sports_outdoors'), np.str_('office_supplies'), np.str_('baby_products')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✗ home_garden: 12 items (100.0%)\n",
            "Accuracy: 0/12 (0.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 6/10: User ID '583'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('baby_products'), np.str_('grocery'), np.str_('industrial')]\n",
            "  - Total interactions: 15\n",
            "\n",
            "--- Balanced recommendations for user '583' ---\n",
            "User's preferred categories (3): [np.str_('baby_products'), np.str_('grocery'), np.str_('industrial')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ baby_products: 4 items (33.3%)\n",
            "  ✓ grocery: 4 items (33.3%)\n",
            "  ✓ industrial: 4 items (33.3%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '583' ---\n",
            "User's preferred categories (3): [np.str_('baby_products'), np.str_('grocery'), np.str_('industrial')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✗ home_garden: 12 items (100.0%)\n",
            "Accuracy: 0/12 (0.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 7/10: User ID '34'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('pet_supplies'), np.str_('sports_outdoors'), np.str_('watches'), np.str_('books')]\n",
            "  - Total interactions: 14\n",
            "\n",
            "--- Balanced recommendations for user '34' ---\n",
            "User's preferred categories (4): [np.str_('pet_supplies'), np.str_('sports_outdoors'), np.str_('watches'), np.str_('books')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ pet_supplies: 3 items (25.0%)\n",
            "  ✓ sports_outdoors: 3 items (25.0%)\n",
            "  ✓ watches: 3 items (25.0%)\n",
            "  ✓ books: 3 items (25.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '34' ---\n",
            "User's preferred categories (4): [np.str_('pet_supplies'), np.str_('sports_outdoors'), np.str_('watches'), np.str_('books')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ pet_supplies: 12 items (100.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 8/10: User ID '142'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('jewelry'), np.str_('office_supplies'), np.str_('crafts')]\n",
            "  - Total interactions: 12\n",
            "\n",
            "--- Balanced recommendations for user '142' ---\n",
            "User's preferred categories (3): [np.str_('jewelry'), np.str_('office_supplies'), np.str_('crafts')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ jewelry: 4 items (33.3%)\n",
            "  ✓ office_supplies: 4 items (33.3%)\n",
            "  ✓ crafts: 4 items (33.3%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '142' ---\n",
            "User's preferred categories (3): [np.str_('jewelry'), np.str_('office_supplies'), np.str_('crafts')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✗ grocery: 12 items (100.0%)\n",
            "Accuracy: 0/12 (0.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 9/10: User ID '890'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('pet_supplies'), np.str_('jewelry'), np.str_('movies_tv'), np.str_('sports_outdoors'), np.str_('grocery')]\n",
            "  - Total interactions: 14\n",
            "\n",
            "--- Balanced recommendations for user '890' ---\n",
            "User's preferred categories (5): [np.str_('pet_supplies'), np.str_('jewelry'), np.str_('movies_tv'), np.str_('sports_outdoors'), np.str_('grocery')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ pet_supplies: 2 items (20.0%)\n",
            "  ✓ jewelry: 2 items (20.0%)\n",
            "  ✓ movies_tv: 2 items (20.0%)\n",
            "  ✓ sports_outdoors: 2 items (20.0%)\n",
            "  ✓ grocery: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '890' ---\n",
            "User's preferred categories (5): [np.str_('pet_supplies'), np.str_('jewelry'), np.str_('movies_tv'), np.str_('sports_outdoors'), np.str_('grocery')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ pet_supplies: 12 items (100.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "============================================================\n",
            "TEST 10/10: User ID '157'\n",
            "============================================================\n",
            "User profile:\n",
            "  - Preferred categories: [np.str_('beauty_health'), np.str_('sports_outdoors'), np.str_('pet_supplies'), np.str_('grocery')]\n",
            "  - Total interactions: 12\n",
            "\n",
            "--- Balanced recommendations for user '157' ---\n",
            "User's preferred categories (4): [np.str_('beauty_health'), np.str_('sports_outdoors'), np.str_('pet_supplies'), np.str_('grocery')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ beauty_health: 3 items (25.0%)\n",
            "  ✓ sports_outdoors: 3 items (25.0%)\n",
            "  ✓ pet_supplies: 3 items (25.0%)\n",
            "  ✓ grocery: 3 items (25.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "--- Collaborative filtering for user '157' ---\n",
            "User's preferred categories (4): [np.str_('beauty_health'), np.str_('sports_outdoors'), np.str_('pet_supplies'), np.str_('grocery')]\n",
            "Generated 12 recommendations\n",
            "Category distribution:\n",
            "  ✓ grocery: 12 items (100.0%)\n",
            "Accuracy: 12/12 (100.0%)\n",
            "\n",
            "================================================================================\n",
            "OVERALL RESULTS\n",
            "================================================================================\n",
            "Balanced Approach:\n",
            "  Average accuracy: 100.0% ± 0.0%\n",
            "  Range: 100.0% - 100.0%\n",
            "Collaborative Filtering:\n",
            "  Average accuracy: 50.0% ± 50.0%\n",
            "  Range: 0.0% - 100.0%\n",
            "Expected random accuracy: 20.1%\n",
            "✓ Balanced approach performs 100.0% better\n",
            "\n",
            "================================================================================\n",
            "SYSTEM ANALYSIS COMPLETE\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "CATEGORY ANALYSIS\n",
            "============================================================\n",
            "Category popularity (by interactions):\n",
            "   1. shoes          :  736 interactions, 241 users\n",
            "   2. office_supplies:  704 interactions, 227 users\n",
            "   3. grocery        :  689 interactions, 226 users\n",
            "   4. watches        :  677 interactions, 214 users\n",
            "   5. clothing       :  653 interactions, 209 users\n",
            "   6. baby_products  :  623 interactions, 213 users\n",
            "   7. beauty_health  :  621 interactions, 210 users\n",
            "   8. books          :  611 interactions, 208 users\n",
            "   9. home_garden    :  604 interactions, 208 users\n",
            "  10. jewelry        :  604 interactions, 197 users\n",
            "\n",
            "Least popular categories:\n",
            "  kitchen_dining :  564 interactions, 197 users\n",
            "  industrial     :  543 interactions, 179 users\n",
            "  pet_supplies   :  537 interactions, 177 users\n",
            "  automotive     :  510 interactions, 175 users\n",
            "  movies_tv      :  460 interactions, 163 users\n"
          ]
        }
      ]
    }
  ]
}