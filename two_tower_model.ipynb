{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhaj-mhd/two-tower-recommedation/blob/main/two_tower_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSb8v80Z2v8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#install packages\n"
      ],
      "metadata": {
        "id": "M-cUjNfc2wZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6rQ19NkZo8s",
        "outputId": "f9adb6d2-e5d3-49f8-c82c-772c7f6fc4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Installing and upgrading all required packages...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "✅ All packages have been installed and upgraded.\n"
          ]
        }
      ],
      "source": [
        "print(\"⏳ Installing and upgrading all required packages...\")\n",
        "\n",
        "%pip install --upgrade -q tensorflow tensorflow-recommenders tf-keras tensorflow-text\n",
        "%pip install -q faiss-cpu\n",
        "\n",
        "print(\"\\n✅ All packages have been installed and upgraded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbdXsqBCabJ4",
        "outputId": "d3653e2d-da73-49ab-8b3a-d46fcdeeb7f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade -q tensorflow-decision-forests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "_roixOESalk5",
        "outputId": "307d9266-918c-4858-90e6-178e3c959d44",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"margin:0px;\">🌲 Try <a href=\"https://ydf.readthedocs.io/en/latest/\" target=\"_blank\">YDF</a>, the successor of\n",
              "    <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow\n",
              "        Decision Forests</a> using the same algorithms but with more features and faster\n",
              "    training!\n",
              "</p>\n",
              "<div style=\"display: flex; flex-wrap: wrap; margin:5px;max-width: 880px;\">\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            Old code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import tensorflow_decision_forests as tfdf\n",
              "\n",
              "tf_ds = tfdf.keras.pd_dataframe_to_tf_dataset(ds, label=\"l\")\n",
              "model = tfdf.keras.RandomForestModel(label=\"l\")\n",
              "model.fit(tf_ds)\n",
              "</pre>\n",
              "    </div>\n",
              "    <div style=\"width: 5px;\"></div>\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            New code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import ydf\n",
              "\n",
              "model = ydf.RandomForestLearner(label=\"l\").train(ds)\n",
              "</pre>\n",
              "    </div>\n",
              "</div>\n",
              "<p style=\"margin:0px;font-size: 9pt;\">(Learn more in the <a\n",
              "        href=\"https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/\" target=\"_blank\">migration\n",
              "        guide</a>)</p>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow: 2.19.0\n",
            "tensorflow-recommenders: v0.7.3\n",
            "tf-keras: 2.19.0\n",
            "faiss-cpu: 1.11.0\n",
            "tensorflow-text: 2.19.0\n",
            "tensorflow-decision-forests: 1.12.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "import tf_keras\n",
        "import faiss\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"tensorflow-recommenders: {tfrs.__version__}\")\n",
        "print(f\"tf-keras: {tf_keras.__version__}\")\n",
        "print(f\"faiss-cpu: {faiss.__version__}\")\n",
        "print(f\"tensorflow-text: {tf_text.__version__}\")\n",
        "print(f\"tensorflow-decision-forests: {tfdf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Two tower model"
      ],
      "metadata": {
        "id": "FMnfU2bI2cdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "\n"
      ],
      "metadata": {
        "id": "_SUBQRpK4Boa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Fabricate Data with 20 Categories and Enhanced User Demographics ---\n",
        "print(\"[1] Fabricating data with 20 categories and enhanced user demographics...\")\n",
        "\n",
        "# 20 diverse categories\n",
        "categories = [\n",
        "    \"electronics\", \"clothing\", \"books\", \"home_garden\", \"sports_outdoors\",\n",
        "    \"beauty_health\", \"automotive\", \"toys_games\", \"jewelry\", \"music\",\n",
        "    \"movies_tv\", \"kitchen_dining\", \"office_supplies\", \"pet_supplies\", \"crafts\",\n",
        "    \"industrial\", \"grocery\", \"baby_products\", \"shoes\", \"watches\"\n",
        "]\n",
        "\n",
        "# User demographics data\n",
        "age_groups = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"]\n",
        "locations = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\",\n",
        "            \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\", \"Austin\", \"Jacksonville\",\n",
        "            \"Fort Worth\", \"Columbus\", \"Charlotte\", \"Seattle\", \"Denver\", \"Boston\"]\n",
        "genders = [\"Male\", \"Female\", \"Other\"]\n",
        "\n",
        "num_items = 10000  # Increased items to accommodate more categories\n",
        "num_users = 1000   # Increased users for better diversity\n",
        "\n",
        "# Create items with explicit category tracking\n",
        "item_titles = [f\"Product {i}\" for i in range(num_items)]\n",
        "item_categories = [categories[i % len(categories)] for i in range(num_items)]\n",
        "\n",
        "# Create diverse descriptions based on category\n",
        "description_templates = {\n",
        "    \"electronics\": [\n",
        "        \"High-tech electronic device with advanced features\",\n",
        "        \"Smart gadget with wireless connectivity and AI integration\",\n",
        "        \"Innovative electronic tool for modern digital life\",\n",
        "        \"Premium electronic device with cutting-edge technology\"\n",
        "    ],\n",
        "    \"clothing\": [\n",
        "        \"Fashionable apparel made from quality sustainable materials\",\n",
        "        \"Comfortable and stylish garment for everyday wear\",\n",
        "        \"Trendy clothing with modern design and premium fabric\",\n",
        "        \"Versatile wardrobe piece suitable for various occasions\"\n",
        "    ],\n",
        "    \"books\": [\n",
        "        \"Educational book covering important academic topics\",\n",
        "        \"Engaging literature for avid readers and book enthusiasts\",\n",
        "        \"Informative guide with practical knowledge and insights\",\n",
        "        \"Bestselling book with compelling storytelling and research\"\n",
        "    ],\n",
        "    \"home_garden\": [\n",
        "        \"Durable home improvement tool for DIY projects\",\n",
        "        \"Garden equipment for landscaping and plant care\",\n",
        "        \"Home decor item to enhance living space aesthetics\",\n",
        "        \"Functional household item for daily convenience\"\n",
        "    ],\n",
        "    \"sports_outdoors\": [\n",
        "        \"Professional sports equipment for athletic performance\",\n",
        "        \"Outdoor gear for adventure and recreational activities\",\n",
        "        \"Fitness equipment for home workout routines\",\n",
        "        \"Camping and hiking essentials for outdoor enthusiasts\"\n",
        "    ],\n",
        "    \"beauty_health\": [\n",
        "        \"Premium skincare product with natural ingredients\",\n",
        "        \"Health supplement for wellness and vitality\",\n",
        "        \"Cosmetic item for beauty enhancement and self-care\",\n",
        "        \"Personal care product for daily hygiene routine\"\n",
        "    ],\n",
        "    \"automotive\": [\n",
        "        \"High-quality automotive part for vehicle maintenance\",\n",
        "        \"Car accessory for enhanced driving experience\",\n",
        "        \"Professional-grade tool for automotive repair\",\n",
        "        \"Vehicle enhancement product for performance optimization\"\n",
        "    ],\n",
        "    \"toys_games\": [\n",
        "        \"Educational toy for children's development and learning\",\n",
        "        \"Board game for family entertainment and bonding\",\n",
        "        \"Creative plaything that sparks imagination and creativity\",\n",
        "        \"Interactive game for skill development and fun\"\n",
        "    ],\n",
        "    \"jewelry\": [\n",
        "        \"Elegant jewelry piece crafted with precious metals\",\n",
        "        \"Stylish accessory for fashion and personal expression\",\n",
        "        \"Handcrafted jewelry with unique design elements\",\n",
        "        \"Luxury jewelry item for special occasions\"\n",
        "    ],\n",
        "    \"music\": [\n",
        "        \"Professional music equipment for audio production\",\n",
        "        \"Musical instrument for creative expression and performance\",\n",
        "        \"High-quality audio device for music enthusiasts\",\n",
        "        \"Music accessory for enhanced listening experience\"\n",
        "    ],\n",
        "    \"movies_tv\": [\n",
        "        \"Entertainment media for leisure and relaxation\",\n",
        "        \"Classic film collection for movie enthusiasts\",\n",
        "        \"TV series with compelling storylines and characters\",\n",
        "        \"Documentary content for educational entertainment\"\n",
        "    ],\n",
        "    \"kitchen_dining\": [\n",
        "        \"Professional kitchen utensil for culinary excellence\",\n",
        "        \"Dining accessory for elegant meal presentation\",\n",
        "        \"Cooking tool made from premium food-safe materials\",\n",
        "        \"Kitchen gadget for efficient food preparation\"\n",
        "    ],\n",
        "    \"office_supplies\": [\n",
        "        \"Professional office equipment for workplace productivity\",\n",
        "        \"Stationery item for organization and documentation\",\n",
        "        \"Ergonomic office accessory for comfort and efficiency\",\n",
        "        \"Business tool for professional operations\"\n",
        "    ],\n",
        "    \"pet_supplies\": [\n",
        "        \"Pet care product for animal health and happiness\",\n",
        "        \"Pet toy for entertainment and exercise\",\n",
        "        \"Pet accessory for comfort and safety\",\n",
        "        \"Pet nutrition product for optimal health\"\n",
        "    ],\n",
        "    \"crafts\": [\n",
        "        \"Art supply for creative projects and expression\",\n",
        "        \"Craft material for DIY projects and hobbies\",\n",
        "        \"Creative tool for artistic endeavors and crafting\",\n",
        "        \"Handcraft supply for personalized creations\"\n",
        "    ],\n",
        "    \"industrial\": [\n",
        "        \"Industrial equipment for manufacturing and production\",\n",
        "        \"Heavy-duty tool for professional industrial use\",\n",
        "        \"Machinery component for industrial operations\",\n",
        "        \"Professional-grade equipment for industrial applications\"\n",
        "    ],\n",
        "    \"grocery\": [\n",
        "        \"Premium food product for nutritious meals\",\n",
        "        \"Organic ingredient for healthy cooking\",\n",
        "        \"Gourmet food item for culinary excellence\",\n",
        "        \"Essential grocery item for daily nutrition\"\n",
        "    ],\n",
        "    \"baby_products\": [\n",
        "        \"Safe baby product for infant care and development\",\n",
        "        \"Baby accessory for comfort and convenience\",\n",
        "        \"Child safety item for protection and security\",\n",
        "        \"Developmental toy for early childhood learning\"\n",
        "    ],\n",
        "    \"shoes\": [\n",
        "        \"Comfortable footwear for daily wear and activities\",\n",
        "        \"Athletic shoe for sports and fitness activities\",\n",
        "        \"Fashion shoe for style and personal expression\",\n",
        "        \"Professional footwear for workplace requirements\"\n",
        "    ],\n",
        "    \"watches\": [\n",
        "        \"Precision timepiece with advanced features\",\n",
        "        \"Luxury watch for style and status\",\n",
        "        \"Sports watch for active lifestyle tracking\",\n",
        "        \"Smart watch with digital connectivity features\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "item_descriptions = []\n",
        "for i in range(num_items):\n",
        "    category = item_categories[i]\n",
        "    template = description_templates[category][i % len(description_templates[category])]\n",
        "    item_descriptions.append(f\"{template}. Model v{i % 15}. Item #{i}.\")\n",
        "\n",
        "items_data = {\n",
        "    \"item_id\": [str(i) for i in range(num_items)],\n",
        "    \"item_title\": item_titles,\n",
        "    \"item_description\": item_descriptions,\n",
        "    \"category\": item_categories\n",
        "}\n",
        "items_df = pd.DataFrame(items_data)\n",
        "\n",
        "# Create category-to-items mapping for easier lookup\n",
        "category_to_items = defaultdict(list)\n",
        "for idx, row in items_df.iterrows():\n",
        "    category_to_items[row['category']].append(row['item_id'])\n",
        "\n",
        "# Generate enhanced user demographics data\n",
        "print(\"Generating enhanced user demographics...\")\n",
        "user_demographics = []\n",
        "for user_id in range(num_users):\n",
        "    # Generate demographics with realistic distributions\n",
        "    age_group = np.random.choice(age_groups, p=[0.15, 0.25, 0.22, 0.18, 0.12, 0.08])  # Weighted towards younger users\n",
        "    location = np.random.choice(locations)\n",
        "    gender = np.random.choice(genders, p=[0.48, 0.50, 0.02])  # Realistic gender distribution\n",
        "\n",
        "    user_demographics.append({\n",
        "        \"user_id\": str(user_id),\n",
        "        \"age_group\": age_group,\n",
        "        \"location\": location,\n",
        "        \"gender\": gender\n",
        "    })\n",
        "\n",
        "users_df = pd.DataFrame(user_demographics)\n",
        "\n",
        "# Generate user interactions with demographic influence on preferences\n",
        "print(\"Generating user interactions with demographic-influenced preferences...\")\n",
        "user_interactions = []\n",
        "user_categories = {}  # Track which categories each user prefers\n",
        "\n",
        "# Define demographic preferences (realistic patterns)\n",
        "demographic_preferences = {\n",
        "    \"age_group\": {\n",
        "        \"18-24\": {\"electronics\": 1.5, \"clothing\": 1.4, \"music\": 1.3, \"toys_games\": 1.2},\n",
        "        \"25-34\": {\"electronics\": 1.3, \"home_garden\": 1.2, \"clothing\": 1.2, \"automotive\": 1.1},\n",
        "        \"35-44\": {\"home_garden\": 1.4, \"automotive\": 1.3, \"office_supplies\": 1.2, \"baby_products\": 1.5},\n",
        "        \"45-54\": {\"home_garden\": 1.3, \"automotive\": 1.2, \"books\": 1.2, \"kitchen_dining\": 1.1},\n",
        "        \"55-64\": {\"books\": 1.3, \"home_garden\": 1.2, \"kitchen_dining\": 1.2, \"beauty_health\": 1.1},\n",
        "        \"65+\": {\"books\": 1.4, \"home_garden\": 1.3, \"beauty_health\": 1.2, \"kitchen_dining\": 1.1}\n",
        "    },\n",
        "    \"gender\": {\n",
        "        \"Male\": {\"electronics\": 1.3, \"automotive\": 1.4, \"sports_outdoors\": 1.3, \"tools\": 1.2},\n",
        "        \"Female\": {\"clothing\": 1.4, \"beauty_health\": 1.5, \"jewelry\": 1.3, \"baby_products\": 1.2},\n",
        "        \"Other\": {\"clothing\": 1.1, \"electronics\": 1.1, \"books\": 1.2, \"music\": 1.1}\n",
        "    }\n",
        "}\n",
        "\n",
        "for user_id in range(num_users):\n",
        "    user_demo = users_df[users_df['user_id'] == str(user_id)].iloc[0]\n",
        "\n",
        "    # Calculate category preferences based on demographics\n",
        "    category_scores = {}\n",
        "    for category in categories:\n",
        "        base_score = 1.0\n",
        "\n",
        "        # Age group influence\n",
        "        age_prefs = demographic_preferences[\"age_group\"].get(user_demo['age_group'], {})\n",
        "        age_multiplier = age_prefs.get(category, 1.0)\n",
        "\n",
        "        # Gender influence\n",
        "        gender_prefs = demographic_preferences[\"gender\"].get(user_demo['gender'], {})\n",
        "        gender_multiplier = gender_prefs.get(category, 1.0)\n",
        "\n",
        "        # Combine influences\n",
        "        category_scores[category] = base_score * age_multiplier * gender_multiplier\n",
        "\n",
        "    # Select 3-5 categories based on weighted preferences\n",
        "    num_categories = np.random.randint(3, 6)\n",
        "    category_weights = np.array([category_scores[cat] for cat in categories])\n",
        "    category_weights = category_weights / np.sum(category_weights)  # Normalize\n",
        "\n",
        "    preferred_categories = np.random.choice(categories, size=num_categories, replace=False, p=category_weights)\n",
        "    user_categories[str(user_id)] = preferred_categories\n",
        "\n",
        "    # Generate interactions for each preferred category\n",
        "    for category in preferred_categories:\n",
        "        # Random number of interactions per category (1-5)\n",
        "        num_interactions = np.random.randint(1, 6)  # 1 to 5 interactions\n",
        "\n",
        "        # Select random items from this category\n",
        "        available_items = category_to_items[category]\n",
        "        selected_items = np.random.choice(available_items, size=num_interactions, replace=True)\n",
        "\n",
        "        for item_id in selected_items:\n",
        "            user_interactions.append({\n",
        "                \"user_id\": str(user_id),\n",
        "                \"item_id\": item_id\n",
        "            })\n",
        "\n",
        "interactions_df = pd.DataFrame(user_interactions)\n",
        "\n",
        "# Calculate statistics\n",
        "total_interactions = len(interactions_df)\n",
        "avg_interactions_per_user = total_interactions / num_users\n",
        "\n",
        "print(f\"Generated {len(items_df)} items across {len(categories)} categories\")\n",
        "print(f\"Generated {total_interactions} interactions from {num_users} users\")\n",
        "print(f\"Average interactions per user: {avg_interactions_per_user:.1f}\")\n",
        "\n",
        "# Display demographic distribution\n",
        "print(f\"\\nUser demographic distribution:\")\n",
        "print(\"Age groups:\")\n",
        "for age_group in age_groups:\n",
        "    count = len(users_df[users_df['age_group'] == age_group])\n",
        "    print(f\"  {age_group}: {count} users ({count/num_users:.1%})\")\n",
        "\n",
        "print(\"Gender distribution:\")\n",
        "for gender in genders:\n",
        "    count = len(users_df[users_df['gender'] == gender])\n",
        "    print(f\"  {gender}: {count} users ({count/num_users:.1%})\")\n",
        "\n",
        "# Display sample user profiles\n",
        "print(\"\\nSample user profiles:\")\n",
        "for i in range(5):\n",
        "    user_id = str(i)\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    cats = user_categories[user_id]\n",
        "    user_interactions_count = len(interactions_df[interactions_df['user_id'] == user_id])\n",
        "    print(f\"User {user_id}: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"  Categories: {list(cats)} ({user_interactions_count} interactions)\")\n",
        "\n",
        "items_ds = tf.data.Dataset.from_tensor_slices(dict(items_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bB1HYdZU39Ss",
        "outputId": "36c93216-aebb-4dd3-8ec8-800ce802c47d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Fabricating data with 20 categories and enhanced user demographics...\n",
            "Generating enhanced user demographics...\n",
            "Generating user interactions with demographic-influenced preferences...\n",
            "Generated 10000 items across 20 categories\n",
            "Generated 12405 interactions from 1000 users\n",
            "Average interactions per user: 12.4\n",
            "\n",
            "User demographic distribution:\n",
            "Age groups:\n",
            "  18-24: 144 users (14.4%)\n",
            "  25-34: 250 users (25.0%)\n",
            "  35-44: 219 users (21.9%)\n",
            "  45-54: 181 users (18.1%)\n",
            "  55-64: 114 users (11.4%)\n",
            "  65+: 92 users (9.2%)\n",
            "Gender distribution:\n",
            "  Male: 471 users (47.1%)\n",
            "  Female: 501 users (50.1%)\n",
            "  Other: 28 users (2.8%)\n",
            "\n",
            "Sample user profiles:\n",
            "User 0: 35-44, Male, San Jose\n",
            "  Categories: [np.str_('electronics'), np.str_('music'), np.str_('books'), np.str_('industrial')] (13 interactions)\n",
            "User 1: 65+, Male, Denver\n",
            "  Categories: [np.str_('kitchen_dining'), np.str_('sports_outdoors'), np.str_('books')] (6 interactions)\n",
            "User 2: 25-34, Male, Phoenix\n",
            "  Categories: [np.str_('electronics'), np.str_('shoes'), np.str_('office_supplies'), np.str_('sports_outdoors'), np.str_('automotive')] (19 interactions)\n",
            "User 3: 45-54, Female, Jacksonville\n",
            "  Categories: [np.str_('kitchen_dining'), np.str_('office_supplies'), np.str_('clothing'), np.str_('toys_games')] (9 interactions)\n",
            "User 4: 25-34, Male, Charlotte\n",
            "  Categories: [np.str_('industrial'), np.str_('automotive'), np.str_('electronics'), np.str_('home_garden')] (10 interactions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Enhanced Item Tower with Category Integration ---\n",
        "print(\"\\n[2] Building and training the enhanced Item Tower with category integration...\")\n",
        "embedding_dimension = 128\n",
        "max_tokens = 15_000\n",
        "sequence_length = 120\n",
        "\n",
        "# Create text vectorizers for both description and category\n",
        "description_vectorizer = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_sequence_length=sequence_length,\n",
        "    name=\"description_vectorizer\"\n",
        ")\n",
        "description_vectorizer.adapt(items_ds.map(lambda x: x[\"item_description\"]).batch(256))\n",
        "\n",
        "# Create category vectorizer (much smaller vocabulary)\n",
        "category_vectorizer = TextVectorization(\n",
        "    max_tokens=len(categories) + 10,  # Small vocabulary for categories\n",
        "    output_sequence_length=1,  # Categories are single tokens\n",
        "    name=\"category_vectorizer\"\n",
        ")\n",
        "category_vectorizer.adapt(items_ds.map(lambda x: x[\"category\"]).batch(256))\n",
        "\n",
        "class EnhancedItemModel(tf.keras.Model):\n",
        "    def __init__(self, description_vectorizer, category_vectorizer, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.description_vectorizer = description_vectorizer\n",
        "        self.category_vectorizer = category_vectorizer\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Description embedding branch\n",
        "        self.description_embedding = tf.keras.Sequential([\n",
        "            self.description_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.description_vectorizer.vocabulary_size(),\n",
        "                output_dim=embedding_dim,\n",
        "                mask_zero=True,\n",
        "                name=\"description_embedding\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(name=\"description_pooling\"),\n",
        "        ], name=\"description_branch\")\n",
        "\n",
        "        # Category embedding branch\n",
        "        self.category_embedding = tf.keras.Sequential([\n",
        "            self.category_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.category_vectorizer.vocabulary_size(),\n",
        "                output_dim=32,  # Smaller embedding for categories\n",
        "                mask_zero=True,\n",
        "                name=\"category_embedding\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(name=\"category_pooling\"),\n",
        "        ], name=\"category_branch\")\n",
        "\n",
        "        # Fusion layer to combine description and category embeddings\n",
        "        self.fusion_dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\", name=\"fusion_dense_1\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"fusion_bn_1\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"fusion_dropout_1\"),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\", name=\"fusion_dense_2\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"fusion_bn_2\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"fusion_dropout_2\"),\n",
        "            tf.keras.layers.Dense(embedding_dim, name=\"fusion_output\")\n",
        "        ], name=\"fusion_layer\")\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Process description\n",
        "        description_emb = self.description_embedding(inputs[\"item_description\"])\n",
        "\n",
        "        # Process category\n",
        "        category_emb = self.category_embedding(inputs[\"category\"])\n",
        "\n",
        "        # Concatenate description and category embeddings\n",
        "        combined = tf.concat([description_emb, category_emb], axis=1)\n",
        "\n",
        "        # Apply fusion layer\n",
        "        output = self.fusion_dense(combined, training=training)\n",
        "\n",
        "        return output\n",
        "\n",
        "class SelfSupervisedItemTwoTower(tfrs.Model):\n",
        "    def __init__(self, item_model):\n",
        "        super().__init__()\n",
        "        self.item_model = item_model\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        item_embeddings = self.item_model(features, training=training)\n",
        "        return self.task(query_embeddings=item_embeddings, candidate_embeddings=item_embeddings)\n",
        "\n",
        "# Initialize the enhanced item tower\n",
        "item_tower = EnhancedItemModel(description_vectorizer, category_vectorizer, embedding_dimension)\n",
        "item_model_trainer = SelfSupervisedItemTwoTower(item_tower)\n",
        "item_model_trainer.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "# Prepare training data with both description and category\n",
        "train_item_ds = items_ds.map(lambda x: {\n",
        "    \"item_description\": x[\"item_description\"],\n",
        "    \"category\": x[\"category\"]\n",
        "}).batch(512).cache()\n",
        "\n",
        "print(\"Training Enhanced Item Tower with description and category...\")\n",
        "item_model_trainer.fit(train_item_ds, epochs=15, verbose=1)\n",
        "print(\"Enhanced Item Tower training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9qVgZbUs4Ruk",
        "outputId": "78fbf7cd-643e-4afe-8aa0-88ed2bfad13f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2] Building and training the enhanced Item Tower with category integration...\n",
            "Training Enhanced Item Tower with description and category...\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - loss: 4.6075 - regularization_loss: 0.0000e+00 - total_loss: 4.6075\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 2.3443 - regularization_loss: 0.0000e+00 - total_loss: 2.3443\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - loss: 1.2802 - regularization_loss: 0.0000e+00 - total_loss: 1.2802\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 0.8636 - regularization_loss: 0.0000e+00 - total_loss: 0.8636\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 1.1293 - regularization_loss: 0.0000e+00 - total_loss: 1.1293\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 1.2937 - regularization_loss: 0.0000e+00 - total_loss: 1.2937\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 3.1955 - regularization_loss: 0.0000e+00 - total_loss: 3.1955\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 3.1839 - regularization_loss: 0.0000e+00 - total_loss: 3.1839\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 1.0777 - regularization_loss: 0.0000e+00 - total_loss: 1.0777\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - loss: 0.8311 - regularization_loss: 0.0000e+00 - total_loss: 0.8311\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0788 - regularization_loss: 0.0000e+00 - total_loss: 0.0788\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.8275 - regularization_loss: 0.0000e+00 - total_loss: 0.8275\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.2656 - regularization_loss: 0.0000e+00 - total_loss: 0.2656\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.0435 - regularization_loss: 0.0000e+00 - total_loss: 0.0435\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.2885 - regularization_loss: 0.0000e+00 - total_loss: 0.2885\n",
            "Enhanced Item Tower training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Generate and Store Item Embeddings in Faiss ---\n",
        "print(\"\\n[3] Generating item embeddings and storing in Faiss...\")\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "# Generate embeddings using both description and category\n",
        "item_embeddings_generator = items_ds.batch(512).map(lambda x: item_tower({\n",
        "    \"item_description\": x[\"item_description\"],\n",
        "    \"category\": x[\"category\"]\n",
        "}))\n",
        "all_item_embeddings = np.concatenate(list(item_embeddings_generator.as_numpy_iterator()))\n",
        "\n",
        "# Normalize embeddings for better similarity search\n",
        "all_item_embeddings = all_item_embeddings / np.linalg.norm(all_item_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "index.add(all_item_embeddings)\n",
        "print(f\"Faiss index now contains {index.ntotal} vectors.\")\n",
        "index_to_item_id = {i: item_id for i, item_id in enumerate(items_df[\"item_id\"])}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e69zBRe4UzL",
        "outputId": "5caf76bb-691e-4003-9802-80b388023326"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3] Generating item embeddings and storing in Faiss...\n",
            "Faiss index now contains 10000 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Step 4: Enhanced User Tower with Demographics ---\n",
        "print(\"\\n[4] Building and training the Enhanced User Tower with demographics...\")\n",
        "unique_user_ids = interactions_df[\"user_id\"].unique()\n",
        "\n",
        "# Create dataset from user demographics\n",
        "users_ds = tf.data.Dataset.from_tensor_slices(dict(users_df))\n",
        "\n",
        "# Create vectorizers for demographic features\n",
        "age_group_vectorizer = TextVectorization(\n",
        "    max_tokens=len(age_groups) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"age_group_vectorizer\"\n",
        ")\n",
        "age_group_vectorizer.adapt(users_ds.map(lambda x: x[\"age_group\"]).batch(256))\n",
        "\n",
        "location_vectorizer = TextVectorization(\n",
        "    max_tokens=len(locations) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"location_vectorizer\"\n",
        ")\n",
        "location_vectorizer.adapt(users_ds.map(lambda x: x[\"location\"]).batch(256))\n",
        "\n",
        "gender_vectorizer = TextVectorization(\n",
        "    max_tokens=len(genders) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"gender_vectorizer\"\n",
        ")\n",
        "gender_vectorizer.adapt(users_ds.map(lambda x: x[\"gender\"]).batch(256))\n",
        "\n",
        "class EnhancedUserModel(tf.keras.Model):\n",
        "    def __init__(self, user_ids, age_group_vectorizer, location_vectorizer, gender_vectorizer):\n",
        "        super().__init__()\n",
        "        self.age_group_vectorizer = age_group_vectorizer\n",
        "        self.location_vectorizer = location_vectorizer\n",
        "        self.gender_vectorizer = gender_vectorizer\n",
        "\n",
        "        # User ID embedding\n",
        "        self.user_id_embedding = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=user_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(user_ids) + 1, 64, name=\"user_id_emb\")\n",
        "        ], name=\"user_id_branch\")\n",
        "\n",
        "        # Age group embedding\n",
        "        self.age_group_embedding = tf.keras.Sequential([\n",
        "            self.age_group_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.age_group_vectorizer.vocabulary_size(),\n",
        "                output_dim=16,\n",
        "                mask_zero=True,\n",
        "                name=\"age_group_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"age_group_branch\")\n",
        "\n",
        "        # Location embedding\n",
        "        self.location_embedding = tf.keras.Sequential([\n",
        "            self.location_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.location_vectorizer.vocabulary_size(),\n",
        "                output_dim=32,\n",
        "                mask_zero=True,\n",
        "                name=\"location_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"location_branch\")\n",
        "\n",
        "        # Gender embedding\n",
        "        self.gender_embedding = tf.keras.Sequential([\n",
        "            self.gender_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.gender_vectorizer.vocabulary_size(),\n",
        "                output_dim=8,\n",
        "                mask_zero=True,\n",
        "                name=\"gender_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"gender_branch\")\n",
        "\n",
        "        # Fusion layers to combine all user features\n",
        "        self.fusion_dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\", name=\"user_fusion_dense_1\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"user_fusion_bn_1\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"user_fusion_dropout_1\"),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\", name=\"user_fusion_dense_2\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"user_fusion_bn_2\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"user_fusion_dropout_2\"),\n",
        "            tf.keras.layers.Dense(embedding_dimension, name=\"user_fusion_output\")\n",
        "        ], name=\"user_fusion_layer\")\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Process user ID\n",
        "        user_id_emb = self.user_id_embedding(inputs[\"user_id\"])\n",
        "\n",
        "        # Process demographics\n",
        "        age_group_emb = self.age_group_embedding(inputs[\"age_group\"])\n",
        "        location_emb = self.location_embedding(inputs[\"location\"])\n",
        "        gender_emb = self.gender_embedding(inputs[\"gender\"])\n",
        "\n",
        "        # Concatenate all user features\n",
        "        combined = tf.concat([user_id_emb, age_group_emb, location_emb, gender_emb], axis=1)\n",
        "\n",
        "        # Apply fusion layer\n",
        "        output = self.fusion_dense(combined, training=training)\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnhancedUserItemRetrievalModel(tfrs.Model):\n",
        "    def __init__(self, user_model, item_model):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.item_model = item_model\n",
        "        self.item_model.trainable = False  # Keep item model frozen\n",
        "\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, data, training=False):\n",
        "        user_embeddings = self.user_model(data, training=training)\n",
        "\n",
        "        # Get item embeddings for the interacted items\n",
        "        item_data = {\n",
        "            \"item_description\": data[\"item_description\"],\n",
        "            \"category\": data[\"category\"]\n",
        "        }\n",
        "        item_embeddings = self.item_model(item_data, training=False)\n",
        "\n",
        "        # Normalize embeddings\n",
        "        user_embeddings = tf.nn.l2_normalize(user_embeddings, axis=1)\n",
        "        item_embeddings = tf.nn.l2_normalize(item_embeddings, axis=1)\n",
        "\n",
        "        return self.task(\n",
        "            query_embeddings=user_embeddings,\n",
        "            candidate_embeddings=item_embeddings\n",
        "        )\n",
        "\n",
        "# Prepare training data with user demographics and item details\n",
        "interactions_with_details_df = pd.merge(\n",
        "    interactions_df,\n",
        "    items_df[['item_id', 'item_description', 'category']],\n",
        "    on='item_id'\n",
        ")\n",
        "\n",
        "# Add user demographics to interactions\n",
        "interactions_with_details_df = pd.merge(\n",
        "    interactions_with_details_df,\n",
        "    users_df[['user_id', 'age_group', 'location', 'gender']],\n",
        "    on='user_id'\n",
        ")\n",
        "\n",
        "# Create negative sampling for better training\n",
        "print(\"Creating training dataset with negative sampling...\")\n",
        "positive_interactions = interactions_with_details_df.copy()\n",
        "positive_interactions['label'] = 1.0\n",
        "\n",
        "# Create negative samples\n",
        "negative_interactions = []\n",
        "for user_id in unique_user_ids:\n",
        "    user_positive_items = set(positive_interactions[positive_interactions['user_id'] == user_id]['item_id'])\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "\n",
        "    # Sample negative items (items the user hasn't interacted with)\n",
        "    num_negatives = min(len(user_positive_items), 20)  # Limit negatives to prevent memory issues\n",
        "    all_items = set(items_df['item_id'])\n",
        "    negative_items = list(all_items - user_positive_items)\n",
        "\n",
        "    if len(negative_items) >= num_negatives:\n",
        "        sampled_negatives = np.random.choice(negative_items, size=num_negatives, replace=False)\n",
        "\n",
        "        for item_id in sampled_negatives:\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            negative_interactions.append({\n",
        "                'user_id': user_id,\n",
        "                'item_id': item_id,\n",
        "                'item_description': item_row['item_description'],\n",
        "                'category': item_row['category'],\n",
        "                'age_group': user_demo['age_group'],\n",
        "                'location': user_demo['location'],\n",
        "                'gender': user_demo['gender'],\n",
        "                'label': 0.0\n",
        "            })\n",
        "\n",
        "negative_interactions_df = pd.DataFrame(negative_interactions)\n",
        "full_training_df = pd.concat([positive_interactions, negative_interactions_df], ignore_index=True)\n",
        "\n",
        "# Shuffle the training data\n",
        "full_training_df = full_training_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "full_interactions_ds = tf.data.Dataset.from_tensor_slices(dict(full_training_df))\n",
        "train_ds_user = full_interactions_ds.shuffle(50_000).batch(512).cache()\n",
        "\n",
        "user_tower = EnhancedUserModel(unique_user_ids, age_group_vectorizer, location_vectorizer, gender_vectorizer)\n",
        "user_model_trainer = EnhancedUserItemRetrievalModel(user_tower, item_tower)\n",
        "user_model_trainer.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "# Train the user model\n",
        "print(\"Training Enhanced User Tower with demographics...\")\n",
        "user_model_trainer.fit(train_ds_user, epochs=15, verbose=1)\n",
        "print(\"Enhanced User Tower training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMmQE4aH4Z4W",
        "outputId": "1dd687fb-139e-459b-e394-3a3fb3521459"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4] Building and training the Enhanced User Tower with demographics...\n",
            "Creating training dataset with negative sampling...\n",
            "Training Enhanced User Tower with demographics...\n",
            "Epoch 1/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - loss: 3099.0576 - regularization_loss: 0.0000e+00 - total_loss: 3099.0576\n",
            "Epoch 2/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 3094.3914 - regularization_loss: 0.0000e+00 - total_loss: 3094.3914\n",
            "Epoch 3/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3089.5247 - regularization_loss: 0.0000e+00 - total_loss: 3089.5247\n",
            "Epoch 4/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 3086.2178 - regularization_loss: 0.0000e+00 - total_loss: 3086.2178\n",
            "Epoch 5/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - loss: 3084.3528 - regularization_loss: 0.0000e+00 - total_loss: 3084.3528\n",
            "Epoch 6/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 3083.2097 - regularization_loss: 0.0000e+00 - total_loss: 3083.2097\n",
            "Epoch 7/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 3082.4958 - regularization_loss: 0.0000e+00 - total_loss: 3082.4958\n",
            "Epoch 8/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3081.7969 - regularization_loss: 0.0000e+00 - total_loss: 3081.7969\n",
            "Epoch 9/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 3081.3999 - regularization_loss: 0.0000e+00 - total_loss: 3081.3999\n",
            "Epoch 10/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 3081.0793 - regularization_loss: 0.0000e+00 - total_loss: 3081.0793\n",
            "Epoch 11/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 3080.6985 - regularization_loss: 0.0000e+00 - total_loss: 3080.6985\n",
            "Epoch 12/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 3080.4714 - regularization_loss: 0.0000e+00 - total_loss: 3080.4714\n",
            "Epoch 13/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 3080.2773 - regularization_loss: 0.0000e+00 - total_loss: 3080.2773\n",
            "Epoch 14/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 3080.1006 - regularization_loss: 0.0000e+00 - total_loss: 3080.1006\n",
            "Epoch 15/15\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 3079.9023 - regularization_loss: 0.0000e+00 - total_loss: 3079.9023\n",
            "Enhanced User Tower training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 5: Enhanced Recommendation Function with Demographics ---\n",
        "print(\"\\n[5] Implementing enhanced recommendation functions with demographics...\")\n",
        "\n",
        "def get_demographic_recommendations(user_id, top_k=15):\n",
        "    \"\"\"Get recommendations using enhanced user model with demographics\"\"\"\n",
        "    print(f\"\\n--- Enhanced demographic recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return None\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    user_embedding = user_tower(user_input).numpy()\n",
        "    user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Search for similar items using enhanced embeddings\n",
        "    search_k = min(top_k * 3, index.ntotal)\n",
        "    distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "    recommendations = []\n",
        "    for idx in indices[0]:\n",
        "        item_id = index_to_item_id[idx]\n",
        "        if item_id in user_interacted_items:\n",
        "            continue\n",
        "\n",
        "        item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "        recommendations.append({\n",
        "            'item_id': item_id,\n",
        "            'category': item_row['category'],\n",
        "            'title': item_row['item_title']\n",
        "        })\n",
        "\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "\n",
        "    # Calculate accuracy\n",
        "    category_counts = defaultdict(int)\n",
        "    correct_recommendations = 0\n",
        "\n",
        "    for rec in recommendations:\n",
        "        category_counts[rec['category']] += 1\n",
        "        if rec['category'] in preferred_categories:\n",
        "            correct_recommendations += 1\n",
        "\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stEEkN4o5cnk",
        "outputId": "ab4dae13-9661-4e50-c41e-040a9e0be3f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[5] Implementing enhanced recommendation functions with demographics...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_balanced_recommendations(user_id, top_k=15):\n",
        "    \"\"\"Get recommendations with explicit category balancing\"\"\"\n",
        "    print(f\"\\n--- Balanced recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return handle_cold_start_user(user_id, top_k)\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    user_embedding = user_tower(user_input).numpy()\n",
        "    user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Balanced recommendation approach\n",
        "    recommendations = []\n",
        "    category_counts = defaultdict(int)\n",
        "\n",
        "    # Calculate items per preferred category\n",
        "    items_per_category = max(1, top_k // len(preferred_categories))\n",
        "    remaining_slots = top_k - (items_per_category * len(preferred_categories))\n",
        "\n",
        "    print(f\"Target: {items_per_category} items per category, {remaining_slots} flexible slots\")\n",
        "\n",
        "    # For each preferred category, find the best items\n",
        "    for category in preferred_categories:\n",
        "        category_items = category_to_items[category]\n",
        "        category_item_embeddings = []\n",
        "        category_item_ids = []\n",
        "\n",
        "        # Get embeddings for items in this category\n",
        "        for item_id in category_items:\n",
        "            if item_id not in user_interacted_items:\n",
        "                idx = int(item_id)\n",
        "                category_item_embeddings.append(all_item_embeddings[idx])\n",
        "                category_item_ids.append(item_id)\n",
        "\n",
        "        if not category_item_embeddings:\n",
        "            continue\n",
        "\n",
        "        # Calculate similarities to user embedding\n",
        "        category_item_embeddings = np.array(category_item_embeddings)\n",
        "        similarities = np.dot(category_item_embeddings, user_embedding.T).flatten()\n",
        "\n",
        "        # Get top items for this category\n",
        "        top_indices = np.argsort(similarities)[::-1][:items_per_category]\n",
        "\n",
        "        for idx in top_indices:\n",
        "            item_id = category_item_ids[idx]\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            recommendations.append({\n",
        "                'item_id': item_id,\n",
        "                'category': item_row['category'],\n",
        "                'title': item_row['item_title'],\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "            category_counts[category] += 1\n",
        "\n",
        "    # Fill remaining slots with best overall recommendations\n",
        "    if remaining_slots > 0:\n",
        "        search_k = min(top_k * 3, index.ntotal)\n",
        "        distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "        added_items = set(rec['item_id'] for rec in recommendations)\n",
        "\n",
        "        for idx in indices[0]:\n",
        "            if remaining_slots <= 0:\n",
        "                break\n",
        "\n",
        "            item_id = index_to_item_id[idx]\n",
        "            if item_id in user_interacted_items or item_id in added_items:\n",
        "                continue\n",
        "\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            recommendations.append({\n",
        "                'item_id': item_id,\n",
        "                'category': item_row['category'],\n",
        "                'title': item_row['item_title'],\n",
        "                'similarity': 1.0 - distances[0][list(indices[0]).index(idx)]  # Convert distance to similarity\n",
        "            })\n",
        "            category_counts[item_row['category']] += 1\n",
        "            added_items.add(item_id)\n",
        "            remaining_slots -= 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_recommendations = sum(1 for rec in recommendations if rec['category'] in preferred_categories)\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "9M7kouKL5m1c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def handle_cold_start_user(user_id, top_k=15):\n",
        "    \"\"\"Handle recommendations for new users without interaction history\"\"\"\n",
        "    print(f\"Handling cold start for user '{user_id}'\")\n",
        "\n",
        "    # For cold start, use demographic-based recommendations\n",
        "    # This is a simplified approach - in practice, you might use popularity-based or content-based filtering\n",
        "\n",
        "    # Sample popular items from each category\n",
        "    popular_items = []\n",
        "    items_per_category = max(1, top_k // len(categories))\n",
        "\n",
        "    for category in categories[:min(len(categories), top_k)]:\n",
        "        category_items = category_to_items[category]\n",
        "        if category_items:\n",
        "            # For simplicity, take the first few items from each category\n",
        "            # In practice, you'd want to use popularity metrics\n",
        "            sample_size = min(items_per_category, len(category_items))\n",
        "            sampled_items = np.random.choice(category_items, size=sample_size, replace=False)\n",
        "\n",
        "            for item_id in sampled_items:\n",
        "                item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "                popular_items.append({\n",
        "                    'item_id': item_id,\n",
        "                    'category': item_row['category'],\n",
        "                    'title': item_row['item_title']\n",
        "                })\n",
        "\n",
        "    # Shuffle and limit to top_k\n",
        "    np.random.shuffle(popular_items)\n",
        "    popular_items = popular_items[:top_k]\n",
        "\n",
        "    print(f\"Generated {len(popular_items)} cold start recommendations\")\n",
        "    category_counts = defaultdict(int)\n",
        "    for item in popular_items:\n",
        "        category_counts[item['category']] += 1\n",
        "\n",
        "    print(\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(popular_items)) * 100 if popular_items else 0\n",
        "        print(f\"  {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    return 0.0  # Cold start accuracy is 0 since we don't know user preferences\n",
        "\n"
      ],
      "metadata": {
        "id": "e9DjVerf-Mhh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_category_aware_recommendations(user_id, top_k=15, category_diversity_weight=0.3):\n",
        "    \"\"\"Get recommendations with category diversity weighting - Fixed infinite loop issue\"\"\"\n",
        "    print(f\"\\n--- Category-aware recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return handle_cold_start_user(user_id, top_k)\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    # Convert numpy strings to regular strings to avoid comparison issues\n",
        "    preferred_categories = set(str(cat) for cat in preferred_categories)\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        user_embedding = user_tower(user_input).numpy()\n",
        "        user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting user embedding: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Get more candidates than needed for diversity selection\n",
        "    search_k = min(top_k * 5, index.ntotal)\n",
        "    distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "    # Build candidate pool with similarity scores\n",
        "    candidates = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        item_id = index_to_item_id[idx]\n",
        "        if item_id in user_interacted_items:\n",
        "            continue\n",
        "\n",
        "        item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "        similarity = 1.0 - distances[0][i]  # Convert distance to similarity\n",
        "\n",
        "        candidates.append({\n",
        "            'item_id': item_id,\n",
        "            'category': str(item_row['category']),  # Convert to string\n",
        "            'title': item_row['item_title'],\n",
        "            'similarity': similarity,\n",
        "            'is_preferred': str(item_row['category']) in preferred_categories\n",
        "        })\n",
        "\n",
        "    print(f\"Found {len(candidates)} candidates after filtering\")\n",
        "\n",
        "    if not candidates:\n",
        "        print(\"No candidates found!\")\n",
        "        return 0.0\n",
        "\n",
        "    # FIXED: Use a simpler, guaranteed-to-terminate selection algorithm\n",
        "    recommendations = []\n",
        "    category_counts = defaultdict(int)\n",
        "\n",
        "    # Sort candidates by adjusted score first\n",
        "    for candidate in candidates:\n",
        "        score = candidate['similarity']\n",
        "        if candidate['is_preferred']:\n",
        "            score *= 1.5\n",
        "        candidate['adjusted_score'] = score\n",
        "\n",
        "    # Sort by adjusted score (highest first)\n",
        "    candidates.sort(key=lambda x: x['adjusted_score'], reverse=True)\n",
        "\n",
        "    # Select with diversity constraints\n",
        "    for candidate in candidates:\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "\n",
        "        # Apply diversity penalty\n",
        "        category_penalty = category_counts[candidate['category']] * category_diversity_weight\n",
        "        final_score = candidate['adjusted_score'] - category_penalty\n",
        "\n",
        "        # Accept if it's still a good candidate or if we need more recommendations\n",
        "        if final_score > 0 or len(recommendations) < top_k // 2:\n",
        "            recommendations.append(candidate)\n",
        "            category_counts[candidate['category']] += 1\n",
        "\n",
        "    # If we still don't have enough, fill with remaining candidates\n",
        "    remaining_candidates = [c for c in candidates if c not in recommendations]\n",
        "    for candidate in remaining_candidates:\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "        recommendations.append(candidate)\n",
        "        category_counts[candidate['category']] += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_recommendations = sum(1 for rec in recommendations if rec['is_preferred'])\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "kg778q9c5sVW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 6: Comprehensive Evaluation ---\n",
        "print(\"\\n[6] Comprehensive evaluation of recommendation approaches...\")\n",
        "\n",
        "def evaluate_recommendation_approaches():\n",
        "    \"\"\"Evaluate different recommendation approaches\"\"\"\n",
        "    print(\"\\n=== COMPREHENSIVE RECOMMENDATION EVALUATION ===\")\n",
        "\n",
        "    # Select diverse test users\n",
        "    test_users = np.random.choice(unique_user_ids, size=min(10, len(unique_user_ids)), replace=False)\n",
        "\n",
        "    results = {\n",
        "        'demographic': [],\n",
        "        'balanced': [],\n",
        "        'category_aware': []\n",
        "    }\n",
        "\n",
        "    for user_id in test_users:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EVALUATING USER {user_id}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Test demographic recommendations\n",
        "        try:\n",
        "            accuracy_demo = get_demographic_recommendations(user_id, top_k=10)\n",
        "            results['demographic'].append(accuracy_demo)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in demographic recommendations: {e}\")\n",
        "            results['demographic'].append(0.0)\n",
        "\n",
        "        # Test balanced recommendations\n",
        "        try:\n",
        "            accuracy_balanced = get_balanced_recommendations(user_id, top_k=10)\n",
        "            results['balanced'].append(accuracy_balanced)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in balanced recommendations: {e}\")\n",
        "            results['balanced'].append(0.0)\n",
        "\n",
        "        # Test category-aware recommendations\n",
        "        try:\n",
        "            accuracy_category = get_category_aware_recommendations(user_id, top_k=10)\n",
        "            results['category_aware'].append(accuracy_category)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in category-aware recommendations: {e}\")\n",
        "            results['category_aware'].append(0.0)\n",
        "\n",
        "    # Calculate and display overall results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OVERALL EVALUATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for approach, accuracies in results.items():\n",
        "        valid_accuracies = [acc for acc in accuracies if acc is not None]\n",
        "        if valid_accuracies:\n",
        "            avg_accuracy = np.mean(valid_accuracies)\n",
        "            std_accuracy = np.std(valid_accuracies)\n",
        "            print(f\"{approach.replace('_', ' ').title()} Approach:\")\n",
        "            print(f\"  Average Accuracy: {avg_accuracy:.1%} (±{std_accuracy:.1%})\")\n",
        "            print(f\"  Valid Evaluations: {len(valid_accuracies)}/{len(accuracies)}\")\n",
        "        else:\n",
        "            print(f\"{approach.replace('_', ' ').title()} Approach: No valid results\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0iWRdKV57Sy",
        "outputId": "19596c71-bec2-43dc-d413-756b401ee5b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6] Comprehensive evaluation of recommendation approaches...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlwTsIUjaz4S",
        "outputId": "dd5b3894-594a-4db6-9d0a-926615788e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COMPREHENSIVE RECOMMENDATION EVALUATION ===\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 205\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '205' ---\n",
            "User demographics: 65+, Male, Austin\n",
            "Preferred categories (3): [np.str_('movies_tv'), np.str_('automotive'), np.str_('grocery')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '205' ---\n",
            "User demographics: 65+, Male, Austin\n",
            "Preferred categories (3): [np.str_('movies_tv'), np.str_('automotive'), np.str_('grocery')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 4 items (40.0%)\n",
            "  ✓ grocery: 3 items (30.0%)\n",
            "  ✓ movies_tv: 3 items (30.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '205' ---\n",
            "User demographics: 65+, Male, Austin\n",
            "Preferred categories (3): ['automotive', 'movies_tv', 'grocery']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 486\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '486' ---\n",
            "User demographics: 25-34, Female, Charlotte\n",
            "Preferred categories (5): [np.str_('office_supplies'), np.str_('grocery'), np.str_('electronics'), np.str_('shoes'), np.str_('clothing')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ electronics: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '486' ---\n",
            "User demographics: 25-34, Female, Charlotte\n",
            "Preferred categories (5): [np.str_('office_supplies'), np.str_('grocery'), np.str_('electronics'), np.str_('shoes'), np.str_('clothing')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 2 items (20.0%)\n",
            "  ✓ electronics: 2 items (20.0%)\n",
            "  ✓ grocery: 2 items (20.0%)\n",
            "  ✓ office_supplies: 2 items (20.0%)\n",
            "  ✓ shoes: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '486' ---\n",
            "User demographics: 25-34, Female, Charlotte\n",
            "Preferred categories (5): ['office_supplies', 'grocery', 'electronics', 'clothing', 'shoes']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ electronics: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 681\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '681' ---\n",
            "User demographics: 45-54, Female, San Antonio\n",
            "Preferred categories (4): [np.str_('watches'), np.str_('electronics'), np.str_('movies_tv'), np.str_('automotive')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ electronics: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '681' ---\n",
            "User demographics: 45-54, Female, San Antonio\n",
            "Preferred categories (4): [np.str_('watches'), np.str_('electronics'), np.str_('movies_tv'), np.str_('automotive')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 2 items (20.0%)\n",
            "  ✓ electronics: 4 items (40.0%)\n",
            "  ✓ movies_tv: 2 items (20.0%)\n",
            "  ✓ watches: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '681' ---\n",
            "User demographics: 45-54, Female, San Antonio\n",
            "Preferred categories (4): ['movies_tv', 'watches', 'electronics', 'automotive']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ electronics: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 24\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '24' ---\n",
            "User demographics: 55-64, Male, San Diego\n",
            "Preferred categories (4): [np.str_('sports_outdoors'), np.str_('movies_tv'), np.str_('automotive'), np.str_('office_supplies')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ office_supplies: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '24' ---\n",
            "User demographics: 55-64, Male, San Diego\n",
            "Preferred categories (4): [np.str_('sports_outdoors'), np.str_('movies_tv'), np.str_('automotive'), np.str_('office_supplies')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 2 items (20.0%)\n",
            "  ✓ movies_tv: 2 items (20.0%)\n",
            "  ✓ office_supplies: 4 items (40.0%)\n",
            "  ✓ sports_outdoors: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '24' ---\n",
            "User demographics: 55-64, Male, San Diego\n",
            "Preferred categories (4): ['automotive', 'movies_tv', 'sports_outdoors', 'office_supplies']\n",
            "Found 49 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ office_supplies: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 488\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '488' ---\n",
            "User demographics: 45-54, Male, Boston\n",
            "Preferred categories (3): [np.str_('electronics'), np.str_('music'), np.str_('industrial')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ electronics: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '488' ---\n",
            "User demographics: 45-54, Male, Boston\n",
            "Preferred categories (3): [np.str_('electronics'), np.str_('music'), np.str_('industrial')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ electronics: 4 items (40.0%)\n",
            "  ✓ industrial: 3 items (30.0%)\n",
            "  ✓ music: 3 items (30.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '488' ---\n",
            "User demographics: 45-54, Male, Boston\n",
            "Preferred categories (3): ['music', 'electronics', 'industrial']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ electronics: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 952\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '952' ---\n",
            "User demographics: 25-34, Male, San Jose\n",
            "Preferred categories (4): [np.str_('electronics'), np.str_('beauty_health'), np.str_('music'), np.str_('sports_outdoors')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ beauty_health: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '952' ---\n",
            "User demographics: 25-34, Male, San Jose\n",
            "Preferred categories (4): [np.str_('electronics'), np.str_('beauty_health'), np.str_('music'), np.str_('sports_outdoors')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ beauty_health: 4 items (40.0%)\n",
            "  ✓ electronics: 2 items (20.0%)\n",
            "  ✓ music: 2 items (20.0%)\n",
            "  ✓ sports_outdoors: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '952' ---\n",
            "User demographics: 25-34, Male, San Jose\n",
            "Preferred categories (4): ['music', 'electronics', 'sports_outdoors', 'beauty_health']\n",
            "Found 49 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ beauty_health: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 132\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '132' ---\n",
            "User demographics: 25-34, Male, San Jose\n",
            "Preferred categories (3): [np.str_('electronics'), np.str_('clothing'), np.str_('movies_tv')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '132' ---\n",
            "User demographics: 25-34, Male, San Jose\n",
            "Preferred categories (3): [np.str_('electronics'), np.str_('clothing'), np.str_('movies_tv')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 4 items (40.0%)\n",
            "  ✓ electronics: 3 items (30.0%)\n",
            "  ✓ movies_tv: 3 items (30.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '132' ---\n",
            "User demographics: 25-34, Male, San Jose\n",
            "Preferred categories (3): ['movies_tv', 'electronics', 'clothing']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 203\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '203' ---\n",
            "User demographics: 45-54, Female, Boston\n",
            "Preferred categories (4): [np.str_('watches'), np.str_('home_garden'), np.str_('books'), np.str_('grocery')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ books: 9 items (90.0%)\n",
            "  ✓ watches: 1 items (10.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '203' ---\n",
            "User demographics: 45-54, Female, Boston\n",
            "Preferred categories (4): [np.str_('watches'), np.str_('home_garden'), np.str_('books'), np.str_('grocery')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ books: 4 items (40.0%)\n",
            "  ✓ grocery: 2 items (20.0%)\n",
            "  ✓ home_garden: 2 items (20.0%)\n",
            "  ✓ watches: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '203' ---\n",
            "User demographics: 45-54, Female, Boston\n",
            "Preferred categories (4): ['books', 'watches', 'grocery', 'home_garden']\n",
            "Found 49 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ books: 9 items (90.0%)\n",
            "  ✓ watches: 1 items (10.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 214\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '214' ---\n",
            "User demographics: 55-64, Female, San Diego\n",
            "Preferred categories (3): [np.str_('movies_tv'), np.str_('kitchen_dining'), np.str_('crafts')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ movies_tv: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '214' ---\n",
            "User demographics: 55-64, Female, San Diego\n",
            "Preferred categories (3): [np.str_('movies_tv'), np.str_('kitchen_dining'), np.str_('crafts')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ crafts: 3 items (30.0%)\n",
            "  ✓ kitchen_dining: 3 items (30.0%)\n",
            "  ✓ movies_tv: 4 items (40.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '214' ---\n",
            "User demographics: 55-64, Female, San Diego\n",
            "Preferred categories (3): ['kitchen_dining', 'movies_tv', 'crafts']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ movies_tv: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 593\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '593' ---\n",
            "User demographics: 45-54, Other, Los Angeles\n",
            "Preferred categories (4): [np.str_('electronics'), np.str_('beauty_health'), np.str_('automotive'), np.str_('sports_outdoors')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '593' ---\n",
            "User demographics: 45-54, Other, Los Angeles\n",
            "Preferred categories (4): [np.str_('electronics'), np.str_('beauty_health'), np.str_('automotive'), np.str_('sports_outdoors')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 4 items (40.0%)\n",
            "  ✓ beauty_health: 2 items (20.0%)\n",
            "  ✓ electronics: 2 items (20.0%)\n",
            "  ✓ sports_outdoors: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '593' ---\n",
            "User demographics: 45-54, Other, Los Angeles\n",
            "Preferred categories (4): ['automotive', 'electronics', 'sports_outdoors', 'beauty_health']\n",
            "Found 48 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "OVERALL EVALUATION RESULTS\n",
            "============================================================\n",
            "Demographic Approach:\n",
            "  Average Accuracy: 100.0% (±0.0%)\n",
            "  Valid Evaluations: 10/10\n",
            "Balanced Approach:\n",
            "  Average Accuracy: 100.0% (±0.0%)\n",
            "  Valid Evaluations: 10/10\n",
            "Category Aware Approach:\n",
            "  Average Accuracy: 100.0% (±0.0%)\n",
            "  Valid Evaluations: 10/10\n",
            "\n",
            "============================================================\n",
            "RECOMMENDATION SYSTEM READY\n",
            "============================================================\n",
            "System trained and evaluated successfully!\n",
            "You can now use the interactive demo to explore recommendations.\n",
            "\n",
            "=== SYSTEM SUMMARY ===\n",
            "✓ Enhanced Item Tower: Trained with 10000 items across 20 categories\n",
            "✓ Enhanced User Tower: Trained with 1000 users with demographic features\n",
            "✓ Faiss Index: Contains 10000 normalized item embeddings\n",
            "✓ Training Data: 12405 user-item interactions\n",
            "✓ Evaluation: Completed on 10 test users\n",
            "\n",
            "Recommendation approaches implemented:\n",
            "  1. Demographic-based recommendations\n",
            "  2. Balanced category recommendations\n",
            "  3. Category-aware diversity recommendations\n",
            "\n",
            "To start the interactive demo, call: interactive_recommendation_demo()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Step 7: Interactive Recommendation Interface ---\n",
        "def interactive_recommendation_demo():\n",
        "    \"\"\"Interactive demo of the recommendation system\"\"\"\n",
        "    print(\"\\n=== INTERACTIVE RECOMMENDATION DEMO ===\")\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nChoose an option:\")\n",
        "        print(\"1. Get recommendations for a specific user\")\n",
        "        print(\"2. Get recommendations for a random user\")\n",
        "        print(\"3. Compare all approaches for a user\")\n",
        "        print(\"4. Show user demographics and preferences\")\n",
        "        print(\"5. Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                approach = input(\"Choose approach (demographic/balanced/category_aware): \").strip().lower()\n",
        "                if approach == 'demographic':\n",
        "                    get_demographic_recommendations(user_id)\n",
        "                elif approach == 'balanced':\n",
        "                    get_balanced_recommendations(user_id)\n",
        "                elif approach == 'category_aware':\n",
        "                    get_category_aware_recommendations(user_id)\n",
        "                else:\n",
        "                    print(\"Invalid approach. Using demographic approach.\")\n",
        "                    get_demographic_recommendations(user_id)\n",
        "            else:\n",
        "                print(f\"User {user_id} not found. Available users: {list(unique_user_ids)[:10]}...\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            user_id = np.random.choice(unique_user_ids)\n",
        "            print(f\"Selected random user: {user_id}\")\n",
        "            get_demographic_recommendations(user_id)\n",
        "\n",
        "        elif choice == '3':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                print(f\"Comparing all approaches for user {user_id}:\")\n",
        "                get_demographic_recommendations(user_id)\n",
        "                get_balanced_recommendations(user_id)\n",
        "                get_category_aware_recommendations(user_id)\n",
        "            else:\n",
        "                print(f\"User {user_id} not found.\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "                preferred_categories = user_categories[user_id]\n",
        "                user_interactions_count = len(interactions_df[interactions_df['user_id'] == user_id])\n",
        "\n",
        "                print(f\"\\nUser {user_id} Profile:\")\n",
        "                print(f\"  Demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "                print(f\"  Preferred Categories: {list(preferred_categories)}\")\n",
        "                print(\"breaking\")\n",
        "                print(f\"  Total Interactions: {user_interactions_count}\")\n",
        "\n",
        "                # Show actual interactions\n",
        "                user_interactions = interactions_df[interactions_df['user_id'] == user_id]\n",
        "                interaction_details = pd.merge(user_interactions, items_df, on='item_id')\n",
        "                category_breakdown = interaction_details['category'].value_counts()\n",
        "\n",
        "                print(\"  Interaction Breakdown:\")\n",
        "                for category, count in category_breakdown.items():\n",
        "                    print(f\"    {category}: {count} interactions\")\n",
        "            else:\n",
        "                print(f\"User {user_id} not found.\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = evaluate_recommendation_approaches()\n",
        "\n",
        "# Start interactive demo\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RECOMMENDATION SYSTEM READY\")\n",
        "print(\"=\"*60)\n",
        "print(\"System trained and evaluated successfully!\")\n",
        "print(\"You can now use the interactive demo to explore recommendations.\")\n",
        "\n",
        "# Uncomment the line below to start the interactive demo\n",
        "# interactive_recommendation_demo()\n",
        "\n",
        "print(\"\\n=== SYSTEM SUMMARY ===\")\n",
        "print(f\"✓ Enhanced Item Tower: Trained with {len(items_df)} items across {len(categories)} categories\")\n",
        "print(f\"✓ Enhanced User Tower: Trained with {len(users_df)} users with demographic features\")\n",
        "print(f\"✓ Faiss Index: Contains {index.ntotal} normalized item embeddings\")\n",
        "print(f\"✓ Training Data: {len(interactions_df)} user-item interactions\")\n",
        "print(f\"✓ Evaluation: Completed on {len(evaluation_results['demographic'])} test users\")\n",
        "print(\"\\nRecommendation approaches implemented:\")\n",
        "print(\"  1. Demographic-based recommendations\")\n",
        "print(\"  2. Balanced category recommendations\")\n",
        "print(\"  3. Category-aware diversity recommendations\")\n",
        "print(\"\\nTo start the interactive demo, call: interactive_recommendation_demo()\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M-cUjNfc2wZg"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPS94vrd345PE+WrXtmoM5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}