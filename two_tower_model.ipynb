{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhaj-mhd/two-tower-recommedation/blob/user-demo/two_tower_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSb8v80Z2v8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#install packages\n"
      ],
      "metadata": {
        "id": "M-cUjNfc2wZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6rQ19NkZo8s",
        "outputId": "9971d3b8-a295-400d-c785-f94f89725bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Installing and upgrading all required packages...\n",
            "\n",
            "✅ All packages have been installed and upgraded.\n"
          ]
        }
      ],
      "source": [
        "print(\"⏳ Installing and upgrading all required packages...\")\n",
        "\n",
        "%pip install --upgrade -q tensorflow tensorflow-recommenders tf-keras tensorflow-text\n",
        "%pip install -q faiss-cpu\n",
        "\n",
        "print(\"\\n✅ All packages have been installed and upgraded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rbdXsqBCabJ4"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade -q tensorflow-decision-forests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "_roixOESalk5",
        "outputId": "9b632c8d-f109-44e1-e581-3562ecd06695",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-855599773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menv\u001b[0m \u001b[0;31m# line: 456\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m \u001b[0;31m# line: 365\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0min_main_process\u001b[0m \u001b[0;31m# line: 418\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/combinations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_all_reduce_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_server_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcross_device_ops_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/cross_device_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvalue_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstruct_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpacked_distributed_variable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/service/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    417\u001b[0m \"\"\"\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_service_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_server_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_utils_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/ops/compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minternal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/ragged/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mAPI\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \"\"\"\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/ragged/ragged_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[0;31m# ===============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RaggedTensorSpec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2320\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtype_spec_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.RaggedTensorSpec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2321\u001b[0m class RaggedTensorSpec(\n\u001b[1;32m   2322\u001b[0m     type_spec.BatchableTypeSpec, internal_types.RaggedTensorSpec):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec_registry.py\u001b[0m in \u001b[0;36mdecorator_fn\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     57\u001b[0m                        (cls.__module__, cls.__name__, _TYPE_SPEC_TO_NAME[cls]))\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_NAME_TO_TYPE_SPEC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       raise ValueError(\"Name %s has already been registered for class %s.%s.\" %\n\u001b[0m\u001b[1;32m     60\u001b[0m                        (name, _NAME_TO_TYPE_SPEC[name].__module__,\n\u001b[1;32m     61\u001b[0m                         _NAME_TO_TYPE_SPEC[name].__name__))\n",
            "\u001b[0;31mValueError\u001b[0m: Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "import tf_keras\n",
        "import faiss\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"tensorflow-recommenders: {tfrs.__version__}\")\n",
        "print(f\"tf-keras: {tf_keras.__version__}\")\n",
        "print(f\"faiss-cpu: {faiss.__version__}\")\n",
        "print(f\"tensorflow-text: {tf_text.__version__}\")\n",
        "print(f\"tensorflow-decision-forests: {tfdf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Two tower model"
      ],
      "metadata": {
        "id": "FMnfU2bI2cdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "\n"
      ],
      "metadata": {
        "id": "_SUBQRpK4Boa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Fabricate Data with 20 Categories and Enhanced User Demographics ---\n",
        "print(\"[1] Fabricating data with 20 categories and enhanced user demographics...\")\n",
        "\n",
        "# 20 diverse categories\n",
        "categories = [\n",
        "    \"electronics\", \"clothing\", \"books\", \"home_garden\", \"sports_outdoors\",\n",
        "    \"beauty_health\", \"automotive\", \"toys_games\", \"jewelry\", \"music\",\n",
        "    \"movies_tv\", \"kitchen_dining\", \"office_supplies\", \"pet_supplies\", \"crafts\",\n",
        "    \"industrial\", \"grocery\", \"baby_products\", \"shoes\", \"watches\"\n",
        "]\n",
        "\n",
        "# User demographics data\n",
        "age_groups = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"]\n",
        "locations = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\",\n",
        "            \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\", \"Austin\", \"Jacksonville\",\n",
        "            \"Fort Worth\", \"Columbus\", \"Charlotte\", \"Seattle\", \"Denver\", \"Boston\"]\n",
        "genders = [\"Male\", \"Female\", \"Other\"]\n",
        "\n",
        "num_items = 10000  # Increased items to accommodate more categories\n",
        "num_users = 1000   # Increased users for better diversity\n",
        "\n",
        "# Create items with explicit category tracking\n",
        "item_titles = [f\"Product {i}\" for i in range(num_items)]\n",
        "item_categories = [categories[i % len(categories)] for i in range(num_items)]\n",
        "\n",
        "# Create diverse descriptions based on category\n",
        "description_templates = {\n",
        "    \"electronics\": [\n",
        "        \"High-tech electronic device with advanced features\",\n",
        "        \"Smart gadget with wireless connectivity and AI integration\",\n",
        "        \"Innovative electronic tool for modern digital life\",\n",
        "        \"Premium electronic device with cutting-edge technology\"\n",
        "    ],\n",
        "    \"clothing\": [\n",
        "        \"Fashionable apparel made from quality sustainable materials\",\n",
        "        \"Comfortable and stylish garment for everyday wear\",\n",
        "        \"Trendy clothing with modern design and premium fabric\",\n",
        "        \"Versatile wardrobe piece suitable for various occasions\"\n",
        "    ],\n",
        "    \"books\": [\n",
        "        \"Educational book covering important academic topics\",\n",
        "        \"Engaging literature for avid readers and book enthusiasts\",\n",
        "        \"Informative guide with practical knowledge and insights\",\n",
        "        \"Bestselling book with compelling storytelling and research\"\n",
        "    ],\n",
        "    \"home_garden\": [\n",
        "        \"Durable home improvement tool for DIY projects\",\n",
        "        \"Garden equipment for landscaping and plant care\",\n",
        "        \"Home decor item to enhance living space aesthetics\",\n",
        "        \"Functional household item for daily convenience\"\n",
        "    ],\n",
        "    \"sports_outdoors\": [\n",
        "        \"Professional sports equipment for athletic performance\",\n",
        "        \"Outdoor gear for adventure and recreational activities\",\n",
        "        \"Fitness equipment for home workout routines\",\n",
        "        \"Camping and hiking essentials for outdoor enthusiasts\"\n",
        "    ],\n",
        "    \"beauty_health\": [\n",
        "        \"Premium skincare product with natural ingredients\",\n",
        "        \"Health supplement for wellness and vitality\",\n",
        "        \"Cosmetic item for beauty enhancement and self-care\",\n",
        "        \"Personal care product for daily hygiene routine\"\n",
        "    ],\n",
        "    \"automotive\": [\n",
        "        \"High-quality automotive part for vehicle maintenance\",\n",
        "        \"Car accessory for enhanced driving experience\",\n",
        "        \"Professional-grade tool for automotive repair\",\n",
        "        \"Vehicle enhancement product for performance optimization\"\n",
        "    ],\n",
        "    \"toys_games\": [\n",
        "        \"Educational toy for children's development and learning\",\n",
        "        \"Board game for family entertainment and bonding\",\n",
        "        \"Creative plaything that sparks imagination and creativity\",\n",
        "        \"Interactive game for skill development and fun\"\n",
        "    ],\n",
        "    \"jewelry\": [\n",
        "        \"Elegant jewelry piece crafted with precious metals\",\n",
        "        \"Stylish accessory for fashion and personal expression\",\n",
        "        \"Handcrafted jewelry with unique design elements\",\n",
        "        \"Luxury jewelry item for special occasions\"\n",
        "    ],\n",
        "    \"music\": [\n",
        "        \"Professional music equipment for audio production\",\n",
        "        \"Musical instrument for creative expression and performance\",\n",
        "        \"High-quality audio device for music enthusiasts\",\n",
        "        \"Music accessory for enhanced listening experience\"\n",
        "    ],\n",
        "    \"movies_tv\": [\n",
        "        \"Entertainment media for leisure and relaxation\",\n",
        "        \"Classic film collection for movie enthusiasts\",\n",
        "        \"TV series with compelling storylines and characters\",\n",
        "        \"Documentary content for educational entertainment\"\n",
        "    ],\n",
        "    \"kitchen_dining\": [\n",
        "        \"Professional kitchen utensil for culinary excellence\",\n",
        "        \"Dining accessory for elegant meal presentation\",\n",
        "        \"Cooking tool made from premium food-safe materials\",\n",
        "        \"Kitchen gadget for efficient food preparation\"\n",
        "    ],\n",
        "    \"office_supplies\": [\n",
        "        \"Professional office equipment for workplace productivity\",\n",
        "        \"Stationery item for organization and documentation\",\n",
        "        \"Ergonomic office accessory for comfort and efficiency\",\n",
        "        \"Business tool for professional operations\"\n",
        "    ],\n",
        "    \"pet_supplies\": [\n",
        "        \"Pet care product for animal health and happiness\",\n",
        "        \"Pet toy for entertainment and exercise\",\n",
        "        \"Pet accessory for comfort and safety\",\n",
        "        \"Pet nutrition product for optimal health\"\n",
        "    ],\n",
        "    \"crafts\": [\n",
        "        \"Art supply for creative projects and expression\",\n",
        "        \"Craft material for DIY projects and hobbies\",\n",
        "        \"Creative tool for artistic endeavors and crafting\",\n",
        "        \"Handcraft supply for personalized creations\"\n",
        "    ],\n",
        "    \"industrial\": [\n",
        "        \"Industrial equipment for manufacturing and production\",\n",
        "        \"Heavy-duty tool for professional industrial use\",\n",
        "        \"Machinery component for industrial operations\",\n",
        "        \"Professional-grade equipment for industrial applications\"\n",
        "    ],\n",
        "    \"grocery\": [\n",
        "        \"Premium food product for nutritious meals\",\n",
        "        \"Organic ingredient for healthy cooking\",\n",
        "        \"Gourmet food item for culinary excellence\",\n",
        "        \"Essential grocery item for daily nutrition\"\n",
        "    ],\n",
        "    \"baby_products\": [\n",
        "        \"Safe baby product for infant care and development\",\n",
        "        \"Baby accessory for comfort and convenience\",\n",
        "        \"Child safety item for protection and security\",\n",
        "        \"Developmental toy for early childhood learning\"\n",
        "    ],\n",
        "    \"shoes\": [\n",
        "        \"Comfortable footwear for daily wear and activities\",\n",
        "        \"Athletic shoe for sports and fitness activities\",\n",
        "        \"Fashion shoe for style and personal expression\",\n",
        "        \"Professional footwear for workplace requirements\"\n",
        "    ],\n",
        "    \"watches\": [\n",
        "        \"Precision timepiece with advanced features\",\n",
        "        \"Luxury watch for style and status\",\n",
        "        \"Sports watch for active lifestyle tracking\",\n",
        "        \"Smart watch with digital connectivity features\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "item_descriptions = []\n",
        "for i in range(num_items):\n",
        "    category = item_categories[i]\n",
        "    template = description_templates[category][i % len(description_templates[category])]\n",
        "    item_descriptions.append(f\"{template}. Model v{i % 15}. Item #{i}.\")\n",
        "\n",
        "items_data = {\n",
        "    \"item_id\": [str(i) for i in range(num_items)],\n",
        "    \"item_title\": item_titles,\n",
        "    \"item_description\": item_descriptions,\n",
        "    \"category\": item_categories\n",
        "}\n",
        "items_df = pd.DataFrame(items_data)\n",
        "\n",
        "# Create category-to-items mapping for easier lookup\n",
        "category_to_items = defaultdict(list)\n",
        "for idx, row in items_df.iterrows():\n",
        "    category_to_items[row['category']].append(row['item_id'])\n",
        "\n",
        "# Generate enhanced user demographics data\n",
        "print(\"Generating enhanced user demographics...\")\n",
        "user_demographics = []\n",
        "for user_id in range(num_users):\n",
        "    # Generate demographics with realistic distributions\n",
        "    age_group = np.random.choice(age_groups, p=[0.15, 0.25, 0.22, 0.18, 0.12, 0.08])  # Weighted towards younger users\n",
        "    location = np.random.choice(locations)\n",
        "    gender = np.random.choice(genders, p=[0.48, 0.50, 0.02])  # Realistic gender distribution\n",
        "\n",
        "    user_demographics.append({\n",
        "        \"user_id\": str(user_id),\n",
        "        \"age_group\": age_group,\n",
        "        \"location\": location,\n",
        "        \"gender\": gender\n",
        "    })\n",
        "\n",
        "users_df = pd.DataFrame(user_demographics)\n",
        "\n",
        "# Generate user interactions with demographic influence on preferences\n",
        "print(\"Generating user interactions with demographic-influenced preferences...\")\n",
        "user_interactions = []\n",
        "user_categories = {}  # Track which categories each user prefers\n",
        "\n",
        "# Define demographic preferences (realistic patterns)\n",
        "demographic_preferences = {\n",
        "    \"age_group\": {\n",
        "        \"18-24\": {\"electronics\": 1.5, \"clothing\": 1.4, \"music\": 1.3, \"toys_games\": 1.2},\n",
        "        \"25-34\": {\"electronics\": 1.3, \"home_garden\": 1.2, \"clothing\": 1.2, \"automotive\": 1.1},\n",
        "        \"35-44\": {\"home_garden\": 1.4, \"automotive\": 1.3, \"office_supplies\": 1.2, \"baby_products\": 1.5},\n",
        "        \"45-54\": {\"home_garden\": 1.3, \"automotive\": 1.2, \"books\": 1.2, \"kitchen_dining\": 1.1},\n",
        "        \"55-64\": {\"books\": 1.3, \"home_garden\": 1.2, \"kitchen_dining\": 1.2, \"beauty_health\": 1.1},\n",
        "        \"65+\": {\"books\": 1.4, \"home_garden\": 1.3, \"beauty_health\": 1.2, \"kitchen_dining\": 1.1}\n",
        "    },\n",
        "    \"gender\": {\n",
        "        \"Male\": {\"electronics\": 1.3, \"automotive\": 1.4, \"sports_outdoors\": 1.3, \"tools\": 1.2},\n",
        "        \"Female\": {\"clothing\": 1.4, \"beauty_health\": 1.5, \"jewelry\": 1.3, \"baby_products\": 1.2},\n",
        "        \"Other\": {\"clothing\": 1.1, \"electronics\": 1.1, \"books\": 1.2, \"music\": 1.1}\n",
        "    }\n",
        "}\n",
        "\n",
        "for user_id in range(num_users):\n",
        "    user_demo = users_df[users_df['user_id'] == str(user_id)].iloc[0]\n",
        "\n",
        "    # Calculate category preferences based on demographics\n",
        "    category_scores = {}\n",
        "    for category in categories:\n",
        "        base_score = 1.0\n",
        "\n",
        "        # Age group influence\n",
        "        age_prefs = demographic_preferences[\"age_group\"].get(user_demo['age_group'], {})\n",
        "        age_multiplier = age_prefs.get(category, 1.0)\n",
        "\n",
        "        # Gender influence\n",
        "        gender_prefs = demographic_preferences[\"gender\"].get(user_demo['gender'], {})\n",
        "        gender_multiplier = gender_prefs.get(category, 1.0)\n",
        "\n",
        "        # Combine influences\n",
        "        category_scores[category] = base_score * age_multiplier * gender_multiplier\n",
        "\n",
        "    # Select 3-5 categories based on weighted preferences\n",
        "    num_categories = np.random.randint(3, 6)\n",
        "    category_weights = np.array([category_scores[cat] for cat in categories])\n",
        "    category_weights = category_weights / np.sum(category_weights)  # Normalize\n",
        "\n",
        "    preferred_categories = np.random.choice(categories, size=num_categories, replace=False, p=category_weights)\n",
        "    user_categories[str(user_id)] = preferred_categories\n",
        "\n",
        "    # Generate interactions for each preferred category\n",
        "    for category in preferred_categories:\n",
        "        # Random number of interactions per category (1-5)\n",
        "        num_interactions = np.random.randint(1, 6)  # 1 to 5 interactions\n",
        "\n",
        "        # Select random items from this category\n",
        "        available_items = category_to_items[category]\n",
        "        selected_items = np.random.choice(available_items, size=num_interactions, replace=True)\n",
        "\n",
        "        for item_id in selected_items:\n",
        "            user_interactions.append({\n",
        "                \"user_id\": str(user_id),\n",
        "                \"item_id\": item_id\n",
        "            })\n",
        "\n",
        "interactions_df = pd.DataFrame(user_interactions)\n",
        "\n",
        "# Calculate statistics\n",
        "total_interactions = len(interactions_df)\n",
        "avg_interactions_per_user = total_interactions / num_users\n",
        "\n",
        "print(f\"Generated {len(items_df)} items across {len(categories)} categories\")\n",
        "print(f\"Generated {total_interactions} interactions from {num_users} users\")\n",
        "print(f\"Average interactions per user: {avg_interactions_per_user:.1f}\")\n",
        "\n",
        "# Display demographic distribution\n",
        "print(f\"\\nUser demographic distribution:\")\n",
        "print(\"Age groups:\")\n",
        "for age_group in age_groups:\n",
        "    count = len(users_df[users_df['age_group'] == age_group])\n",
        "    print(f\"  {age_group}: {count} users ({count/num_users:.1%})\")\n",
        "\n",
        "print(\"Gender distribution:\")\n",
        "for gender in genders:\n",
        "    count = len(users_df[users_df['gender'] == gender])\n",
        "    print(f\"  {gender}: {count} users ({count/num_users:.1%})\")\n",
        "\n",
        "# Display sample user profiles\n",
        "print(\"\\nSample user profiles:\")\n",
        "for i in range(5):\n",
        "    user_id = str(i)\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    cats = user_categories[user_id]\n",
        "    user_interactions_count = len(interactions_df[interactions_df['user_id'] == user_id])\n",
        "    print(f\"User {user_id}: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"  Categories: {list(cats)} ({user_interactions_count} interactions)\")\n",
        "\n",
        "items_ds = tf.data.Dataset.from_tensor_slices(dict(items_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bB1HYdZU39Ss",
        "outputId": "da697de9-89cf-4855-ee37-4f21b0c2ee5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Fabricating data with 20 categories and enhanced user demographics...\n",
            "Generating enhanced user demographics...\n",
            "Generating user interactions with demographic-influenced preferences...\n",
            "Generated 10000 items across 20 categories\n",
            "Generated 12064 interactions from 1000 users\n",
            "Average interactions per user: 12.1\n",
            "\n",
            "User demographic distribution:\n",
            "Age groups:\n",
            "  18-24: 127 users (12.7%)\n",
            "  25-34: 234 users (23.4%)\n",
            "  35-44: 249 users (24.9%)\n",
            "  45-54: 179 users (17.9%)\n",
            "  55-64: 125 users (12.5%)\n",
            "  65+: 86 users (8.6%)\n",
            "Gender distribution:\n",
            "  Male: 474 users (47.4%)\n",
            "  Female: 509 users (50.9%)\n",
            "  Other: 17 users (1.7%)\n",
            "\n",
            "Sample user profiles:\n",
            "User 0: 25-34, Female, Boston\n",
            "  Categories: [np.str_('office_supplies'), np.str_('automotive'), np.str_('beauty_health'), np.str_('toys_games')] (13 interactions)\n",
            "User 1: 35-44, Male, Austin\n",
            "  Categories: [np.str_('shoes'), np.str_('music'), np.str_('jewelry'), np.str_('kitchen_dining')] (14 interactions)\n",
            "User 2: 25-34, Female, Houston\n",
            "  Categories: [np.str_('crafts'), np.str_('music'), np.str_('grocery'), np.str_('automotive')] (10 interactions)\n",
            "User 3: 65+, Male, Columbus\n",
            "  Categories: [np.str_('beauty_health'), np.str_('watches'), np.str_('pet_supplies')] (8 interactions)\n",
            "User 4: 35-44, Male, Chicago\n",
            "  Categories: [np.str_('watches'), np.str_('electronics'), np.str_('kitchen_dining'), np.str_('books')] (9 interactions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Enhanced Item Tower with Category Integration ---\n",
        "print(\"\\n[2] Building and training the enhanced Item Tower with category integration...\")\n",
        "embedding_dimension = 128\n",
        "max_tokens = 15_000\n",
        "sequence_length = 120\n",
        "\n",
        "# Create text vectorizers for both description and category\n",
        "description_vectorizer = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_sequence_length=sequence_length,\n",
        "    name=\"description_vectorizer\"\n",
        ")\n",
        "description_vectorizer.adapt(items_ds.map(lambda x: x[\"item_description\"]).batch(256))\n",
        "\n",
        "# Create category vectorizer (much smaller vocabulary)\n",
        "category_vectorizer = TextVectorization(\n",
        "    max_tokens=len(categories) + 10,  # Small vocabulary for categories\n",
        "    output_sequence_length=1,  # Categories are single tokens\n",
        "    name=\"category_vectorizer\"\n",
        ")\n",
        "category_vectorizer.adapt(items_ds.map(lambda x: x[\"category\"]).batch(256))\n",
        "\n",
        "class EnhancedItemModel(tf.keras.Model):\n",
        "    def __init__(self, description_vectorizer, category_vectorizer, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.description_vectorizer = description_vectorizer\n",
        "        self.category_vectorizer = category_vectorizer\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Description embedding branch\n",
        "        self.description_embedding = tf.keras.Sequential([\n",
        "            self.description_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.description_vectorizer.vocabulary_size(),\n",
        "                output_dim=embedding_dim,\n",
        "                mask_zero=True,\n",
        "                name=\"description_embedding\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(name=\"description_pooling\"),\n",
        "        ], name=\"description_branch\")\n",
        "\n",
        "        # Category embedding branch\n",
        "        self.category_embedding = tf.keras.Sequential([\n",
        "            self.category_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.category_vectorizer.vocabulary_size(),\n",
        "                output_dim=32,  # Smaller embedding for categories\n",
        "                mask_zero=True,\n",
        "                name=\"category_embedding\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(name=\"category_pooling\"),\n",
        "        ], name=\"category_branch\")\n",
        "\n",
        "        # Fusion layer to combine description and category embeddings\n",
        "        self.fusion_dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\", name=\"fusion_dense_1\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"fusion_bn_1\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"fusion_dropout_1\"),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\", name=\"fusion_dense_2\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"fusion_bn_2\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"fusion_dropout_2\"),\n",
        "            tf.keras.layers.Dense(embedding_dim, name=\"fusion_output\")\n",
        "        ], name=\"fusion_layer\")\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Process description\n",
        "        description_emb = self.description_embedding(inputs[\"item_description\"])\n",
        "\n",
        "        # Process category\n",
        "        category_emb = self.category_embedding(inputs[\"category\"])\n",
        "\n",
        "        # Concatenate description and category embeddings\n",
        "        combined = tf.concat([description_emb, category_emb], axis=1)\n",
        "\n",
        "        # Apply fusion layer\n",
        "        output = self.fusion_dense(combined, training=training)\n",
        "\n",
        "        return output\n",
        "\n",
        "class SelfSupervisedItemTwoTower(tfrs.Model):\n",
        "    def __init__(self, item_model):\n",
        "        super().__init__()\n",
        "        self.item_model = item_model\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        item_embeddings = self.item_model(features, training=training)\n",
        "        return self.task(query_embeddings=item_embeddings, candidate_embeddings=item_embeddings)\n",
        "\n",
        "# Initialize the enhanced item tower\n",
        "item_tower = EnhancedItemModel(description_vectorizer, category_vectorizer, embedding_dimension)\n",
        "item_model_trainer = SelfSupervisedItemTwoTower(item_tower)\n",
        "item_model_trainer.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "# Prepare training data with both description and category\n",
        "train_item_ds = items_ds.map(lambda x: {\n",
        "    \"item_description\": x[\"item_description\"],\n",
        "    \"category\": x[\"category\"]\n",
        "}).batch(512).cache()\n",
        "\n",
        "print(\"Training Enhanced Item Tower with description and category...\")\n",
        "item_model_trainer.fit(train_item_ds, epochs=15, verbose=1)\n",
        "print(\"Enhanced Item Tower training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9qVgZbUs4Ruk",
        "outputId": "66fec7f5-ab03-4144-a028-9c96469fdc49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2] Building and training the enhanced Item Tower with category integration...\n",
            "Training Enhanced Item Tower with description and category...\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - loss: 4.6062 - regularization_loss: 0.0000e+00 - total_loss: 4.6062\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 3.2723 - regularization_loss: 0.0000e+00 - total_loss: 3.2723\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.7164 - regularization_loss: 0.0000e+00 - total_loss: 0.7164\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 1.7562 - regularization_loss: 0.0000e+00 - total_loss: 1.7562\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 2.4998 - regularization_loss: 0.0000e+00 - total_loss: 2.4998\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 0.3838 - regularization_loss: 0.0000e+00 - total_loss: 0.3838\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.8252 - regularization_loss: 0.0000e+00 - total_loss: 0.8252\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 1.9160 - regularization_loss: 0.0000e+00 - total_loss: 1.9160\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 1.4483 - regularization_loss: 0.0000e+00 - total_loss: 1.4483\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.4936 - regularization_loss: 0.0000e+00 - total_loss: 0.4936\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - loss: 0.0503 - regularization_loss: 0.0000e+00 - total_loss: 0.0503\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 1.4709 - regularization_loss: 0.0000e+00 - total_loss: 1.4709\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 0.4562 - regularization_loss: 0.0000e+00 - total_loss: 0.4562\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 0.1914 - regularization_loss: 0.0000e+00 - total_loss: 0.1914\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 0.2865 - regularization_loss: 0.0000e+00 - total_loss: 0.2865\n",
            "Enhanced Item Tower training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Generate and Store Item Embeddings in Faiss ---\n",
        "print(\"\\n[3] Generating item embeddings and storing in Faiss...\")\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "# Generate embeddings using both description and category\n",
        "item_embeddings_generator = items_ds.batch(512).map(lambda x: item_tower({\n",
        "    \"item_description\": x[\"item_description\"],\n",
        "    \"category\": x[\"category\"]\n",
        "}))\n",
        "all_item_embeddings = np.concatenate(list(item_embeddings_generator.as_numpy_iterator()))\n",
        "\n",
        "# Normalize embeddings for better similarity search\n",
        "all_item_embeddings = all_item_embeddings / np.linalg.norm(all_item_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "index.add(all_item_embeddings)\n",
        "print(f\"Faiss index now contains {index.ntotal} vectors.\")\n",
        "index_to_item_id = {i: item_id for i, item_id in enumerate(items_df[\"item_id\"])}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e69zBRe4UzL",
        "outputId": "b4e274dc-7984-482a-88dc-3060a60c75c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3] Generating item embeddings and storing in Faiss...\n",
            "Faiss index now contains 10000 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Step 4: Enhanced User Tower with Demographics ---\n",
        "print(\"\\n[4] Building and training the Enhanced User Tower with demographics...\")\n",
        "unique_user_ids = interactions_df[\"user_id\"].unique()\n",
        "\n",
        "# Create dataset from user demographics\n",
        "users_ds = tf.data.Dataset.from_tensor_slices(dict(users_df))\n",
        "\n",
        "# Create vectorizers for demographic features\n",
        "age_group_vectorizer = TextVectorization(\n",
        "    max_tokens=len(age_groups) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"age_group_vectorizer\"\n",
        ")\n",
        "age_group_vectorizer.adapt(users_ds.map(lambda x: x[\"age_group\"]).batch(256))\n",
        "\n",
        "location_vectorizer = TextVectorization(\n",
        "    max_tokens=len(locations) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"location_vectorizer\"\n",
        ")\n",
        "location_vectorizer.adapt(users_ds.map(lambda x: x[\"location\"]).batch(256))\n",
        "\n",
        "gender_vectorizer = TextVectorization(\n",
        "    max_tokens=len(genders) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"gender_vectorizer\"\n",
        ")\n",
        "gender_vectorizer.adapt(users_ds.map(lambda x: x[\"gender\"]).batch(256))\n",
        "\n",
        "class EnhancedUserModel(tf.keras.Model):\n",
        "    def __init__(self, user_ids, age_group_vectorizer, location_vectorizer, gender_vectorizer):\n",
        "        super().__init__()\n",
        "        self.age_group_vectorizer = age_group_vectorizer\n",
        "        self.location_vectorizer = location_vectorizer\n",
        "        self.gender_vectorizer = gender_vectorizer\n",
        "\n",
        "        # User ID embedding\n",
        "        self.user_id_embedding = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=user_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(user_ids) + 1, 64, name=\"user_id_emb\")\n",
        "        ], name=\"user_id_branch\")\n",
        "\n",
        "        # Age group embedding\n",
        "        self.age_group_embedding = tf.keras.Sequential([\n",
        "            self.age_group_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.age_group_vectorizer.vocabulary_size(),\n",
        "                output_dim=16,\n",
        "                mask_zero=True,\n",
        "                name=\"age_group_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"age_group_branch\")\n",
        "\n",
        "        # Location embedding\n",
        "        self.location_embedding = tf.keras.Sequential([\n",
        "            self.location_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.location_vectorizer.vocabulary_size(),\n",
        "                output_dim=32,\n",
        "                mask_zero=True,\n",
        "                name=\"location_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"location_branch\")\n",
        "\n",
        "        # Gender embedding\n",
        "        self.gender_embedding = tf.keras.Sequential([\n",
        "            self.gender_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.gender_vectorizer.vocabulary_size(),\n",
        "                output_dim=8,\n",
        "                mask_zero=True,\n",
        "                name=\"gender_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"gender_branch\")\n",
        "\n",
        "        # Fusion layers to combine all user features\n",
        "        self.fusion_dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\", name=\"user_fusion_dense_1\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"user_fusion_bn_1\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"user_fusion_dropout_1\"),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\", name=\"user_fusion_dense_2\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"user_fusion_bn_2\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"user_fusion_dropout_2\"),\n",
        "            tf.keras.layers.Dense(embedding_dimension, name=\"user_fusion_output\")\n",
        "        ], name=\"user_fusion_layer\")\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Process user ID\n",
        "        user_id_emb = self.user_id_embedding(inputs[\"user_id\"])\n",
        "\n",
        "        # Process demographics\n",
        "        age_group_emb = self.age_group_embedding(inputs[\"age_group\"])\n",
        "        location_emb = self.location_embedding(inputs[\"location\"])\n",
        "        gender_emb = self.gender_embedding(inputs[\"gender\"])\n",
        "\n",
        "        # Concatenate all user features\n",
        "        combined = tf.concat([user_id_emb, age_group_emb, location_emb, gender_emb], axis=1)\n",
        "\n",
        "        # Apply fusion layer\n",
        "        output = self.fusion_dense(combined, training=training)\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnhancedUserItemRetrievalModel(tfrs.Model):\n",
        "    def __init__(self, user_model, item_model):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.item_model = item_model\n",
        "        self.item_model.trainable = False  # Keep item model frozen\n",
        "\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, data, training=False):\n",
        "        user_embeddings = self.user_model(data, training=training)\n",
        "\n",
        "        # Get item embeddings for the interacted items\n",
        "        item_data = {\n",
        "            \"item_description\": data[\"item_description\"],\n",
        "            \"category\": data[\"category\"]\n",
        "        }\n",
        "        item_embeddings = self.item_model(item_data, training=False)\n",
        "\n",
        "        # Normalize embeddings\n",
        "        user_embeddings = tf.nn.l2_normalize(user_embeddings, axis=1)\n",
        "        item_embeddings = tf.nn.l2_normalize(item_embeddings, axis=1)\n",
        "\n",
        "        return self.task(\n",
        "            query_embeddings=user_embeddings,\n",
        "            candidate_embeddings=item_embeddings\n",
        "        )\n",
        "\n",
        "# Prepare training data with user demographics and item details\n",
        "interactions_with_details_df = pd.merge(\n",
        "    interactions_df,\n",
        "    items_df[['item_id', 'item_description', 'category']],\n",
        "    on='item_id'\n",
        ")\n",
        "\n",
        "# Add user demographics to interactions\n",
        "interactions_with_details_df = pd.merge(\n",
        "    interactions_with_details_df,\n",
        "    users_df[['user_id', 'age_group', 'location', 'gender']],\n",
        "    on='user_id'\n",
        ")\n",
        "\n",
        "# Create negative sampling for better training\n",
        "print(\"Creating training dataset with negative sampling...\")\n",
        "positive_interactions = interactions_with_details_df.copy()\n",
        "positive_interactions['label'] = 1.0\n",
        "\n",
        "# Create negative samples\n",
        "negative_interactions = []\n",
        "for user_id in unique_user_ids:\n",
        "    user_positive_items = set(positive_interactions[positive_interactions['user_id'] == user_id]['item_id'])\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "\n",
        "    # Sample negative items (items the user hasn't interacted with)\n",
        "    num_negatives = min(len(user_positive_items), 20)  # Limit negatives to prevent memory issues\n",
        "    all_items = set(items_df['item_id'])\n",
        "    negative_items = list(all_items - user_positive_items)\n",
        "\n",
        "    if len(negative_items) >= num_negatives:\n",
        "        sampled_negatives = np.random.choice(negative_items, size=num_negatives, replace=False)\n",
        "\n",
        "        for item_id in sampled_negatives:\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            negative_interactions.append({\n",
        "                'user_id': user_id,\n",
        "                'item_id': item_id,\n",
        "                'item_description': item_row['item_description'],\n",
        "                'category': item_row['category'],\n",
        "                'age_group': user_demo['age_group'],\n",
        "                'location': user_demo['location'],\n",
        "                'gender': user_demo['gender'],\n",
        "                'label': 0.0\n",
        "            })\n",
        "\n",
        "negative_interactions_df = pd.DataFrame(negative_interactions)\n",
        "full_training_df = pd.concat([positive_interactions, negative_interactions_df], ignore_index=True)\n",
        "\n",
        "# Shuffle the training data\n",
        "full_training_df = full_training_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "full_interactions_ds = tf.data.Dataset.from_tensor_slices(dict(full_training_df))\n",
        "train_ds_user = full_interactions_ds.shuffle(50_000).batch(512).cache()\n",
        "\n",
        "user_tower = EnhancedUserModel(unique_user_ids, age_group_vectorizer, location_vectorizer, gender_vectorizer)\n",
        "user_model_trainer = EnhancedUserItemRetrievalModel(user_tower, item_tower)\n",
        "user_model_trainer.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "# Train the user model\n",
        "print(\"Training Enhanced User Tower with demographics...\")\n",
        "user_model_trainer.fit(train_ds_user, epochs=15, verbose=1)\n",
        "print(\"Enhanced User Tower training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMmQE4aH4Z4W",
        "outputId": "3e7947f8-4ccb-4e6d-f8fd-6d2148c8b42b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4] Building and training the Enhanced User Tower with demographics...\n",
            "Creating training dataset with negative sampling...\n",
            "Training Enhanced User Tower with demographics...\n",
            "Epoch 1/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - loss: 3063.8237 - regularization_loss: 0.0000e+00 - total_loss: 3063.8237\n",
            "Epoch 2/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 3059.4219 - regularization_loss: 0.0000e+00 - total_loss: 3059.4219\n",
            "Epoch 3/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 3054.3215 - regularization_loss: 0.0000e+00 - total_loss: 3054.3215\n",
            "Epoch 4/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - loss: 3051.1416 - regularization_loss: 0.0000e+00 - total_loss: 3051.1416\n",
            "Epoch 5/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 3049.2173 - regularization_loss: 0.0000e+00 - total_loss: 3049.2173\n",
            "Epoch 6/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 3048.1846 - regularization_loss: 0.0000e+00 - total_loss: 3048.1846\n",
            "Epoch 7/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 3047.4436 - regularization_loss: 0.0000e+00 - total_loss: 3047.4436\n",
            "Epoch 8/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 3046.8403 - regularization_loss: 0.0000e+00 - total_loss: 3046.8403\n",
            "Epoch 9/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - loss: 3046.3472 - regularization_loss: 0.0000e+00 - total_loss: 3046.3472\n",
            "Epoch 10/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - loss: 3045.9766 - regularization_loss: 0.0000e+00 - total_loss: 3045.9766\n",
            "Epoch 11/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 3045.6726 - regularization_loss: 0.0000e+00 - total_loss: 3045.6726\n",
            "Epoch 12/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 3045.4036 - regularization_loss: 0.0000e+00 - total_loss: 3045.4036\n",
            "Epoch 13/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 3045.2473 - regularization_loss: 0.0000e+00 - total_loss: 3045.2473\n",
            "Epoch 14/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 3044.8989 - regularization_loss: 0.0000e+00 - total_loss: 3044.8989\n",
            "Epoch 15/15\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 3044.7749 - regularization_loss: 0.0000e+00 - total_loss: 3044.7749\n",
            "Enhanced User Tower training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 5: Enhanced Recommendation Function with Demographics ---\n",
        "print(\"\\n[5] Implementing enhanced recommendation functions with demographics...\")\n",
        "\n",
        "def get_demographic_recommendations(user_id, top_k=15):\n",
        "    \"\"\"Get recommendations using enhanced user model with demographics\"\"\"\n",
        "    print(f\"\\n--- Enhanced demographic recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return None\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    user_embedding = user_tower(user_input).numpy()\n",
        "    user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Search for similar items using enhanced embeddings\n",
        "    search_k = min(top_k * 3, index.ntotal)\n",
        "    distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "    recommendations = []\n",
        "    for idx in indices[0]:\n",
        "        item_id = index_to_item_id[idx]\n",
        "        if item_id in user_interacted_items:\n",
        "            continue\n",
        "\n",
        "        item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "        recommendations.append({\n",
        "            'item_id': item_id,\n",
        "            'category': item_row['category'],\n",
        "            'title': item_row['item_title']\n",
        "        })\n",
        "\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "\n",
        "    # Calculate accuracy\n",
        "    category_counts = defaultdict(int)\n",
        "    correct_recommendations = 0\n",
        "\n",
        "    for rec in recommendations:\n",
        "        category_counts[rec['category']] += 1\n",
        "        if rec['category'] in preferred_categories:\n",
        "            correct_recommendations += 1\n",
        "\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stEEkN4o5cnk",
        "outputId": "2cfa5831-92a5-416e-e82c-4858f4bd1610"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[5] Implementing enhanced recommendation functions with demographics...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_balanced_recommendations(user_id, top_k=15):\n",
        "    \"\"\"Get recommendations with explicit category balancing\"\"\"\n",
        "    print(f\"\\n--- Balanced recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return handle_cold_start_user(user_id, top_k)\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    user_embedding = user_tower(user_input).numpy()\n",
        "    user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Balanced recommendation approach\n",
        "    recommendations = []\n",
        "    category_counts = defaultdict(int)\n",
        "\n",
        "    # Calculate items per preferred category\n",
        "    items_per_category = max(1, top_k // len(preferred_categories))\n",
        "    remaining_slots = top_k - (items_per_category * len(preferred_categories))\n",
        "\n",
        "    print(f\"Target: {items_per_category} items per category, {remaining_slots} flexible slots\")\n",
        "\n",
        "    # For each preferred category, find the best items\n",
        "    for category in preferred_categories:\n",
        "        category_items = category_to_items[category]\n",
        "        category_item_embeddings = []\n",
        "        category_item_ids = []\n",
        "\n",
        "        # Get embeddings for items in this category\n",
        "        for item_id in category_items:\n",
        "            if item_id not in user_interacted_items:\n",
        "                idx = int(item_id)\n",
        "                category_item_embeddings.append(all_item_embeddings[idx])\n",
        "                category_item_ids.append(item_id)\n",
        "\n",
        "        if not category_item_embeddings:\n",
        "            continue\n",
        "\n",
        "        # Calculate similarities to user embedding\n",
        "        category_item_embeddings = np.array(category_item_embeddings)\n",
        "        similarities = np.dot(category_item_embeddings, user_embedding.T).flatten()\n",
        "\n",
        "        # Get top items for this category\n",
        "        top_indices = np.argsort(similarities)[::-1][:items_per_category]\n",
        "\n",
        "        for idx in top_indices:\n",
        "            item_id = category_item_ids[idx]\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            recommendations.append({\n",
        "                'item_id': item_id,\n",
        "                'category': item_row['category'],\n",
        "                'title': item_row['item_title'],\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "            category_counts[category] += 1\n",
        "\n",
        "    # Fill remaining slots with best overall recommendations\n",
        "    if remaining_slots > 0:\n",
        "        search_k = min(top_k * 3, index.ntotal)\n",
        "        distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "        added_items = set(rec['item_id'] for rec in recommendations)\n",
        "\n",
        "        for idx in indices[0]:\n",
        "            if remaining_slots <= 0:\n",
        "                break\n",
        "\n",
        "            item_id = index_to_item_id[idx]\n",
        "            if item_id in user_interacted_items or item_id in added_items:\n",
        "                continue\n",
        "\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            recommendations.append({\n",
        "                'item_id': item_id,\n",
        "                'category': item_row['category'],\n",
        "                'title': item_row['item_title'],\n",
        "                'similarity': 1.0 - distances[0][list(indices[0]).index(idx)]  # Convert distance to similarity\n",
        "            })\n",
        "            category_counts[item_row['category']] += 1\n",
        "            added_items.add(item_id)\n",
        "            remaining_slots -= 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_recommendations = sum(1 for rec in recommendations if rec['category'] in preferred_categories)\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "9M7kouKL5m1c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def handle_cold_start_user(user_id, top_k=15):\n",
        "    \"\"\"Handle recommendations for new users without interaction history\"\"\"\n",
        "    print(f\"Handling cold start for user '{user_id}'\")\n",
        "\n",
        "    # For cold start, use demographic-based recommendations\n",
        "    # This is a simplified approach - in practice, you might use popularity-based or content-based filtering\n",
        "\n",
        "    # Sample popular items from each category\n",
        "    popular_items = []\n",
        "    items_per_category = max(1, top_k // len(categories))\n",
        "\n",
        "    for category in categories[:min(len(categories), top_k)]:\n",
        "        category_items = category_to_items[category]\n",
        "        if category_items:\n",
        "            # For simplicity, take the first few items from each category\n",
        "            # In practice, you'd want to use popularity metrics\n",
        "            sample_size = min(items_per_category, len(category_items))\n",
        "            sampled_items = np.random.choice(category_items, size=sample_size, replace=False)\n",
        "\n",
        "            for item_id in sampled_items:\n",
        "                item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "                popular_items.append({\n",
        "                    'item_id': item_id,\n",
        "                    'category': item_row['category'],\n",
        "                    'title': item_row['item_title']\n",
        "                })\n",
        "\n",
        "    # Shuffle and limit to top_k\n",
        "    np.random.shuffle(popular_items)\n",
        "    popular_items = popular_items[:top_k]\n",
        "\n",
        "    print(f\"Generated {len(popular_items)} cold start recommendations\")\n",
        "    category_counts = defaultdict(int)\n",
        "    for item in popular_items:\n",
        "        category_counts[item['category']] += 1\n",
        "\n",
        "    print(\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(popular_items)) * 100 if popular_items else 0\n",
        "        print(f\"  {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    return 0.0  # Cold start accuracy is 0 since we don't know user preferences\n",
        "\n"
      ],
      "metadata": {
        "id": "e9DjVerf-Mhh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_category_aware_recommendations(user_id, top_k=15, category_diversity_weight=0.3):\n",
        "    \"\"\"Get recommendations with category diversity weighting - Fixed infinite loop issue\"\"\"\n",
        "    print(f\"\\n--- Category-aware recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return handle_cold_start_user(user_id, top_k)\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    # Convert numpy strings to regular strings to avoid comparison issues\n",
        "    preferred_categories = set(str(cat) for cat in preferred_categories)\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        user_embedding = user_tower(user_input).numpy()\n",
        "        user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting user embedding: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Get more candidates than needed for diversity selection\n",
        "    search_k = min(top_k * 5, index.ntotal)\n",
        "    distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "    # Build candidate pool with similarity scores\n",
        "    candidates = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        item_id = index_to_item_id[idx]\n",
        "        if item_id in user_interacted_items:\n",
        "            continue\n",
        "\n",
        "        item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "        similarity = 1.0 - distances[0][i]  # Convert distance to similarity\n",
        "\n",
        "        candidates.append({\n",
        "            'item_id': item_id,\n",
        "            'category': str(item_row['category']),  # Convert to string\n",
        "            'title': item_row['item_title'],\n",
        "            'similarity': similarity,\n",
        "            'is_preferred': str(item_row['category']) in preferred_categories\n",
        "        })\n",
        "\n",
        "    print(f\"Found {len(candidates)} candidates after filtering\")\n",
        "\n",
        "    if not candidates:\n",
        "        print(\"No candidates found!\")\n",
        "        return 0.0\n",
        "\n",
        "    # FIXED: Use a simpler, guaranteed-to-terminate selection algorithm\n",
        "    recommendations = []\n",
        "    category_counts = defaultdict(int)\n",
        "\n",
        "    # Sort candidates by adjusted score first\n",
        "    for candidate in candidates:\n",
        "        score = candidate['similarity']\n",
        "        if candidate['is_preferred']:\n",
        "            score *= 1.5\n",
        "        candidate['adjusted_score'] = score\n",
        "\n",
        "    # Sort by adjusted score (highest first)\n",
        "    candidates.sort(key=lambda x: x['adjusted_score'], reverse=True)\n",
        "\n",
        "    # Select with diversity constraints\n",
        "    for candidate in candidates:\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "\n",
        "        # Apply diversity penalty\n",
        "        category_penalty = category_counts[candidate['category']] * category_diversity_weight\n",
        "        final_score = candidate['adjusted_score'] - category_penalty\n",
        "\n",
        "        # Accept if it's still a good candidate or if we need more recommendations\n",
        "        if final_score > 0 or len(recommendations) < top_k // 2:\n",
        "            recommendations.append(candidate)\n",
        "            category_counts[candidate['category']] += 1\n",
        "\n",
        "    # If we still don't have enough, fill with remaining candidates\n",
        "    remaining_candidates = [c for c in candidates if c not in recommendations]\n",
        "    for candidate in remaining_candidates:\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "        recommendations.append(candidate)\n",
        "        category_counts[candidate['category']] += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_recommendations = sum(1 for rec in recommendations if rec['is_preferred'])\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "kg778q9c5sVW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 6: Comprehensive Evaluation ---\n",
        "print(\"\\n[6] Comprehensive evaluation of recommendation approaches...\")\n",
        "\n",
        "def evaluate_recommendation_approaches():\n",
        "    \"\"\"Evaluate different recommendation approaches\"\"\"\n",
        "    print(\"\\n=== COMPREHENSIVE RECOMMENDATION EVALUATION ===\")\n",
        "\n",
        "    # Select diverse test users\n",
        "    test_users = np.random.choice(unique_user_ids, size=min(10, len(unique_user_ids)), replace=False)\n",
        "\n",
        "    results = {\n",
        "        'demographic': [],\n",
        "        'balanced': [],\n",
        "        'category_aware': []\n",
        "    }\n",
        "\n",
        "    for user_id in test_users:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EVALUATING USER {user_id}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Test demographic recommendations\n",
        "        try:\n",
        "            accuracy_demo = get_demographic_recommendations(user_id, top_k=10)\n",
        "            results['demographic'].append(accuracy_demo)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in demographic recommendations: {e}\")\n",
        "            results['demographic'].append(0.0)\n",
        "\n",
        "        # Test balanced recommendations\n",
        "        try:\n",
        "            accuracy_balanced = get_balanced_recommendations(user_id, top_k=10)\n",
        "            results['balanced'].append(accuracy_balanced)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in balanced recommendations: {e}\")\n",
        "            results['balanced'].append(0.0)\n",
        "\n",
        "        # Test category-aware recommendations\n",
        "        try:\n",
        "            accuracy_category = get_category_aware_recommendations(user_id, top_k=10)\n",
        "            results['category_aware'].append(accuracy_category)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in category-aware recommendations: {e}\")\n",
        "            results['category_aware'].append(0.0)\n",
        "\n",
        "    # Calculate and display overall results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OVERALL EVALUATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for approach, accuracies in results.items():\n",
        "        valid_accuracies = [acc for acc in accuracies if acc is not None]\n",
        "        if valid_accuracies:\n",
        "            avg_accuracy = np.mean(valid_accuracies)\n",
        "            std_accuracy = np.std(valid_accuracies)\n",
        "            print(f\"{approach.replace('_', ' ').title()} Approach:\")\n",
        "            print(f\"  Average Accuracy: {avg_accuracy:.1%} (±{std_accuracy:.1%})\")\n",
        "            print(f\"  Valid Evaluations: {len(valid_accuracies)}/{len(accuracies)}\")\n",
        "        else:\n",
        "            print(f\"{approach.replace('_', ' ').title()} Approach: No valid results\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0iWRdKV57Sy",
        "outputId": "70edb781-e0fb-452b-b9b1-fb34ce322a01"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6] Comprehensive evaluation of recommendation approaches...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlwTsIUjaz4S",
        "outputId": "fa1ccae4-6a31-49e6-fb2d-ac4d72693553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COMPREHENSIVE RECOMMENDATION EVALUATION ===\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 256\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '256' ---\n",
            "User demographics: 55-64, Male, New York\n",
            "Preferred categories (5): [np.str_('sports_outdoors'), np.str_('home_garden'), np.str_('baby_products'), np.str_('clothing'), np.str_('kitchen_dining')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ home_garden: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '256' ---\n",
            "User demographics: 55-64, Male, New York\n",
            "Preferred categories (5): [np.str_('sports_outdoors'), np.str_('home_garden'), np.str_('baby_products'), np.str_('clothing'), np.str_('kitchen_dining')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ baby_products: 2 items (20.0%)\n",
            "  ✓ clothing: 2 items (20.0%)\n",
            "  ✓ home_garden: 2 items (20.0%)\n",
            "  ✓ kitchen_dining: 2 items (20.0%)\n",
            "  ✓ sports_outdoors: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '256' ---\n",
            "User demographics: 55-64, Male, New York\n",
            "Preferred categories (5): ['clothing', 'sports_outdoors', 'kitchen_dining', 'home_garden', 'baby_products']\n",
            "Found 48 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ home_garden: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 680\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '680' ---\n",
            "User demographics: 55-64, Female, Seattle\n",
            "Preferred categories (3): [np.str_('toys_games'), np.str_('baby_products'), np.str_('crafts')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ toys_games: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '680' ---\n",
            "User demographics: 55-64, Female, Seattle\n",
            "Preferred categories (3): [np.str_('toys_games'), np.str_('baby_products'), np.str_('crafts')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ baby_products: 3 items (30.0%)\n",
            "  ✓ crafts: 3 items (30.0%)\n",
            "  ✓ toys_games: 4 items (40.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '680' ---\n",
            "User demographics: 55-64, Female, Seattle\n",
            "Preferred categories (3): ['crafts', 'baby_products', 'toys_games']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ toys_games: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 175\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '175' ---\n",
            "User demographics: 65+, Male, Denver\n",
            "Preferred categories (5): [np.str_('home_garden'), np.str_('shoes'), np.str_('kitchen_dining'), np.str_('books'), np.str_('beauty_health')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ shoes: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '175' ---\n",
            "User demographics: 65+, Male, Denver\n",
            "Preferred categories (5): [np.str_('home_garden'), np.str_('shoes'), np.str_('kitchen_dining'), np.str_('books'), np.str_('beauty_health')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ beauty_health: 2 items (20.0%)\n",
            "  ✓ books: 2 items (20.0%)\n",
            "  ✓ home_garden: 2 items (20.0%)\n",
            "  ✓ kitchen_dining: 2 items (20.0%)\n",
            "  ✓ shoes: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '175' ---\n",
            "User demographics: 65+, Male, Denver\n",
            "Preferred categories (5): ['beauty_health', 'shoes', 'kitchen_dining', 'books', 'home_garden']\n",
            "Found 49 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ shoes: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 303\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '303' ---\n",
            "User demographics: 35-44, Male, Chicago\n",
            "Preferred categories (4): [np.str_('baby_products'), np.str_('pet_supplies'), np.str_('beauty_health'), np.str_('crafts')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ crafts: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '303' ---\n",
            "User demographics: 35-44, Male, Chicago\n",
            "Preferred categories (4): [np.str_('baby_products'), np.str_('pet_supplies'), np.str_('beauty_health'), np.str_('crafts')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ baby_products: 2 items (20.0%)\n",
            "  ✓ beauty_health: 2 items (20.0%)\n",
            "  ✓ crafts: 4 items (40.0%)\n",
            "  ✓ pet_supplies: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '303' ---\n",
            "User demographics: 35-44, Male, Chicago\n",
            "Preferred categories (4): ['beauty_health', 'pet_supplies', 'baby_products', 'crafts']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ crafts: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 704\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '704' ---\n",
            "User demographics: 65+, Female, Houston\n",
            "Preferred categories (3): [np.str_('shoes'), np.str_('industrial'), np.str_('clothing')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '704' ---\n",
            "User demographics: 65+, Female, Houston\n",
            "Preferred categories (3): [np.str_('shoes'), np.str_('industrial'), np.str_('clothing')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 4 items (40.0%)\n",
            "  ✓ industrial: 3 items (30.0%)\n",
            "  ✓ shoes: 3 items (30.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '704' ---\n",
            "User demographics: 65+, Female, Houston\n",
            "Preferred categories (3): ['industrial', 'clothing', 'shoes']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 900\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '900' ---\n",
            "User demographics: 35-44, Female, Fort Worth\n",
            "Preferred categories (4): [np.str_('watches'), np.str_('clothing'), np.str_('automotive'), np.str_('home_garden')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ home_garden: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '900' ---\n",
            "User demographics: 35-44, Female, Fort Worth\n",
            "Preferred categories (4): [np.str_('watches'), np.str_('clothing'), np.str_('automotive'), np.str_('home_garden')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 2 items (20.0%)\n",
            "  ✓ clothing: 2 items (20.0%)\n",
            "  ✓ home_garden: 4 items (40.0%)\n",
            "  ✓ watches: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '900' ---\n",
            "User demographics: 35-44, Female, Fort Worth\n",
            "Preferred categories (4): ['clothing', 'watches', 'home_garden', 'automotive']\n",
            "Found 49 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ home_garden: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 604\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '604' ---\n",
            "User demographics: 55-64, Female, Chicago\n",
            "Preferred categories (5): [np.str_('home_garden'), np.str_('jewelry'), np.str_('crafts'), np.str_('baby_products'), np.str_('beauty_health')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ jewelry: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '604' ---\n",
            "User demographics: 55-64, Female, Chicago\n",
            "Preferred categories (5): [np.str_('home_garden'), np.str_('jewelry'), np.str_('crafts'), np.str_('baby_products'), np.str_('beauty_health')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ baby_products: 2 items (20.0%)\n",
            "  ✓ beauty_health: 2 items (20.0%)\n",
            "  ✓ crafts: 2 items (20.0%)\n",
            "  ✓ home_garden: 2 items (20.0%)\n",
            "  ✓ jewelry: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '604' ---\n",
            "User demographics: 55-64, Female, Chicago\n",
            "Preferred categories (5): ['beauty_health', 'crafts', 'baby_products', 'home_garden', 'jewelry']\n",
            "Found 49 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ jewelry: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 622\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '622' ---\n",
            "User demographics: 25-34, Female, Philadelphia\n",
            "Preferred categories (4): [np.str_('clothing'), np.str_('automotive'), np.str_('music'), np.str_('home_garden')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ home_garden: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '622' ---\n",
            "User demographics: 25-34, Female, Philadelphia\n",
            "Preferred categories (4): [np.str_('clothing'), np.str_('automotive'), np.str_('music'), np.str_('home_garden')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 2 items (20.0%)\n",
            "  ✓ clothing: 2 items (20.0%)\n",
            "  ✓ home_garden: 4 items (40.0%)\n",
            "  ✓ music: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '622' ---\n",
            "User demographics: 25-34, Female, Philadelphia\n",
            "Preferred categories (4): ['clothing', 'home_garden', 'music', 'automotive']\n",
            "Found 49 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ home_garden: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 299\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '299' ---\n",
            "User demographics: 25-34, Male, Austin\n",
            "Preferred categories (4): [np.str_('watches'), np.str_('office_supplies'), np.str_('clothing'), np.str_('kitchen_dining')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '299' ---\n",
            "User demographics: 25-34, Male, Austin\n",
            "Preferred categories (4): [np.str_('watches'), np.str_('office_supplies'), np.str_('clothing'), np.str_('kitchen_dining')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 4 items (40.0%)\n",
            "  ✓ kitchen_dining: 2 items (20.0%)\n",
            "  ✓ office_supplies: 2 items (20.0%)\n",
            "  ✓ watches: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '299' ---\n",
            "User demographics: 25-34, Male, Austin\n",
            "Preferred categories (4): ['office_supplies', 'kitchen_dining', 'watches', 'clothing']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ clothing: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 300\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '300' ---\n",
            "User demographics: 25-34, Male, Los Angeles\n",
            "Preferred categories (5): [np.str_('pet_supplies'), np.str_('sports_outdoors'), np.str_('jewelry'), np.str_('shoes'), np.str_('automotive')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ shoes: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '300' ---\n",
            "User demographics: 25-34, Male, Los Angeles\n",
            "Preferred categories (5): [np.str_('pet_supplies'), np.str_('sports_outdoors'), np.str_('jewelry'), np.str_('shoes'), np.str_('automotive')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 2 items (20.0%)\n",
            "  ✓ jewelry: 2 items (20.0%)\n",
            "  ✓ pet_supplies: 2 items (20.0%)\n",
            "  ✓ shoes: 2 items (20.0%)\n",
            "  ✓ sports_outdoors: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '300' ---\n",
            "User demographics: 25-34, Male, Los Angeles\n",
            "Preferred categories (5): ['pet_supplies', 'shoes', 'sports_outdoors', 'automotive', 'jewelry']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ shoes: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "OVERALL EVALUATION RESULTS\n",
            "============================================================\n",
            "Demographic Approach:\n",
            "  Average Accuracy: 100.0% (±0.0%)\n",
            "  Valid Evaluations: 10/10\n",
            "Balanced Approach:\n",
            "  Average Accuracy: 100.0% (±0.0%)\n",
            "  Valid Evaluations: 10/10\n",
            "Category Aware Approach:\n",
            "  Average Accuracy: 100.0% (±0.0%)\n",
            "  Valid Evaluations: 10/10\n",
            "\n",
            "============================================================\n",
            "RECOMMENDATION SYSTEM READY\n",
            "============================================================\n",
            "System trained and evaluated successfully!\n",
            "You can now use the interactive demo to explore recommendations.\n",
            "\n",
            "=== SYSTEM SUMMARY ===\n",
            "✓ Enhanced Item Tower: Trained with 10000 items across 20 categories\n",
            "✓ Enhanced User Tower: Trained with 1000 users with demographic features\n",
            "✓ Faiss Index: Contains 10000 normalized item embeddings\n",
            "✓ Training Data: 12064 user-item interactions\n",
            "✓ Evaluation: Completed on 10 test users\n",
            "\n",
            "Recommendation approaches implemented:\n",
            "  1. Demographic-based recommendations\n",
            "  2. Balanced category recommendations\n",
            "  3. Category-aware diversity recommendations\n",
            "\n",
            "To start the interactive demo, call: interactive_recommendation_demo()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Step 7: Interactive Recommendation Interface ---\n",
        "def interactive_recommendation_demo():\n",
        "    \"\"\"Interactive demo of the recommendation system\"\"\"\n",
        "    print(\"\\n=== INTERACTIVE RECOMMENDATION DEMO ===\")\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nChoose an option:\")\n",
        "        print(\"1. Get recommendations for a specific user\")\n",
        "        print(\"2. Get recommendations for a random user\")\n",
        "        print(\"3. Compare all approaches for a user\")\n",
        "        print(\"4. Show user demographics and preferences\")\n",
        "        print(\"5. Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                approach = input(\"Choose approach (demographic/balanced/category_aware): \").strip().lower()\n",
        "                if approach == 'demographic':\n",
        "                    get_demographic_recommendations(user_id)\n",
        "                elif approach == 'balanced':\n",
        "                    get_balanced_recommendations(user_id)\n",
        "                elif approach == 'category_aware':\n",
        "                    get_category_aware_recommendations(user_id)\n",
        "                else:\n",
        "                    print(\"Invalid approach. Using demographic approach.\")\n",
        "                    get_demographic_recommendations(user_id)\n",
        "            else:\n",
        "                print(f\"User {user_id} not found. Available users: {list(unique_user_ids)[:10]}...\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            user_id = np.random.choice(unique_user_ids)\n",
        "            print(f\"Selected random user: {user_id}\")\n",
        "            get_demographic_recommendations(user_id)\n",
        "\n",
        "        elif choice == '3':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                print(f\"Comparing all approaches for user {user_id}:\")\n",
        "                get_demographic_recommendations(user_id)\n",
        "                get_balanced_recommendations(user_id)\n",
        "                get_category_aware_recommendations(user_id)\n",
        "            else:\n",
        "                print(f\"User {user_id} not found.\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "                preferred_categories = user_categories[user_id]\n",
        "                user_interactions_count = len(interactions_df[interactions_df['user_id'] == user_id])\n",
        "\n",
        "                print(f\"\\nUser {user_id} Profile:\")\n",
        "                print(f\"  Demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "                print(f\"  Preferred Categories: {list(preferred_categories)}\")\n",
        "                print(\"breaking\")\n",
        "                print(f\"  Total Interactions: {user_interactions_count}\")\n",
        "\n",
        "                # Show actual interactions\n",
        "                user_interactions = interactions_df[interactions_df['user_id'] == user_id]\n",
        "                interaction_details = pd.merge(user_interactions, items_df, on='item_id')\n",
        "                category_breakdown = interaction_details['category'].value_counts()\n",
        "\n",
        "                print(\"  Interaction Breakdown:\")\n",
        "                for category, count in category_breakdown.items():\n",
        "                    print(f\"    {category}: {count} interactions\")\n",
        "            else:\n",
        "                print(f\"User {user_id} not found.\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = evaluate_recommendation_approaches()\n",
        "\n",
        "# Start interactive demo\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RECOMMENDATION SYSTEM READY\")\n",
        "print(\"=\"*60)\n",
        "print(\"System trained and evaluated successfully!\")\n",
        "print(\"You can now use the interactive demo to explore recommendations.\")\n",
        "\n",
        "# Uncomment the line below to start the interactive demo\n",
        "# interactive_recommendation_demo()\n",
        "\n",
        "print(\"\\n=== SYSTEM SUMMARY ===\")\n",
        "print(f\"✓ Enhanced Item Tower: Trained with {len(items_df)} items across {len(categories)} categories\")\n",
        "print(f\"✓ Enhanced User Tower: Trained with {len(users_df)} users with demographic features\")\n",
        "print(f\"✓ Faiss Index: Contains {index.ntotal} normalized item embeddings\")\n",
        "print(f\"✓ Training Data: {len(interactions_df)} user-item interactions\")\n",
        "print(f\"✓ Evaluation: Completed on {len(evaluation_results['demographic'])} test users\")\n",
        "print(\"\\nRecommendation approaches implemented:\")\n",
        "print(\"  1. Demographic-based recommendations\")\n",
        "print(\"  2. Balanced category recommendations\")\n",
        "print(\"  3. Category-aware diversity recommendations\")\n",
        "print(\"\\nTo start the interactive demo, call: interactive_recommendation_demo()\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M-cUjNfc2wZg"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPS94vrd345PE+WrXtmoM5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}