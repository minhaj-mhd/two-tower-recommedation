{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhaj-mhd/two-tower-recommedation/blob/main/two_tower_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSb8v80Z2v8D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#install packages\n"
      ],
      "metadata": {
        "id": "M-cUjNfc2wZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6rQ19NkZo8s",
        "outputId": "c88108aa-6b87-4e6c-e3e7-86f67fa2d2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Installing and upgrading all required packages...\n",
            "\n",
            "✅ All packages have been installed and upgraded.\n"
          ]
        }
      ],
      "source": [
        "print(\"⏳ Installing and upgrading all required packages...\")\n",
        "\n",
        "%pip install --upgrade -q tensorflow tensorflow-recommenders tf-keras tensorflow-text\n",
        "%pip install -q faiss-cpu\n",
        "\n",
        "print(\"\\n✅ All packages have been installed and upgraded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rbdXsqBCabJ4"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade -q tensorflow-decision-forests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_roixOESalk5",
        "outputId": "cb540946-87bf-4263-e62b-f86b1e715698",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow: 2.19.0\n",
            "tensorflow-recommenders: v0.7.3\n",
            "tf-keras: 2.19.0\n",
            "faiss-cpu: 1.11.0\n",
            "tensorflow-text: 2.19.0\n",
            "tensorflow-decision-forests: 1.12.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "import tf_keras\n",
        "import faiss\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"tensorflow-recommenders: {tfrs.__version__}\")\n",
        "print(f\"tf-keras: {tf_keras.__version__}\")\n",
        "print(f\"faiss-cpu: {faiss.__version__}\")\n",
        "print(f\"tensorflow-text: {tf_text.__version__}\")\n",
        "print(f\"tensorflow-decision-forests: {tfdf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Two tower model"
      ],
      "metadata": {
        "id": "FMnfU2bI2cdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "\n"
      ],
      "metadata": {
        "id": "_SUBQRpK4Boa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Fabricate Data with 20 Categories and Enhanced User Demographics ---\n",
        "print(\"[1] Fabricating data with 20 categories and enhanced user demographics...\")\n",
        "\n",
        "# 20 diverse categories\n",
        "categories = [\n",
        "    \"electronics\", \"clothing\", \"books\", \"home_garden\", \"sports_outdoors\",\n",
        "    \"beauty_health\", \"automotive\", \"toys_games\", \"jewelry\", \"music\",\n",
        "    \"movies_tv\", \"kitchen_dining\", \"office_supplies\", \"pet_supplies\", \"crafts\",\n",
        "    \"industrial\", \"grocery\", \"baby_products\", \"shoes\", \"watches\"\n",
        "]\n",
        "\n",
        "# User demographics data\n",
        "age_groups = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"]\n",
        "locations = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\",\n",
        "            \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\", \"Austin\", \"Jacksonville\",\n",
        "            \"Fort Worth\", \"Columbus\", \"Charlotte\", \"Seattle\", \"Denver\", \"Boston\"]\n",
        "genders = [\"Male\", \"Female\", \"Other\"]\n",
        "\n",
        "num_items = 10000  # Increased items to accommodate more categories\n",
        "num_users = 1000   # Increased users for better diversity\n",
        "interactions_per_user = 50  # Target interactions per user\n",
        "\n",
        "# Create items with explicit category tracking\n",
        "item_titles = [f\"Product {i}\" for i in range(num_items)]\n",
        "item_categories = [categories[i % len(categories)] for i in range(num_items)]\n",
        "\n",
        "# Create diverse descriptions based on category\n",
        "description_templates = {\n",
        "    \"electronics\": [\n",
        "        \"High-tech electronic device with advanced features\",\n",
        "        \"Smart gadget with wireless connectivity and AI integration\",\n",
        "        \"Innovative electronic tool for modern digital life\",\n",
        "        \"Premium electronic device with cutting-edge technology\"\n",
        "    ],\n",
        "    \"clothing\": [\n",
        "        \"Fashionable apparel made from quality sustainable materials\",\n",
        "        \"Comfortable and stylish garment for everyday wear\",\n",
        "        \"Trendy clothing with modern design and premium fabric\",\n",
        "        \"Versatile wardrobe piece suitable for various occasions\"\n",
        "    ],\n",
        "    \"books\": [\n",
        "        \"Educational book covering important academic topics\",\n",
        "        \"Engaging literature for avid readers and book enthusiasts\",\n",
        "        \"Informative guide with practical knowledge and insights\",\n",
        "        \"Bestselling book with compelling storytelling and research\"\n",
        "    ],\n",
        "    \"home_garden\": [\n",
        "        \"Durable home improvement tool for DIY projects\",\n",
        "        \"Garden equipment for landscaping and plant care\",\n",
        "        \"Home decor item to enhance living space aesthetics\",\n",
        "        \"Functional household item for daily convenience\"\n",
        "    ],\n",
        "    \"sports_outdoors\": [\n",
        "        \"Professional sports equipment for athletic performance\",\n",
        "        \"Outdoor gear for adventure and recreational activities\",\n",
        "        \"Fitness equipment for home workout routines\",\n",
        "        \"Camping and hiking essentials for outdoor enthusiasts\"\n",
        "    ],\n",
        "    \"beauty_health\": [\n",
        "        \"Premium skincare product with natural ingredients\",\n",
        "        \"Health supplement for wellness and vitality\",\n",
        "        \"Cosmetic item for beauty enhancement and self-care\",\n",
        "        \"Personal care product for daily hygiene routine\"\n",
        "    ],\n",
        "    \"automotive\": [\n",
        "        \"High-quality automotive part for vehicle maintenance\",\n",
        "        \"Car accessory for enhanced driving experience\",\n",
        "        \"Professional-grade tool for automotive repair\",\n",
        "        \"Vehicle enhancement product for performance optimization\"\n",
        "    ],\n",
        "    \"toys_games\": [\n",
        "        \"Educational toy for children's development and learning\",\n",
        "        \"Board game for family entertainment and bonding\",\n",
        "        \"Creative plaything that sparks imagination and creativity\",\n",
        "        \"Interactive game for skill development and fun\"\n",
        "    ],\n",
        "    \"jewelry\": [\n",
        "        \"Elegant jewelry piece crafted with precious metals\",\n",
        "        \"Stylish accessory for fashion and personal expression\",\n",
        "        \"Handcrafted jewelry with unique design elements\",\n",
        "        \"Luxury jewelry item for special occasions\"\n",
        "    ],\n",
        "    \"music\": [\n",
        "        \"Professional music equipment for audio production\",\n",
        "        \"Musical instrument for creative expression and performance\",\n",
        "        \"High-quality audio device for music enthusiasts\",\n",
        "        \"Music accessory for enhanced listening experience\"\n",
        "    ],\n",
        "    \"movies_tv\": [\n",
        "        \"Entertainment media for leisure and relaxation\",\n",
        "        \"Classic film collection for movie enthusiasts\",\n",
        "        \"TV series with compelling storylines and characters\",\n",
        "        \"Documentary content for educational entertainment\"\n",
        "    ],\n",
        "    \"kitchen_dining\": [\n",
        "        \"Professional kitchen utensil for culinary excellence\",\n",
        "        \"Dining accessory for elegant meal presentation\",\n",
        "        \"Cooking tool made from premium food-safe materials\",\n",
        "        \"Kitchen gadget for efficient food preparation\"\n",
        "    ],\n",
        "    \"office_supplies\": [\n",
        "        \"Professional office equipment for workplace productivity\",\n",
        "        \"Stationery item for organization and documentation\",\n",
        "        \"Ergonomic office accessory for comfort and efficiency\",\n",
        "        \"Business tool for professional operations\"\n",
        "    ],\n",
        "    \"pet_supplies\": [\n",
        "        \"Pet care product for animal health and happiness\",\n",
        "        \"Pet toy for entertainment and exercise\",\n",
        "        \"Pet accessory for comfort and safety\",\n",
        "        \"Pet nutrition product for optimal health\"\n",
        "    ],\n",
        "    \"crafts\": [\n",
        "        \"Art supply for creative projects and expression\",\n",
        "        \"Craft material for DIY projects and hobbies\",\n",
        "        \"Creative tool for artistic endeavors and crafting\",\n",
        "        \"Handcraft supply for personalized creations\"\n",
        "    ],\n",
        "    \"industrial\": [\n",
        "        \"Industrial equipment for manufacturing and production\",\n",
        "        \"Heavy-duty tool for professional industrial use\",\n",
        "        \"Machinery component for industrial operations\",\n",
        "        \"Professional-grade equipment for industrial applications\"\n",
        "    ],\n",
        "    \"grocery\": [\n",
        "        \"Premium food product for nutritious meals\",\n",
        "        \"Organic ingredient for healthy cooking\",\n",
        "        \"Gourmet food item for culinary excellence\",\n",
        "        \"Essential grocery item for daily nutrition\"\n",
        "    ],\n",
        "    \"baby_products\": [\n",
        "        \"Safe baby product for infant care and development\",\n",
        "        \"Baby accessory for comfort and convenience\",\n",
        "        \"Child safety item for protection and security\",\n",
        "        \"Developmental toy for early childhood learning\"\n",
        "    ],\n",
        "    \"shoes\": [\n",
        "        \"Comfortable footwear for daily wear and activities\",\n",
        "        \"Athletic shoe for sports and fitness activities\",\n",
        "        \"Fashion shoe for style and personal expression\",\n",
        "        \"Professional footwear for workplace requirements\"\n",
        "    ],\n",
        "    \"watches\": [\n",
        "        \"Precision timepiece with advanced features\",\n",
        "        \"Luxury watch for style and status\",\n",
        "        \"Sports watch for active lifestyle tracking\",\n",
        "        \"Smart watch with digital connectivity features\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "item_descriptions = []\n",
        "for i in range(num_items):\n",
        "    category = item_categories[i]\n",
        "    template = description_templates[category][i % len(description_templates[category])]\n",
        "    item_descriptions.append(f\"{template}. Model v{i % 15}. Item #{i}.\")\n",
        "\n",
        "items_data = {\n",
        "    \"item_id\": [str(i) for i in range(num_items)],\n",
        "    \"item_title\": item_titles,\n",
        "    \"item_description\": item_descriptions,\n",
        "    \"category\": item_categories\n",
        "}\n",
        "items_df = pd.DataFrame(items_data)\n",
        "\n",
        "# Create category-to-items mapping for easier lookup\n",
        "category_to_items = defaultdict(list)\n",
        "for idx, row in items_df.iterrows():\n",
        "    category_to_items[row['category']].append(row['item_id'])\n",
        "\n",
        "# Generate enhanced user demographics data\n",
        "print(\"Generating enhanced user demographics...\")\n",
        "user_demographics = []\n",
        "for user_id in range(num_users):\n",
        "    # Generate demographics with realistic distributions\n",
        "    age_group = np.random.choice(age_groups, p=[0.15, 0.25, 0.22, 0.18, 0.12, 0.08])  # Weighted towards younger users\n",
        "    location = np.random.choice(locations)\n",
        "    gender = np.random.choice(genders, p=[0.48, 0.50, 0.02])  # Realistic gender distribution\n",
        "\n",
        "    user_demographics.append({\n",
        "        \"user_id\": str(user_id),\n",
        "        \"age_group\": age_group,\n",
        "        \"location\": location,\n",
        "        \"gender\": gender\n",
        "    })\n",
        "\n",
        "users_df = pd.DataFrame(user_demographics)\n",
        "\n",
        "# Generate user interactions with demographic influence on preferences\n",
        "print(\"Generating user interactions with demographic-influenced preferences...\")\n",
        "user_interactions = []\n",
        "user_categories = {}  # Track which categories each user prefers\n",
        "\n",
        "# Define demographic preferences (realistic patterns)\n",
        "demographic_preferences = {\n",
        "    \"age_group\": {\n",
        "        \"18-24\": {\"electronics\": 1.5, \"clothing\": 1.4, \"music\": 1.3, \"toys_games\": 1.2},\n",
        "        \"25-34\": {\"electronics\": 1.3, \"home_garden\": 1.2, \"clothing\": 1.2, \"automotive\": 1.1},\n",
        "        \"35-44\": {\"home_garden\": 1.4, \"automotive\": 1.3, \"office_supplies\": 1.2, \"baby_products\": 1.5},\n",
        "        \"45-54\": {\"home_garden\": 1.3, \"automotive\": 1.2, \"books\": 1.2, \"kitchen_dining\": 1.1},\n",
        "        \"55-64\": {\"books\": 1.3, \"home_garden\": 1.2, \"kitchen_dining\": 1.2, \"beauty_health\": 1.1},\n",
        "        \"65+\": {\"books\": 1.4, \"home_garden\": 1.3, \"beauty_health\": 1.2, \"kitchen_dining\": 1.1}\n",
        "    },\n",
        "    \"gender\": {\n",
        "        \"Male\": {\"electronics\": 1.3, \"automotive\": 1.4, \"sports_outdoors\": 1.3, \"tools\": 1.2},\n",
        "        \"Female\": {\"clothing\": 1.4, \"beauty_health\": 1.5, \"jewelry\": 1.3, \"baby_products\": 1.2},\n",
        "        \"Other\": {\"clothing\": 1.1, \"electronics\": 1.1, \"books\": 1.2, \"music\": 1.1}\n",
        "    }\n",
        "}\n",
        "\n",
        "for user_id in range(num_users):\n",
        "    user_demo = users_df[users_df['user_id'] == str(user_id)].iloc[0]\n",
        "\n",
        "    # Calculate category preferences based on demographics\n",
        "    category_scores = {}\n",
        "    for category in categories:\n",
        "        base_score = 1.0\n",
        "\n",
        "        # Age group influence\n",
        "        age_prefs = demographic_preferences[\"age_group\"].get(user_demo['age_group'], {})\n",
        "        age_multiplier = age_prefs.get(category, 1.0)\n",
        "\n",
        "        # Gender influence\n",
        "        gender_prefs = demographic_preferences[\"gender\"].get(user_demo['gender'], {})\n",
        "        gender_multiplier = gender_prefs.get(category, 1.0)\n",
        "\n",
        "        # Combine influences\n",
        "        category_scores[category] = base_score * age_multiplier * gender_multiplier\n",
        "\n",
        "    # Select 3-5 categories based on weighted preferences\n",
        "    num_categories = np.random.randint(3, 6)\n",
        "    category_weights = np.array([category_scores[cat] for cat in categories])\n",
        "    category_weights = category_weights / np.sum(category_weights)  # Normalize\n",
        "\n",
        "    preferred_categories = np.random.choice(categories, size=num_categories, replace=False, p=category_weights)\n",
        "    user_categories[str(user_id)] = preferred_categories\n",
        "\n",
        "    # Generate exactly 50 interactions per user\n",
        "    interactions_generated = 0\n",
        "    while interactions_generated < interactions_per_user:\n",
        "        # Select categories with weighted preference (more interactions in preferred categories)\n",
        "        category_selection_weights = np.array([category_scores[cat] for cat in preferred_categories])\n",
        "        category_selection_weights = category_selection_weights / np.sum(category_selection_weights)\n",
        "        selected_category = np.random.choice(preferred_categories, p=category_selection_weights)\n",
        "\n",
        "        # Select random item from this category\n",
        "        available_items = category_to_items[selected_category]\n",
        "        selected_item = np.random.choice(available_items)\n",
        "\n",
        "        user_interactions.append({\n",
        "            \"user_id\": str(user_id),\n",
        "            \"item_id\": selected_item\n",
        "        })\n",
        "\n",
        "        interactions_generated += 1\n",
        "\n",
        "interactions_df = pd.DataFrame(user_interactions)\n",
        "\n",
        "# Calculate statistics\n",
        "total_interactions = len(interactions_df)\n",
        "avg_interactions_per_user = total_interactions / num_users\n",
        "\n",
        "print(f\"Generated {len(items_df)} items across {len(categories)} categories\")\n",
        "print(f\"Generated {total_interactions} interactions from {num_users} users\")\n",
        "print(f\"Average interactions per user: {avg_interactions_per_user:.1f}\")\n",
        "\n",
        "# Verify that each user has exactly 50 interactions\n",
        "user_interaction_counts = interactions_df['user_id'].value_counts()\n",
        "print(f\"Min interactions per user: {user_interaction_counts.min()}\")\n",
        "print(f\"Max interactions per user: {user_interaction_counts.max()}\")\n",
        "print(f\"All users have exactly 50 interactions: {all(user_interaction_counts == 50)}\")\n",
        "\n",
        "# Display demographic distribution\n",
        "print(f\"\\nUser demographic distribution:\")\n",
        "print(\"Age groups:\")\n",
        "for age_group in age_groups:\n",
        "    count = len(users_df[users_df['age_group'] == age_group])\n",
        "    print(f\"  {age_group}: {count} users ({count/num_users:.1%})\")\n",
        "\n",
        "print(\"Gender distribution:\")\n",
        "for gender in genders:\n",
        "    count = len(users_df[users_df['gender'] == gender])\n",
        "    print(f\"  {gender}: {count} users ({count/num_users:.1%})\")\n",
        "\n",
        "# Display sample user profiles\n",
        "print(\"\\nSample user profiles:\")\n",
        "for i in range(5):\n",
        "    user_id = str(i)\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    cats = user_categories[user_id]\n",
        "    user_interactions_count = len(interactions_df[interactions_df['user_id'] == user_id])\n",
        "    print(f\"User {user_id}: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"  Categories: {list(cats)} ({user_interactions_count} interactions)\")\n",
        "\n",
        "items_ds = tf.data.Dataset.from_tensor_slices(dict(items_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bB1HYdZU39Ss",
        "outputId": "656b757c-dcd5-403d-abc4-8c18e0b5a414"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Fabricating data with 20 categories and enhanced user demographics...\n",
            "Generating enhanced user demographics...\n",
            "Generating user interactions with demographic-influenced preferences...\n",
            "Generated 10000 items across 20 categories\n",
            "Generated 50000 interactions from 1000 users\n",
            "Average interactions per user: 50.0\n",
            "Min interactions per user: 50\n",
            "Max interactions per user: 50\n",
            "All users have exactly 50 interactions: True\n",
            "\n",
            "User demographic distribution:\n",
            "Age groups:\n",
            "  18-24: 155 users (15.5%)\n",
            "  25-34: 229 users (22.9%)\n",
            "  35-44: 225 users (22.5%)\n",
            "  45-54: 192 users (19.2%)\n",
            "  55-64: 126 users (12.6%)\n",
            "  65+: 73 users (7.3%)\n",
            "Gender distribution:\n",
            "  Male: 486 users (48.6%)\n",
            "  Female: 494 users (49.4%)\n",
            "  Other: 20 users (2.0%)\n",
            "\n",
            "Sample user profiles:\n",
            "User 0: 25-34, Male, Denver\n",
            "  Categories: [np.str_('jewelry'), np.str_('sports_outdoors'), np.str_('clothing'), np.str_('office_supplies')] (50 interactions)\n",
            "User 1: 35-44, Female, Austin\n",
            "  Categories: [np.str_('movies_tv'), np.str_('electronics'), np.str_('clothing')] (50 interactions)\n",
            "User 2: 25-34, Male, Columbus\n",
            "  Categories: [np.str_('office_supplies'), np.str_('jewelry'), np.str_('grocery'), np.str_('kitchen_dining')] (50 interactions)\n",
            "User 3: 18-24, Female, Los Angeles\n",
            "  Categories: [np.str_('grocery'), np.str_('baby_products'), np.str_('sports_outdoors')] (50 interactions)\n",
            "User 4: 55-64, Female, San Jose\n",
            "  Categories: [np.str_('movies_tv'), np.str_('beauty_health'), np.str_('watches')] (50 interactions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Enhanced Item Tower with Category Integration ---\n",
        "print(\"\\n[2] Building and training the enhanced Item Tower with category integration...\")\n",
        "embedding_dimension = 128\n",
        "max_tokens = 15_000\n",
        "sequence_length = 120\n",
        "\n",
        "# Create text vectorizers for both description and category\n",
        "description_vectorizer = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_sequence_length=sequence_length,\n",
        "    name=\"description_vectorizer\"\n",
        ")\n",
        "description_vectorizer.adapt(items_ds.map(lambda x: x[\"item_description\"]).batch(256))\n",
        "\n",
        "# Create category vectorizer (much smaller vocabulary)\n",
        "category_vectorizer = TextVectorization(\n",
        "    max_tokens=len(categories) + 10,  # Small vocabulary for categories\n",
        "    output_sequence_length=1,  # Categories are single tokens\n",
        "    name=\"category_vectorizer\"\n",
        ")\n",
        "category_vectorizer.adapt(items_ds.map(lambda x: x[\"category\"]).batch(256))\n",
        "\n",
        "class EnhancedItemModel(tf.keras.Model):\n",
        "    def __init__(self, description_vectorizer, category_vectorizer, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.description_vectorizer = description_vectorizer\n",
        "        self.category_vectorizer = category_vectorizer\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Description embedding branch\n",
        "        self.description_embedding = tf.keras.Sequential([\n",
        "            self.description_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.description_vectorizer.vocabulary_size(),\n",
        "                output_dim=embedding_dim,\n",
        "                mask_zero=True,\n",
        "                name=\"description_embedding\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(name=\"description_pooling\"),\n",
        "        ], name=\"description_branch\")\n",
        "\n",
        "        # Category embedding branch\n",
        "        self.category_embedding = tf.keras.Sequential([\n",
        "            self.category_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.category_vectorizer.vocabulary_size(),\n",
        "                output_dim=32,  # Smaller embedding for categories\n",
        "                mask_zero=True,\n",
        "                name=\"category_embedding\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(name=\"category_pooling\"),\n",
        "        ], name=\"category_branch\")\n",
        "\n",
        "        # Fusion layer to combine description and category embeddings\n",
        "        self.fusion_dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\", name=\"fusion_dense_1\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"fusion_bn_1\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"fusion_dropout_1\"),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\", name=\"fusion_dense_2\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"fusion_bn_2\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"fusion_dropout_2\"),\n",
        "            tf.keras.layers.Dense(embedding_dim, name=\"fusion_output\")\n",
        "        ], name=\"fusion_layer\")\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Process description\n",
        "        description_emb = self.description_embedding(inputs[\"item_description\"])\n",
        "\n",
        "        # Process category\n",
        "        category_emb = self.category_embedding(inputs[\"category\"])\n",
        "\n",
        "        # Concatenate description and category embeddings\n",
        "        combined = tf.concat([description_emb, category_emb], axis=1)\n",
        "\n",
        "        # Apply fusion layer\n",
        "        output = self.fusion_dense(combined, training=training)\n",
        "\n",
        "        return output\n",
        "\n",
        "class SelfSupervisedItemTwoTower(tfrs.Model):\n",
        "    def __init__(self, item_model):\n",
        "        super().__init__()\n",
        "        self.item_model = item_model\n",
        "        self.task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        item_embeddings = self.item_model(features, training=training)\n",
        "        return self.task(query_embeddings=item_embeddings, candidate_embeddings=item_embeddings)\n",
        "\n",
        "# Initialize the enhanced item tower\n",
        "item_tower = EnhancedItemModel(description_vectorizer, category_vectorizer, embedding_dimension)\n",
        "item_model_trainer = SelfSupervisedItemTwoTower(item_tower)\n",
        "item_model_trainer.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "# Prepare training data with both description and category\n",
        "train_item_ds = items_ds.map(lambda x: {\n",
        "    \"item_description\": x[\"item_description\"],\n",
        "    \"category\": x[\"category\"]\n",
        "}).batch(512).cache()\n",
        "\n",
        "print(\"Training Enhanced Item Tower with description and category...\")\n",
        "item_model_trainer.fit(train_item_ds, epochs=15, verbose=1)\n",
        "print(\"Enhanced Item Tower training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9qVgZbUs4Ruk",
        "outputId": "dc25ff86-841c-428e-bdee-ef32bdf1bdc1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2] Building and training the enhanced Item Tower with category integration...\n",
            "Training Enhanced Item Tower with description and category...\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 3.3528 - regularization_loss: 0.0000e+00 - total_loss: 3.3528\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2479 - regularization_loss: 0.0000e+00 - total_loss: 2.2479\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.7173 - regularization_loss: 0.0000e+00 - total_loss: 1.7173\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 1.2098 - regularization_loss: 0.0000e+00 - total_loss: 1.2098\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.1805 - regularization_loss: 0.0000e+00 - total_loss: 1.1805\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.0986 - regularization_loss: 0.0000e+00 - total_loss: 1.0986\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7120 - regularization_loss: 0.0000e+00 - total_loss: 3.7120\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.8893 - regularization_loss: 0.0000e+00 - total_loss: 1.8893\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2997 - regularization_loss: 0.0000e+00 - total_loss: 0.2997\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6370 - regularization_loss: 0.0000e+00 - total_loss: 0.6370\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2181 - regularization_loss: 0.0000e+00 - total_loss: 0.2181\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4071 - regularization_loss: 0.0000e+00 - total_loss: 0.4071\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2265 - regularization_loss: 0.0000e+00 - total_loss: 0.2265\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8430 - regularization_loss: 0.0000e+00 - total_loss: 0.8430\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0548 - regularization_loss: 0.0000e+00 - total_loss: 0.0548\n",
            "Enhanced Item Tower training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Generate and Store Item Embeddings in Faiss ---\n",
        "print(\"\\n[3] Generating item embeddings and storing in Faiss...\")\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "# Generate embeddings using both description and category\n",
        "item_embeddings_generator = items_ds.batch(512).map(lambda x: item_tower({\n",
        "    \"item_description\": x[\"item_description\"],\n",
        "    \"category\": x[\"category\"]\n",
        "}))\n",
        "all_item_embeddings = np.concatenate(list(item_embeddings_generator.as_numpy_iterator()))\n",
        "\n",
        "# Normalize embeddings for better similarity search\n",
        "all_item_embeddings = all_item_embeddings / np.linalg.norm(all_item_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "index.add(all_item_embeddings)\n",
        "print(f\"Faiss index now contains {index.ntotal} vectors.\")\n",
        "index_to_item_id = {i: item_id for i, item_id in enumerate(items_df[\"item_id\"])}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e69zBRe4UzL",
        "outputId": "2a34c12f-a060-411a-ec31-0490268b9941"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3] Generating item embeddings and storing in Faiss...\n",
            "Faiss index now contains 10000 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Enhanced User Tower with Demographics and Joint Training ---\n",
        "print(\"\\n[4] Building and training the Enhanced User Tower with demographics and joint training...\")\n",
        "unique_user_ids = interactions_df[\"user_id\"].unique()\n",
        "\n",
        "# Create dataset from user demographics\n",
        "users_ds = tf.data.Dataset.from_tensor_slices(dict(users_df))\n",
        "\n",
        "# Create vectorizers for demographic features\n",
        "age_group_vectorizer = TextVectorization(\n",
        "    max_tokens=len(age_groups) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"age_group_vectorizer\"\n",
        ")\n",
        "age_group_vectorizer.adapt(users_ds.map(lambda x: x[\"age_group\"]).batch(256))\n",
        "\n",
        "location_vectorizer = TextVectorization(\n",
        "    max_tokens=len(locations) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"location_vectorizer\"\n",
        ")\n",
        "location_vectorizer.adapt(users_ds.map(lambda x: x[\"location\"]).batch(256))\n",
        "\n",
        "gender_vectorizer = TextVectorization(\n",
        "    max_tokens=len(genders) + 5,\n",
        "    output_sequence_length=1,\n",
        "    name=\"gender_vectorizer\"\n",
        ")\n",
        "gender_vectorizer.adapt(users_ds.map(lambda x: x[\"gender\"]).batch(256))\n",
        "\n",
        "class EnhancedUserModel(tf.keras.Model):\n",
        "    def __init__(self, user_ids, age_group_vectorizer, location_vectorizer, gender_vectorizer):\n",
        "        super().__init__()\n",
        "        self.age_group_vectorizer = age_group_vectorizer\n",
        "        self.location_vectorizer = location_vectorizer\n",
        "        self.gender_vectorizer = gender_vectorizer\n",
        "\n",
        "        # User ID embedding\n",
        "        self.user_id_embedding = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=user_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(user_ids) + 1, 64, name=\"user_id_emb\")\n",
        "        ], name=\"user_id_branch\")\n",
        "\n",
        "        # Age group embedding\n",
        "        self.age_group_embedding = tf.keras.Sequential([\n",
        "            self.age_group_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.age_group_vectorizer.vocabulary_size(),\n",
        "                output_dim=16,\n",
        "                mask_zero=True,\n",
        "                name=\"age_group_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"age_group_branch\")\n",
        "\n",
        "        # Location embedding\n",
        "        self.location_embedding = tf.keras.Sequential([\n",
        "            self.location_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.location_vectorizer.vocabulary_size(),\n",
        "                output_dim=32,\n",
        "                mask_zero=True,\n",
        "                name=\"location_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"location_branch\")\n",
        "\n",
        "        # Gender embedding\n",
        "        self.gender_embedding = tf.keras.Sequential([\n",
        "            self.gender_vectorizer,\n",
        "            tf.keras.layers.Embedding(\n",
        "                input_dim=self.gender_vectorizer.vocabulary_size(),\n",
        "                output_dim=8,\n",
        "                mask_zero=True,\n",
        "                name=\"gender_emb\"\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        ], name=\"gender_branch\")\n",
        "\n",
        "        # Fusion layers to combine all user features\n",
        "        self.fusion_dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\", name=\"user_fusion_dense_1\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"user_fusion_bn_1\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"user_fusion_dropout_1\"),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\", name=\"user_fusion_dense_2\"),\n",
        "            tf.keras.layers.BatchNormalization(name=\"user_fusion_bn_2\"),\n",
        "            tf.keras.layers.Dropout(0.3, name=\"user_fusion_dropout_2\"),\n",
        "            tf.keras.layers.Dense(embedding_dimension, name=\"user_fusion_output\")\n",
        "        ], name=\"user_fusion_layer\")\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Process user ID\n",
        "        user_id_emb = self.user_id_embedding(inputs[\"user_id\"])\n",
        "\n",
        "        # Process demographics\n",
        "        age_group_emb = self.age_group_embedding(inputs[\"age_group\"])\n",
        "        location_emb = self.location_embedding(inputs[\"location\"])\n",
        "        gender_emb = self.gender_embedding(inputs[\"gender\"])\n",
        "\n",
        "        # Concatenate all user features\n",
        "        combined = tf.concat([user_id_emb, age_group_emb, location_emb, gender_emb], axis=1)\n",
        "\n",
        "        # Apply fusion layer\n",
        "        output = self.fusion_dense(combined, training=training)\n",
        "\n",
        "        return output\n",
        "\n",
        "class JointTrainingModel(tfrs.Model):\n",
        "    \"\"\"\n",
        "    Joint training model that updates both user and item towers simultaneously.\n",
        "    This allows the model to learn better representations by considering user-item interactions.\n",
        "    \"\"\"\n",
        "    def __init__(self, user_model, item_model, rating_weight=1.0, retrieval_weight=1.0):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.item_model = item_model\n",
        "\n",
        "        # Both models are now trainable for joint training\n",
        "        self.user_model.trainable = True\n",
        "        self.item_model.trainable = True\n",
        "\n",
        "        # Multi-task learning: retrieval + rating prediction\n",
        "        self.retrieval_task = tfrs.tasks.Retrieval(\n",
        "            name=\"retrieval_task\"\n",
        "        )\n",
        "\n",
        "        self.rating_task = tfrs.tasks.Ranking(\n",
        "            loss=tf.keras.losses.MeanSquaredError(),\n",
        "            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        "            name=\"rating_task\"\n",
        "        )\n",
        "\n",
        "        self.rating_weight = rating_weight\n",
        "        self.retrieval_weight = retrieval_weight\n",
        "\n",
        "        # Additional regularization for joint training\n",
        "        self.user_reg = tf.keras.regularizers.l2(0.001)\n",
        "        self.item_reg = tf.keras.regularizers.l2(0.001)\n",
        "\n",
        "    def call(self, features, training=None):\n",
        "        \"\"\"Forward pass through both towers\"\"\"\n",
        "        # User tower\n",
        "        user_embeddings = self.user_model({\n",
        "            \"user_id\": features[\"user_id\"],\n",
        "            \"age_group\": features[\"age_group\"],\n",
        "            \"location\": features[\"location\"],\n",
        "            \"gender\": features[\"gender\"]\n",
        "        }, training=training)\n",
        "\n",
        "        # Item tower\n",
        "        item_embeddings = self.item_model({\n",
        "            \"item_description\": features[\"item_description\"],\n",
        "            \"category\": features[\"category\"]\n",
        "        }, training=training)\n",
        "\n",
        "        return {\n",
        "            \"user_embeddings\": user_embeddings,\n",
        "            \"item_embeddings\": item_embeddings,\n",
        "            \"predicted_rating\": tf.reduce_sum(user_embeddings * item_embeddings, axis=1)\n",
        "        }\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        \"\"\"Compute joint loss for both retrieval and rating tasks\"\"\"\n",
        "        # Get embeddings\n",
        "        user_embeddings = self.user_model({\n",
        "            \"user_id\": features[\"user_id\"],\n",
        "            \"age_group\": features[\"age_group\"],\n",
        "            \"location\": features[\"location\"],\n",
        "            \"gender\": features[\"gender\"]\n",
        "        }, training=training)\n",
        "\n",
        "        item_embeddings = self.item_model({\n",
        "            \"item_description\": features[\"item_description\"],\n",
        "            \"category\": features[\"category\"]\n",
        "        }, training=training)\n",
        "\n",
        "        # Normalize embeddings for retrieval task\n",
        "        user_embeddings_norm = tf.nn.l2_normalize(user_embeddings, axis=1)\n",
        "        item_embeddings_norm = tf.nn.l2_normalize(item_embeddings, axis=1)\n",
        "\n",
        "        # Retrieval loss\n",
        "        retrieval_loss = self.retrieval_task(\n",
        "            query_embeddings=user_embeddings_norm,\n",
        "            candidate_embeddings=item_embeddings_norm\n",
        "        )\n",
        "\n",
        "        # Rating prediction loss (using dot product)\n",
        "        predicted_rating = tf.reduce_sum(user_embeddings * item_embeddings, axis=1)\n",
        "        rating_loss = self.rating_task(\n",
        "            labels=features[\"label\"],\n",
        "            predictions=predicted_rating\n",
        "        )\n",
        "\n",
        "        # Regularization losses\n",
        "        user_reg_loss = tf.add_n([\n",
        "            self.user_reg(weight) for weight in self.user_model.trainable_weights\n",
        "        ]) if self.user_model.trainable_weights else 0\n",
        "\n",
        "        item_reg_loss = tf.add_n([\n",
        "            self.item_reg(weight) for weight in self.item_model.trainable_weights\n",
        "        ]) if self.item_model.trainable_weights else 0\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = (\n",
        "            self.retrieval_weight * retrieval_loss\n",
        "            + self.rating_weight * rating_loss\n",
        "            + user_reg_loss\n",
        "            + item_reg_loss\n",
        "        )\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "# Prepare training data with user demographics and item details\n",
        "interactions_with_details_df = pd.merge(\n",
        "    interactions_df,\n",
        "    items_df[['item_id', 'item_description', 'category']],\n",
        "    on='item_id'\n",
        ")\n",
        "\n",
        "# Add user demographics to interactions\n",
        "interactions_with_details_df = pd.merge(\n",
        "    interactions_with_details_df,\n",
        "    users_df[['user_id', 'age_group', 'location', 'gender']],\n",
        "    on='user_id'\n",
        ")\n",
        "\n",
        "# Create negative sampling for better training\n",
        "print(\"Creating training dataset with negative sampling...\")\n",
        "positive_interactions = interactions_with_details_df.copy()\n",
        "positive_interactions['label'] = 1.0\n",
        "\n",
        "# Create negative samples\n",
        "negative_interactions = []\n",
        "for user_id in unique_user_ids:\n",
        "    user_positive_items = set(positive_interactions[positive_interactions['user_id'] == user_id]['item_id'])\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "\n",
        "    # Sample negative items (items the user hasn't interacted with)\n",
        "    num_negatives = min(len(user_positive_items), 30)  # Increased negatives for better training\n",
        "    all_items = set(items_df['item_id'])\n",
        "    negative_items = list(all_items - user_positive_items)\n",
        "\n",
        "    if len(negative_items) >= num_negatives:\n",
        "        sampled_negatives = np.random.choice(negative_items, size=num_negatives, replace=False)\n",
        "\n",
        "        for item_id in sampled_negatives:\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            negative_interactions.append({\n",
        "                'user_id': user_id,\n",
        "                'item_id': item_id,\n",
        "                'item_description': item_row['item_description'],\n",
        "                'category': item_row['category'],\n",
        "                'age_group': user_demo['age_group'],\n",
        "                'location': user_demo['location'],\n",
        "                'gender': user_demo['gender'],\n",
        "                'label': 0.0\n",
        "            })\n",
        "\n",
        "negative_interactions_df = pd.DataFrame(negative_interactions)\n",
        "full_training_df = pd.concat([positive_interactions, negative_interactions_df], ignore_index=True)\n",
        "\n",
        "# Shuffle the training data\n",
        "full_training_df = full_training_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(f\"Training dataset size: {len(full_training_df)}\")\n",
        "print(f\"Positive samples: {len(positive_interactions)}\")\n",
        "print(f\"Negative samples: {len(negative_interactions_df)}\")\n",
        "\n",
        "# Create training dataset\n",
        "full_interactions_ds = tf.data.Dataset.from_tensor_slices(dict(full_training_df))\n",
        "train_ds_joint = full_interactions_ds.shuffle(100_000).batch(512).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create validation dataset (20% of data)\n",
        "val_size = int(0.2 * len(full_training_df))\n",
        "val_df = full_training_df.tail(val_size)\n",
        "train_df = full_training_df.head(len(full_training_df) - val_size)\n",
        "\n",
        "val_interactions_ds = tf.data.Dataset.from_tensor_slices(dict(val_df))\n",
        "val_ds_joint = val_interactions_ds.batch(512).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Initialize models\n",
        "user_tower = EnhancedUserModel(unique_user_ids, age_group_vectorizer, location_vectorizer, gender_vectorizer)\n",
        "\n",
        "# Create joint training model\n",
        "joint_model = JointTrainingModel(\n",
        "    user_model=user_tower,\n",
        "    item_model=item_tower,  # Assumes item_tower is already defined from previous steps\n",
        "    rating_weight=1.0,\n",
        "    retrieval_weight=1.0\n",
        ")\n",
        "\n",
        "# Build the model first by calling it on a sample batch\n",
        "print(\"Building model...\")\n",
        "sample_batch = train_ds_joint.take(1)\n",
        "for batch in sample_batch:\n",
        "    _ = joint_model(batch)\n",
        "    break\n",
        "print(\"Model built successfully!\")\n",
        "\n",
        "# FIXED: Use only float learning rate (no schedule) to work with ReduceLROnPlateau\n",
        "initial_learning_rate = 0.001\n",
        "\n",
        "joint_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
        "    run_eagerly=False\n",
        ")\n",
        "\n",
        "# Callbacks for better training\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,  # Increased patience for better training\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,  # Increased patience\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_joint_model.weights.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the joint model\n",
        "print(\"Training Joint Model (User + Item Towers)...\")\n",
        "print(\"This will update both user and item representations simultaneously...\")\n",
        "\n",
        "history = joint_model.fit(\n",
        "    train_ds_joint,\n",
        "    validation_data=val_ds_joint,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Joint training complete!\")\n",
        "print(\"\\nTraining benefits:\")\n",
        "print(\"- Both user and item towers learned together\")\n",
        "print(\"- Better representation alignment between users and items\")\n",
        "print(\"- Multi-task learning with retrieval and rating prediction\")\n",
        "print(\"- Regularization to prevent overfitting\")\n",
        "\n",
        "# Save the trained models\n",
        "print(\"\\nSaving trained models...\")\n",
        "joint_model.user_model.save_weights('joint_user_model.weights.h5')\n",
        "joint_model.item_model.save_weights('joint_item_model.weights.h5')\n",
        "print(\"Models saved successfully!\")\n",
        "\n",
        "# If you also want to save the complete models (not just weights)\n",
        "# You can use the .keras format which is the new recommended format\n",
        "try:\n",
        "    joint_model.user_model.save('joint_user_model.keras')\n",
        "    joint_model.item_model.save('joint_item_model.keras')\n",
        "    print(\"Complete models saved successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Complete model saving failed: {e}\")\n",
        "    print(\"This might happen if the model has custom layers or components\")\n",
        "\n",
        "# Alternative: Save in the older SavedModel format if .keras doesn't work\n",
        "# joint_model.user_model.save('joint_user_model', save_format='tf')\n",
        "# joint_model.item_model.save('joint_item_model', save_format='tf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMmQE4aH4Z4W",
        "outputId": "4a74a2fc-f2f9-489f-f9ca-a5193a361487"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4] Building and training the Enhanced User Tower with demographics and joint training...\n",
            "Creating training dataset with negative sampling...\n",
            "Training dataset size: 80000\n",
            "Positive samples: 50000\n",
            "Negative samples: 30000\n",
            "Building model...\n",
            "Model built successfully!\n",
            "Training Joint Model (User + Item Towers)...\n",
            "This will update both user and item representations simultaneously...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'joint_training_model_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3237.2476 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 9.7334 - total_loss: 3237.2476\n",
            "Epoch 1: val_loss improved from inf to 623.85321, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 37ms/step - loss: 3220.8018 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 9.7197 - total_loss: 3220.8018 - val_loss: 623.8532 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 0.6057 - val_total_loss: 623.8532 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3208.3098 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 4.0814 - total_loss: 3208.3098\n",
            "Epoch 2: val_loss did not improve from 623.85321\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - loss: 3175.7593 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 4.0768 - total_loss: 3175.7593 - val_loss: 624.3149 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.1494 - val_total_loss: 624.3149 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3199.3989 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.8949 - total_loss: 3199.3989\n",
            "Epoch 3: val_loss improved from 623.85321 to 622.60413, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 3166.8789 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.8936 - total_loss: 3166.8789 - val_loss: 622.6041 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.1803 - val_total_loss: 622.6041 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3175.1340 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.6335 - total_loss: 3175.1340\n",
            "Epoch 4: val_loss improved from 622.60413 to 616.46967, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 3159.0088 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.6337 - total_loss: 3159.0088 - val_loss: 616.4697 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.3491 - val_total_loss: 616.4697 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3159.4670 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.8112 - total_loss: 3159.4670\n",
            "Epoch 5: val_loss improved from 616.46967 to 609.39160, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 3143.4189 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.8116 - total_loss: 3143.4189 - val_loss: 609.3916 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 2.1801 - val_total_loss: 609.3916 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3135.1790 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.9305 - total_loss: 3135.1790\n",
            "Epoch 6: val_loss improved from 609.39160 to 605.27502, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 3119.2429 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.9304 - total_loss: 3119.2429 - val_loss: 605.2750 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 2.3570 - val_total_loss: 605.2750 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3113.4856 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.6996 - total_loss: 3113.4856\n",
            "Epoch 7: val_loss improved from 605.27502 to 602.85706, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 3097.6711 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.6992 - total_loss: 3097.6711 - val_loss: 602.8571 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 2.1870 - val_total_loss: 602.8571 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3094.8376 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.4317 - total_loss: 3094.8376\n",
            "Epoch 8: val_loss improved from 602.85706 to 596.45667, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - loss: 3079.1003 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.4313 - total_loss: 3079.1003 - val_loss: 596.4567 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.8706 - val_total_loss: 596.4567 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3096.0266 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.1843 - total_loss: 3096.0266\n",
            "Epoch 9: val_loss improved from 596.45667 to 594.10388, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 3064.4529 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.1833 - total_loss: 3064.4529 - val_loss: 594.1039 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.7793 - val_total_loss: 594.1039 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3083.8958 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.0022 - total_loss: 3083.8958\n",
            "Epoch 10: val_loss improved from 594.10388 to 592.21680, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 3052.4729 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.0015 - total_loss: 3052.4729 - val_loss: 592.2168 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.6668 - val_total_loss: 592.2168 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3059.1372 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.8262 - total_loss: 3059.1372\n",
            "Epoch 11: val_loss improved from 592.21680 to 590.95758, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - loss: 3043.5205 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.8260 - total_loss: 3043.5205 - val_loss: 590.9576 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.4961 - val_total_loss: 590.9576 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3052.1096 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.7084 - total_loss: 3052.1096\n",
            "Epoch 12: val_loss improved from 590.95758 to 588.62323, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 3036.5225 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.7082 - total_loss: 3036.5225 - val_loss: 588.6232 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.3833 - val_total_loss: 588.6232 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3062.0620 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.6737 - total_loss: 3062.0620\n",
            "Epoch 13: val_loss improved from 588.62323 to 587.05530, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 3030.7783 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.6730 - total_loss: 3030.7783 - val_loss: 587.0553 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.2518 - val_total_loss: 587.0553 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3040.0771 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.5183 - total_loss: 3040.0771\n",
            "Epoch 14: val_loss did not improve from 587.05530\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 3024.5745 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.5181 - total_loss: 3024.5745 - val_loss: 587.0875 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.1817 - val_total_loss: 587.0875 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3050.4487 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.4319 - total_loss: 3050.4487\n",
            "Epoch 15: val_loss improved from 587.05530 to 584.71387, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 3019.3584 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.4315 - total_loss: 3019.3584 - val_loss: 584.7139 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.1066 - val_total_loss: 584.7139 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3029.8940 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.3248 - total_loss: 3029.8940\n",
            "Epoch 16: val_loss did not improve from 584.71387\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 3014.4321 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.3247 - total_loss: 3014.4321 - val_loss: 585.5489 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.0497 - val_total_loss: 585.5489 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3041.6436 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.2897 - total_loss: 3041.6436\n",
            "Epoch 17: val_loss did not improve from 584.71387\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 3010.5300 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.2898 - total_loss: 3010.5300 - val_loss: 585.3771 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 1.0276 - val_total_loss: 585.3771 - learning_rate: 0.0010\n",
            "Epoch 18/20\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3038.3296 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.2695 - total_loss: 3038.3296\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 18: val_loss did not improve from 584.71387\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 3007.3560 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.2690 - total_loss: 3007.3560 - val_loss: 586.5627 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 0.9867 - val_total_loss: 586.5627 - learning_rate: 0.0010\n",
            "Epoch 19/20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3015.0205 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.1841 - total_loss: 3015.0205\n",
            "Epoch 19: val_loss improved from 584.71387 to 583.57672, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 2999.6260 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.1839 - total_loss: 2999.6260 - val_loss: 583.5767 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 0.8579 - val_total_loss: 583.5767 - learning_rate: 5.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3028.4824 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0786 - total_loss: 3028.4824\n",
            "Epoch 20: val_loss improved from 583.57672 to 582.29279, saving model to best_joint_model.weights.h5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 2997.6177 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0788 - total_loss: 2997.6177 - val_loss: 582.2928 - val_regularization_loss: 0.0000e+00 - val_root_mean_squared_error: 0.8419 - val_total_loss: 582.2928 - learning_rate: 5.0000e-04\n",
            "Joint training complete!\n",
            "\n",
            "Training benefits:\n",
            "- Both user and item towers learned together\n",
            "- Better representation alignment between users and items\n",
            "- Multi-task learning with retrieval and rating prediction\n",
            "- Regularization to prevent overfitting\n",
            "\n",
            "Saving trained models...\n",
            "Models saved successfully!\n",
            "Complete models saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 5: Enhanced Recommendation Function with Demographics ---\n",
        "print(\"\\n[5] Implementing enhanced recommendation functions with demographics...\")\n",
        "\n",
        "def get_demographic_recommendations(user_id, top_k=15):\n",
        "    \"\"\"Get recommendations using enhanced user model with demographics\"\"\"\n",
        "    print(f\"\\n--- Enhanced demographic recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return None\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    user_embedding = user_tower(user_input).numpy()\n",
        "    user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Search for similar items using enhanced embeddings\n",
        "    search_k = min(top_k * 3, index.ntotal)\n",
        "    distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "    recommendations = []\n",
        "    for idx in indices[0]:\n",
        "        item_id = index_to_item_id[idx]\n",
        "        if item_id in user_interacted_items:\n",
        "            continue\n",
        "\n",
        "        item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "        recommendations.append({\n",
        "            'item_id': item_id,\n",
        "            'category': item_row['category'],\n",
        "            'title': item_row['item_title']\n",
        "        })\n",
        "\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "\n",
        "    # Calculate accuracy\n",
        "    category_counts = defaultdict(int)\n",
        "    correct_recommendations = 0\n",
        "\n",
        "    for rec in recommendations:\n",
        "        category_counts[rec['category']] += 1\n",
        "        if rec['category'] in preferred_categories:\n",
        "            correct_recommendations += 1\n",
        "\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stEEkN4o5cnk",
        "outputId": "fef36c97-f983-46d2-fb03-781a7793da30"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[5] Implementing enhanced recommendation functions with demographics...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_balanced_recommendations(user_id, top_k=15):\n",
        "    \"\"\"Get recommendations with explicit category balancing\"\"\"\n",
        "    print(f\"\\n--- Balanced recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return handle_cold_start_user(user_id, top_k)\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    user_embedding = user_tower(user_input).numpy()\n",
        "    user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Balanced recommendation approach\n",
        "    recommendations = []\n",
        "    category_counts = defaultdict(int)\n",
        "\n",
        "    # Calculate items per preferred category\n",
        "    items_per_category = max(1, top_k // len(preferred_categories))\n",
        "    remaining_slots = top_k - (items_per_category * len(preferred_categories))\n",
        "\n",
        "    print(f\"Target: {items_per_category} items per category, {remaining_slots} flexible slots\")\n",
        "\n",
        "    # For each preferred category, find the best items\n",
        "    for category in preferred_categories:\n",
        "        category_items = category_to_items[category]\n",
        "        category_item_embeddings = []\n",
        "        category_item_ids = []\n",
        "\n",
        "        # Get embeddings for items in this category\n",
        "        for item_id in category_items:\n",
        "            if item_id not in user_interacted_items:\n",
        "                idx = int(item_id)\n",
        "                category_item_embeddings.append(all_item_embeddings[idx])\n",
        "                category_item_ids.append(item_id)\n",
        "\n",
        "        if not category_item_embeddings:\n",
        "            continue\n",
        "\n",
        "        # Calculate similarities to user embedding\n",
        "        category_item_embeddings = np.array(category_item_embeddings)\n",
        "        similarities = np.dot(category_item_embeddings, user_embedding.T).flatten()\n",
        "\n",
        "        # Get top items for this category\n",
        "        top_indices = np.argsort(similarities)[::-1][:items_per_category]\n",
        "\n",
        "        for idx in top_indices:\n",
        "            item_id = category_item_ids[idx]\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            recommendations.append({\n",
        "                'item_id': item_id,\n",
        "                'category': item_row['category'],\n",
        "                'title': item_row['item_title'],\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "            category_counts[category] += 1\n",
        "\n",
        "    # Fill remaining slots with best overall recommendations\n",
        "    if remaining_slots > 0:\n",
        "        search_k = min(top_k * 3, index.ntotal)\n",
        "        distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "        added_items = set(rec['item_id'] for rec in recommendations)\n",
        "\n",
        "        for idx in indices[0]:\n",
        "            if remaining_slots <= 0:\n",
        "                break\n",
        "\n",
        "            item_id = index_to_item_id[idx]\n",
        "            if item_id in user_interacted_items or item_id in added_items:\n",
        "                continue\n",
        "\n",
        "            item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "            recommendations.append({\n",
        "                'item_id': item_id,\n",
        "                'category': item_row['category'],\n",
        "                'title': item_row['item_title'],\n",
        "                'similarity': 1.0 - distances[0][list(indices[0]).index(idx)]  # Convert distance to similarity\n",
        "            })\n",
        "            category_counts[item_row['category']] += 1\n",
        "            added_items.add(item_id)\n",
        "            remaining_slots -= 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_recommendations = sum(1 for rec in recommendations if rec['category'] in preferred_categories)\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "9M7kouKL5m1c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def handle_cold_start_user(user_id, top_k=15):\n",
        "    \"\"\"Handle recommendations for new users without interaction history\"\"\"\n",
        "    print(f\"Handling cold start for user '{user_id}'\")\n",
        "\n",
        "    # For cold start, use demographic-based recommendations\n",
        "    # This is a simplified approach - in practice, you might use popularity-based or content-based filtering\n",
        "\n",
        "    # Sample popular items from each category\n",
        "    popular_items = []\n",
        "    items_per_category = max(1, top_k // len(categories))\n",
        "\n",
        "    for category in categories[:min(len(categories), top_k)]:\n",
        "        category_items = category_to_items[category]\n",
        "        if category_items:\n",
        "            # For simplicity, take the first few items from each category\n",
        "            # In practice, you'd want to use popularity metrics\n",
        "            sample_size = min(items_per_category, len(category_items))\n",
        "            sampled_items = np.random.choice(category_items, size=sample_size, replace=False)\n",
        "\n",
        "            for item_id in sampled_items:\n",
        "                item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "                popular_items.append({\n",
        "                    'item_id': item_id,\n",
        "                    'category': item_row['category'],\n",
        "                    'title': item_row['item_title']\n",
        "                })\n",
        "\n",
        "    # Shuffle and limit to top_k\n",
        "    np.random.shuffle(popular_items)\n",
        "    popular_items = popular_items[:top_k]\n",
        "\n",
        "    print(f\"Generated {len(popular_items)} cold start recommendations\")\n",
        "    category_counts = defaultdict(int)\n",
        "    for item in popular_items:\n",
        "        category_counts[item['category']] += 1\n",
        "\n",
        "    print(\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(popular_items)) * 100 if popular_items else 0\n",
        "        print(f\"  {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    return 0.0  # Cold start accuracy is 0 since we don't know user preferences\n",
        "\n"
      ],
      "metadata": {
        "id": "e9DjVerf-Mhh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_category_aware_recommendations(user_id, top_k=15, category_diversity_weight=0.3):\n",
        "    \"\"\"Get recommendations with category diversity weighting - Fixed infinite loop issue\"\"\"\n",
        "    print(f\"\\n--- Category-aware recommendations for user '{user_id}' ---\")\n",
        "\n",
        "    if user_id not in unique_user_ids:\n",
        "        print(f\"User '{user_id}' is a new user (cold start).\")\n",
        "        return handle_cold_start_user(user_id, top_k)\n",
        "\n",
        "    # Get user demographics and preferences\n",
        "    user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "    preferred_categories = user_categories[user_id]\n",
        "\n",
        "    # Convert numpy strings to regular strings to avoid comparison issues\n",
        "    preferred_categories = set(str(cat) for cat in preferred_categories)\n",
        "\n",
        "    print(f\"User demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "    print(f\"Preferred categories ({len(preferred_categories)}): {list(preferred_categories)}\")\n",
        "\n",
        "    # Get enhanced user embedding with demographics\n",
        "    user_input = {\n",
        "        \"user_id\": tf.constant([user_id]),\n",
        "        \"age_group\": tf.constant([user_demo['age_group']]),\n",
        "        \"location\": tf.constant([user_demo['location']]),\n",
        "        \"gender\": tf.constant([user_demo['gender']])\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        user_embedding = user_tower(user_input).numpy()\n",
        "        user_embedding = user_embedding / np.linalg.norm(user_embedding, axis=1, keepdims=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting user embedding: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "    # Get items user has already interacted with\n",
        "    user_interacted_items = set(interactions_df[interactions_df['user_id'] == user_id]['item_id'])\n",
        "\n",
        "    # Get more candidates than needed for diversity selection\n",
        "    search_k = min(top_k * 5, index.ntotal)\n",
        "    distances, indices = index.search(user_embedding, search_k)\n",
        "\n",
        "    # Build candidate pool with similarity scores\n",
        "    candidates = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        item_id = index_to_item_id[idx]\n",
        "        if item_id in user_interacted_items:\n",
        "            continue\n",
        "\n",
        "        item_row = items_df[items_df['item_id'] == item_id].iloc[0]\n",
        "        similarity = 1.0 - distances[0][i]  # Convert distance to similarity\n",
        "\n",
        "        candidates.append({\n",
        "            'item_id': item_id,\n",
        "            'category': str(item_row['category']),  # Convert to string\n",
        "            'title': item_row['item_title'],\n",
        "            'similarity': similarity,\n",
        "            'is_preferred': str(item_row['category']) in preferred_categories\n",
        "        })\n",
        "\n",
        "    print(f\"Found {len(candidates)} candidates after filtering\")\n",
        "\n",
        "    if not candidates:\n",
        "        print(\"No candidates found!\")\n",
        "        return 0.0\n",
        "\n",
        "    # FIXED: Use a simpler, guaranteed-to-terminate selection algorithm\n",
        "    recommendations = []\n",
        "    category_counts = defaultdict(int)\n",
        "\n",
        "    # Sort candidates by adjusted score first\n",
        "    for candidate in candidates:\n",
        "        score = candidate['similarity']\n",
        "        if candidate['is_preferred']:\n",
        "            score *= 1.5\n",
        "        candidate['adjusted_score'] = score\n",
        "\n",
        "    # Sort by adjusted score (highest first)\n",
        "    candidates.sort(key=lambda x: x['adjusted_score'], reverse=True)\n",
        "\n",
        "    # Select with diversity constraints\n",
        "    for candidate in candidates:\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "\n",
        "        # Apply diversity penalty\n",
        "        category_penalty = category_counts[candidate['category']] * category_diversity_weight\n",
        "        final_score = candidate['adjusted_score'] - category_penalty\n",
        "\n",
        "        # Accept if it's still a good candidate or if we need more recommendations\n",
        "        if final_score > 0 or len(recommendations) < top_k // 2:\n",
        "            recommendations.append(candidate)\n",
        "            category_counts[candidate['category']] += 1\n",
        "\n",
        "    # If we still don't have enough, fill with remaining candidates\n",
        "    remaining_candidates = [c for c in candidates if c not in recommendations]\n",
        "    for candidate in remaining_candidates:\n",
        "        if len(recommendations) >= top_k:\n",
        "            break\n",
        "        recommendations.append(candidate)\n",
        "        category_counts[candidate['category']] += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_recommendations = sum(1 for rec in recommendations if rec['is_preferred'])\n",
        "    accuracy = correct_recommendations / len(recommendations) if recommendations else 0\n",
        "\n",
        "    print(f\"Generated {len(recommendations)} recommendations\")\n",
        "    print(f\"Category distribution:\")\n",
        "    for category in sorted(category_counts.keys()):\n",
        "        count = category_counts[category]\n",
        "        percentage = (count / len(recommendations)) * 100 if recommendations else 0\n",
        "        is_preferred = \"✓\" if category in preferred_categories else \"✗\"\n",
        "        print(f\"  {is_preferred} {category}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Accuracy: {correct_recommendations}/{len(recommendations)} ({accuracy:.1%})\")\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "kg778q9c5sVW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 6: Comprehensive Evaluation ---\n",
        "print(\"\\n[6] Comprehensive evaluation of recommendation approaches...\")\n",
        "\n",
        "def evaluate_recommendation_approaches():\n",
        "    \"\"\"Evaluate different recommendation approaches\"\"\"\n",
        "    print(\"\\n=== COMPREHENSIVE RECOMMENDATION EVALUATION ===\")\n",
        "\n",
        "    # Select diverse test users\n",
        "    test_users = np.random.choice(unique_user_ids, size=min(10, len(unique_user_ids)), replace=False)\n",
        "\n",
        "    results = {\n",
        "        'demographic': [],\n",
        "        'balanced': [],\n",
        "        'category_aware': []\n",
        "    }\n",
        "\n",
        "    for user_id in test_users:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EVALUATING USER {user_id}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Test demographic recommendations\n",
        "        try:\n",
        "            accuracy_demo = get_demographic_recommendations(user_id, top_k=10)\n",
        "            results['demographic'].append(accuracy_demo)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in demographic recommendations: {e}\")\n",
        "            results['demographic'].append(0.0)\n",
        "\n",
        "        # Test balanced recommendations\n",
        "        try:\n",
        "            accuracy_balanced = get_balanced_recommendations(user_id, top_k=10)\n",
        "            results['balanced'].append(accuracy_balanced)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in balanced recommendations: {e}\")\n",
        "            results['balanced'].append(0.0)\n",
        "\n",
        "        # Test category-aware recommendations\n",
        "        try:\n",
        "            accuracy_category = get_category_aware_recommendations(user_id, top_k=10)\n",
        "            results['category_aware'].append(accuracy_category)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in category-aware recommendations: {e}\")\n",
        "            results['category_aware'].append(0.0)\n",
        "\n",
        "    # Calculate and display overall results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OVERALL EVALUATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for approach, accuracies in results.items():\n",
        "        valid_accuracies = [acc for acc in accuracies if acc is not None]\n",
        "        if valid_accuracies:\n",
        "            avg_accuracy = np.mean(valid_accuracies)\n",
        "            std_accuracy = np.std(valid_accuracies)\n",
        "            print(f\"{approach.replace('_', ' ').title()} Approach:\")\n",
        "            print(f\"  Average Accuracy: {avg_accuracy:.1%} (±{std_accuracy:.1%})\")\n",
        "            print(f\"  Valid Evaluations: {len(valid_accuracies)}/{len(accuracies)}\")\n",
        "        else:\n",
        "            print(f\"{approach.replace('_', ' ').title()} Approach: No valid results\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0iWRdKV57Sy",
        "outputId": "b4463daa-0bd6-4642-8ce1-0b0c30ab920d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6] Comprehensive evaluation of recommendation approaches...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlwTsIUjaz4S",
        "outputId": "d3eb3082-aaee-425d-e73d-9f82e954bd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COMPREHENSIVE RECOMMENDATION EVALUATION ===\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 801\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '801' ---\n",
            "User demographics: 25-34, Female, Houston\n",
            "Preferred categories (5): [np.str_('baby_products'), np.str_('clothing'), np.str_('jewelry'), np.str_('music'), np.str_('grocery')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ jewelry: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '801' ---\n",
            "User demographics: 25-34, Female, Houston\n",
            "Preferred categories (5): [np.str_('baby_products'), np.str_('clothing'), np.str_('jewelry'), np.str_('music'), np.str_('grocery')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ baby_products: 2 items (20.0%)\n",
            "  ✓ clothing: 2 items (20.0%)\n",
            "  ✓ grocery: 2 items (20.0%)\n",
            "  ✓ jewelry: 2 items (20.0%)\n",
            "  ✓ music: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '801' ---\n",
            "User demographics: 25-34, Female, Houston\n",
            "Preferred categories (5): ['grocery', 'jewelry', 'music', 'baby_products', 'clothing']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ jewelry: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 242\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '242' ---\n",
            "User demographics: 18-24, Female, Fort Worth\n",
            "Preferred categories (5): [np.str_('home_garden'), np.str_('pet_supplies'), np.str_('industrial'), np.str_('shoes'), np.str_('grocery')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✗ books: 10 items (100.0%)\n",
            "Accuracy: 0/10 (0.0%)\n",
            "\n",
            "--- Balanced recommendations for user '242' ---\n",
            "User demographics: 18-24, Female, Fort Worth\n",
            "Preferred categories (5): [np.str_('home_garden'), np.str_('pet_supplies'), np.str_('industrial'), np.str_('shoes'), np.str_('grocery')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ grocery: 2 items (20.0%)\n",
            "  ✓ home_garden: 2 items (20.0%)\n",
            "  ✓ industrial: 2 items (20.0%)\n",
            "  ✓ pet_supplies: 2 items (20.0%)\n",
            "  ✓ shoes: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '242' ---\n",
            "User demographics: 18-24, Female, Fort Worth\n",
            "Preferred categories (5): ['pet_supplies', 'grocery', 'shoes', 'home_garden', 'industrial']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✗ books: 10 items (100.0%)\n",
            "  ✓ industrial: 0 items (0.0%)\n",
            "Accuracy: 0/10 (0.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 618\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '618' ---\n",
            "User demographics: 55-64, Male, Dallas\n",
            "Preferred categories (5): [np.str_('office_supplies'), np.str_('clothing'), np.str_('beauty_health'), np.str_('watches'), np.str_('shoes')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ shoes: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '618' ---\n",
            "User demographics: 55-64, Male, Dallas\n",
            "Preferred categories (5): [np.str_('office_supplies'), np.str_('clothing'), np.str_('beauty_health'), np.str_('watches'), np.str_('shoes')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ beauty_health: 2 items (20.0%)\n",
            "  ✓ clothing: 2 items (20.0%)\n",
            "  ✓ office_supplies: 2 items (20.0%)\n",
            "  ✓ shoes: 2 items (20.0%)\n",
            "  ✓ watches: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '618' ---\n",
            "User demographics: 55-64, Male, Dallas\n",
            "Preferred categories (5): ['shoes', 'beauty_health', 'watches', 'office_supplies', 'clothing']\n",
            "Found 48 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ shoes: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 268\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '268' ---\n",
            "User demographics: 35-44, Male, Boston\n",
            "Preferred categories (3): [np.str_('kitchen_dining'), np.str_('office_supplies'), np.str_('shoes')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ shoes: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '268' ---\n",
            "User demographics: 35-44, Male, Boston\n",
            "Preferred categories (3): [np.str_('kitchen_dining'), np.str_('office_supplies'), np.str_('shoes')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ kitchen_dining: 3 items (30.0%)\n",
            "  ✓ office_supplies: 3 items (30.0%)\n",
            "  ✓ shoes: 4 items (40.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '268' ---\n",
            "User demographics: 35-44, Male, Boston\n",
            "Preferred categories (3): ['office_supplies', 'shoes', 'kitchen_dining']\n",
            "Found 47 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ shoes: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 880\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '880' ---\n",
            "User demographics: 55-64, Male, San Diego\n",
            "Preferred categories (3): [np.str_('music'), np.str_('automotive'), np.str_('crafts')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 1 items (10.0%)\n",
            "  ✗ books: 9 items (90.0%)\n",
            "Accuracy: 1/10 (10.0%)\n",
            "\n",
            "--- Balanced recommendations for user '880' ---\n",
            "User demographics: 55-64, Male, San Diego\n",
            "Preferred categories (3): [np.str_('music'), np.str_('automotive'), np.str_('crafts')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 3 items (30.0%)\n",
            "  ✗ books: 1 items (10.0%)\n",
            "  ✓ crafts: 3 items (30.0%)\n",
            "  ✓ music: 3 items (30.0%)\n",
            "Accuracy: 9/10 (90.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '880' ---\n",
            "User demographics: 55-64, Male, San Diego\n",
            "Preferred categories (3): ['crafts', 'automotive', 'music']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 0 items (0.0%)\n",
            "  ✗ books: 10 items (100.0%)\n",
            "Accuracy: 0/10 (0.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 989\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '989' ---\n",
            "User demographics: 25-34, Female, Houston\n",
            "Preferred categories (3): [np.str_('crafts'), np.str_('home_garden'), np.str_('movies_tv')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ home_garden: 7 items (70.0%)\n",
            "  ✗ office_supplies: 1 items (10.0%)\n",
            "  ✗ toys_games: 2 items (20.0%)\n",
            "Accuracy: 7/10 (70.0%)\n",
            "\n",
            "--- Balanced recommendations for user '989' ---\n",
            "User demographics: 25-34, Female, Houston\n",
            "Preferred categories (3): [np.str_('crafts'), np.str_('home_garden'), np.str_('movies_tv')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ crafts: 3 items (30.0%)\n",
            "  ✓ home_garden: 4 items (40.0%)\n",
            "  ✓ movies_tv: 3 items (30.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '989' ---\n",
            "User demographics: 25-34, Female, Houston\n",
            "Preferred categories (3): ['crafts', 'home_garden', 'movies_tv']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ home_garden: 0 items (0.0%)\n",
            "  ✗ office_supplies: 7 items (70.0%)\n",
            "  ✗ toys_games: 3 items (30.0%)\n",
            "Accuracy: 0/10 (0.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 149\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '149' ---\n",
            "User demographics: 45-54, Male, Los Angeles\n",
            "Preferred categories (3): [np.str_('books'), np.str_('kitchen_dining'), np.str_('beauty_health')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ books: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '149' ---\n",
            "User demographics: 45-54, Male, Los Angeles\n",
            "Preferred categories (3): [np.str_('books'), np.str_('kitchen_dining'), np.str_('beauty_health')]\n",
            "Target: 3 items per category, 1 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ beauty_health: 3 items (30.0%)\n",
            "  ✓ books: 4 items (40.0%)\n",
            "  ✓ kitchen_dining: 3 items (30.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '149' ---\n",
            "User demographics: 45-54, Male, Los Angeles\n",
            "Preferred categories (3): ['books', 'beauty_health', 'kitchen_dining']\n",
            "Found 47 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ books: 10 items (100.0%)\n",
            "  ✓ kitchen_dining: 0 items (0.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 341\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '341' ---\n",
            "User demographics: 35-44, Female, Charlotte\n",
            "Preferred categories (4): [np.str_('pet_supplies'), np.str_('toys_games'), np.str_('baby_products'), np.str_('books')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ pet_supplies: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '341' ---\n",
            "User demographics: 35-44, Female, Charlotte\n",
            "Preferred categories (4): [np.str_('pet_supplies'), np.str_('toys_games'), np.str_('baby_products'), np.str_('books')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ baby_products: 2 items (20.0%)\n",
            "  ✓ books: 2 items (20.0%)\n",
            "  ✓ pet_supplies: 4 items (40.0%)\n",
            "  ✓ toys_games: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '341' ---\n",
            "User demographics: 35-44, Female, Charlotte\n",
            "Preferred categories (4): ['books', 'pet_supplies', 'baby_products', 'toys_games']\n",
            "Found 48 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ pet_supplies: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 460\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '460' ---\n",
            "User demographics: 35-44, Male, Boston\n",
            "Preferred categories (4): [np.str_('industrial'), np.str_('movies_tv'), np.str_('toys_games'), np.str_('music')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ industrial: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Balanced recommendations for user '460' ---\n",
            "User demographics: 35-44, Male, Boston\n",
            "Preferred categories (4): [np.str_('industrial'), np.str_('movies_tv'), np.str_('toys_games'), np.str_('music')]\n",
            "Target: 2 items per category, 2 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ industrial: 4 items (40.0%)\n",
            "  ✓ movies_tv: 2 items (20.0%)\n",
            "  ✓ music: 2 items (20.0%)\n",
            "  ✓ toys_games: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '460' ---\n",
            "User demographics: 35-44, Male, Boston\n",
            "Preferred categories (4): ['industrial', 'movies_tv', 'toys_games', 'music']\n",
            "Found 49 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ industrial: 10 items (100.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "============================================================\n",
            "EVALUATING USER 288\n",
            "============================================================\n",
            "\n",
            "--- Enhanced demographic recommendations for user '288' ---\n",
            "User demographics: 18-24, Female, Jacksonville\n",
            "Preferred categories (5): [np.str_('music'), np.str_('automotive'), np.str_('home_garden'), np.str_('baby_products'), np.str_('jewelry')]\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✗ pet_supplies: 10 items (100.0%)\n",
            "Accuracy: 0/10 (0.0%)\n",
            "\n",
            "--- Balanced recommendations for user '288' ---\n",
            "User demographics: 18-24, Female, Jacksonville\n",
            "Preferred categories (5): [np.str_('music'), np.str_('automotive'), np.str_('home_garden'), np.str_('baby_products'), np.str_('jewelry')]\n",
            "Target: 2 items per category, 0 flexible slots\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✓ automotive: 2 items (20.0%)\n",
            "  ✓ baby_products: 2 items (20.0%)\n",
            "  ✓ home_garden: 2 items (20.0%)\n",
            "  ✓ jewelry: 2 items (20.0%)\n",
            "  ✓ music: 2 items (20.0%)\n",
            "Accuracy: 10/10 (100.0%)\n",
            "\n",
            "--- Category-aware recommendations for user '288' ---\n",
            "User demographics: 18-24, Female, Jacksonville\n",
            "Preferred categories (5): ['automotive', 'jewelry', 'music', 'home_garden', 'baby_products']\n",
            "Found 50 candidates after filtering\n",
            "Generated 10 recommendations\n",
            "Category distribution:\n",
            "  ✗ pet_supplies: 10 items (100.0%)\n",
            "Accuracy: 0/10 (0.0%)\n",
            "\n",
            "============================================================\n",
            "OVERALL EVALUATION RESULTS\n",
            "============================================================\n",
            "Demographic Approach:\n",
            "  Average Accuracy: 68.0% (±43.3%)\n",
            "  Valid Evaluations: 10/10\n",
            "Balanced Approach:\n",
            "  Average Accuracy: 99.0% (±3.0%)\n",
            "  Valid Evaluations: 10/10\n",
            "Category Aware Approach:\n",
            "  Average Accuracy: 60.0% (±49.0%)\n",
            "  Valid Evaluations: 10/10\n",
            "\n",
            "============================================================\n",
            "RECOMMENDATION SYSTEM READY\n",
            "============================================================\n",
            "System trained and evaluated successfully!\n",
            "You can now use the interactive demo to explore recommendations.\n",
            "\n",
            "=== SYSTEM SUMMARY ===\n",
            "✓ Enhanced Item Tower: Trained with 10000 items across 20 categories\n",
            "✓ Enhanced User Tower: Trained with 1000 users with demographic features\n",
            "✓ Faiss Index: Contains 10000 normalized item embeddings\n",
            "✓ Training Data: 50000 user-item interactions\n",
            "✓ Evaluation: Completed on 10 test users\n",
            "\n",
            "Recommendation approaches implemented:\n",
            "  1. Demographic-based recommendations\n",
            "  2. Balanced category recommendations\n",
            "  3. Category-aware diversity recommendations\n",
            "\n",
            "To start the interactive demo, call: interactive_recommendation_demo()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Step 7: Interactive Recommendation Interface ---\n",
        "def interactive_recommendation_demo():\n",
        "    \"\"\"Interactive demo of the recommendation system\"\"\"\n",
        "    print(\"\\n=== INTERACTIVE RECOMMENDATION DEMO ===\")\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nChoose an option:\")\n",
        "        print(\"1. Get recommendations for a specific user\")\n",
        "        print(\"2. Get recommendations for a random user\")\n",
        "        print(\"3. Compare all approaches for a user\")\n",
        "        print(\"4. Show user demographics and preferences\")\n",
        "        print(\"5. Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                approach = input(\"Choose approach (demographic/balanced/category_aware): \").strip().lower()\n",
        "                if approach == 'demographic':\n",
        "                    get_demographic_recommendations(user_id)\n",
        "                elif approach == 'balanced':\n",
        "                    get_balanced_recommendations(user_id)\n",
        "                elif approach == 'category_aware':\n",
        "                    get_category_aware_recommendations(user_id)\n",
        "                else:\n",
        "                    print(\"Invalid approach. Using demographic approach.\")\n",
        "                    get_demographic_recommendations(user_id)\n",
        "            else:\n",
        "                print(f\"User {user_id} not found. Available users: {list(unique_user_ids)[:10]}...\")\n",
        "\n",
        "        elif choice == '2':\n",
        "            user_id = np.random.choice(unique_user_ids)\n",
        "            print(f\"Selected random user: {user_id}\")\n",
        "            get_demographic_recommendations(user_id)\n",
        "\n",
        "        elif choice == '3':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                print(f\"Comparing all approaches for user {user_id}:\")\n",
        "                get_demographic_recommendations(user_id)\n",
        "                get_balanced_recommendations(user_id)\n",
        "                get_category_aware_recommendations(user_id)\n",
        "            else:\n",
        "                print(f\"User {user_id} not found.\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            user_id = input(\"Enter user ID: \").strip()\n",
        "            if user_id in unique_user_ids:\n",
        "                user_demo = users_df[users_df['user_id'] == user_id].iloc[0]\n",
        "                preferred_categories = user_categories[user_id]\n",
        "                user_interactions_count = len(interactions_df[interactions_df['user_id'] == user_id])\n",
        "\n",
        "                print(f\"\\nUser {user_id} Profile:\")\n",
        "                print(f\"  Demographics: {user_demo['age_group']}, {user_demo['gender']}, {user_demo['location']}\")\n",
        "                print(f\"  Preferred Categories: {list(preferred_categories)}\")\n",
        "                print(\"breaking\")\n",
        "                print(f\"  Total Interactions: {user_interactions_count}\")\n",
        "\n",
        "                # Show actual interactions\n",
        "                user_interactions = interactions_df[interactions_df['user_id'] == user_id]\n",
        "                interaction_details = pd.merge(user_interactions, items_df, on='item_id')\n",
        "                category_breakdown = interaction_details['category'].value_counts()\n",
        "\n",
        "                print(\"  Interaction Breakdown:\")\n",
        "                for category, count in category_breakdown.items():\n",
        "                    print(f\"    {category}: {count} interactions\")\n",
        "            else:\n",
        "                print(f\"User {user_id} not found.\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = evaluate_recommendation_approaches()\n",
        "\n",
        "# Start interactive demo\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RECOMMENDATION SYSTEM READY\")\n",
        "print(\"=\"*60)\n",
        "print(\"System trained and evaluated successfully!\")\n",
        "print(\"You can now use the interactive demo to explore recommendations.\")\n",
        "\n",
        "# Uncomment the line below to start the interactive demo\n",
        "# interactive_recommendation_demo()\n",
        "\n",
        "print(\"\\n=== SYSTEM SUMMARY ===\")\n",
        "print(f\"✓ Enhanced Item Tower: Trained with {len(items_df)} items across {len(categories)} categories\")\n",
        "print(f\"✓ Enhanced User Tower: Trained with {len(users_df)} users with demographic features\")\n",
        "print(f\"✓ Faiss Index: Contains {index.ntotal} normalized item embeddings\")\n",
        "print(f\"✓ Training Data: {len(interactions_df)} user-item interactions\")\n",
        "print(f\"✓ Evaluation: Completed on {len(evaluation_results['demographic'])} test users\")\n",
        "print(\"\\nRecommendation approaches implemented:\")\n",
        "print(\"  1. Demographic-based recommendations\")\n",
        "print(\"  2. Balanced category recommendations\")\n",
        "print(\"  3. Category-aware diversity recommendations\")\n",
        "print(\"\\nTo start the interactive demo, call: interactive_recommendation_demo()\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M-cUjNfc2wZg"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxLTbHaCWjiIkpHpuecYum",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}